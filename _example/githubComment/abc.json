{
    "list": [
        {
            "time": "May 15, 2014",
            "title": "是否会出PHP版本的接口？",
            "contents": [
                "Hi,\nI am using Jieba in python3, and having the following problem:\nfor the sentence with word \"省\", e.g.  \"丰田太省了\", the results are: 丰田/ 太省/ 了.\nHowever, it should be expected as: 丰田/ 太/ 省/ 了.  it happens also to \"最省\".\nI checked the jieba.dict, and do not find the word \"太省\" in the dictionary. Could you check if it is an issue in the code? Thank you.\nBest Regards,\nQing",
                "\u003e\u003e\u003e tuple(jieba.cut('丰田太省了'))\n('丰田', '太省', '了')\n\u003e\u003e\u003e tuple(jieba.cut('丰田太省了',HMM=0))\n('丰田', '太', '省', '了')\n\nIt's a 'new word' found by the HMM model because the model thinks '太省' is probably a word. So is the case of '最'. If you want to get the result only according to the dictionary, set HMM=False.",
                "yes, it works great after set HMM=False. thanks!",
                "当文中出现英文句号时会被当作小数点处理，分词和相应的词性标注会出现错误。\n如\"1.hello.2.word.\"\n另外“了解Java7.0.”也会出现分词错误。",
                "结巴不支持多线程分词吗？为什么多线程分词还要更慢呢？\n缘起\n对14206篇文章进行相似性去重 首先要分词 总用时12分钟 发现光分词就用了8分钟\n所以想通过多线程分词来加快处理速度 如5个线程 每个线程处理2841篇文章\n如下所示\ndef segment(texts_, start, end):\n    print(start, end)\n    texts_cut = []\n    for t in texts_[start:end]:\n        if not isinstance(t, str):\n            t = \"\"\n        t = regex.sub(\"\", t)\n        cut = list(jieba.cut(t))\n        texts_cut.append(cut)\n\n    return texts_cut\n\nmax_workers = 5\nbatch_count = len(texts)//max_workers\nlogging.info(\"start\")\nwith concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n    future_2_start = {executor.submit(segment, texts, s*batch_count, (s+1)*batch_count if s \u003c max_workers-1 else len(texts)) : s for s in range(max_workers)}\n    start_2_data = {}\n    for future in concurrent.futures.as_completed(future_2_start):\n        start = future_2_start[future]\n        data = future.result()\n        start_2_data[start] = data\n\n    texts_cut = []\n    for start in sorted(start_2_data):\n        texts_cut.extend(start_2_data[start])\n\n    print(len(texts_cut))\n    \nlogging.info(\"done\")\n\n于是用了5193篇文章做了实验\n单线程（max_workers=1）用时约43秒\n2018-08-01 19:10:14,529 : INFO : start\n2018-08-01 19:10:57,450 : INFO : done\n\n5个线程（max_workers=5）用时约50秒\n2018-08-01 19:11:38,841 : INFO : start\n2018-08-01 19:12:29,616 : INFO : done\n\n为什么多线程分词还要更慢呢？",
                "原因\n\nThe GIL is an interpreter-level lock. This lock prevents execution of multiple threads at once in the Python interpreter. Each thread that wants to run must wait for the GIL to be released by the other thread, which means your multi-threaded Python application is actually single threaded. The GIL prevents simultaneous access to Python objects by multiple threads.\n摘自：https://medium.com/practo-engineering/threading-vs-multiprocessing-in-python-7b57f224eadb\n\n解决方法：\n\nBut of course, we would want to use the ProcessPoolExecutor for CPU intensive tasks. The ThreadPoolExecutor is better suited for network operations or I/O\n摘自： http://masnun.com/2016/03/29/python-a-quick-introduction-to-the-concurrent-futures-module.html\n\n上述代码 ThreadPoolExecutor ==》 ProcessPoolExecutor 即可\n注意：\n先显式初始化 否则每个进程都要初始化\njieba.initialize()\n\n用时\n# 多进程 ProcessPoolExecutor 8个进程\n2018-08-11 10:38:34,569 : INFO : start\n2018-08-11 10:38:47,999 : INFO : done took time: 12.43\n\n2018-08-11 10:40:23,051 : INFO : start\n2018-08-11 10:40:35,909 : INFO : done took time: 11.95",
                "其实jieba自带了并行分词，只需要jieba.enable_parallel()",
                "需求非常大",
                "自己搞个thrift之类的service... 转一下... = =",
                "ls +1",
                "+1",
                "虽然有做广告的嫌疑，但是还是发一下把：\njieba的c++版本有提供http接口供php使用啊：https://github.com/aszxqw/cppjieba",
                "@raykwok @tonicbupt @tonyseek @binaryoung @yanyiwu @fxsjy 我這個週末心血來潮翻譯了 jieba 的 PHP 版本，fukuball/jieba-php，目前是從 python 的 0.16 版翻譯過來的，未來再慢慢往上升級，效能也需要再改善，請有興趣的開發者一起加入開發！",
                "@fukuball 非常感谢"
            ]
        },
        {
            "time": "Apr 30, 2015",
            "title": "想拆除带有特殊符号的词",
            "contents": [
                "这个分词里面多少只有一种词性吗。",
                "逻辑上 第一个多少应该分为 多 和 少",
                "@sidealice , 这个难度挺高的。先mark一下。",
                "这个好难哈哈",
                "这种问题估计本身就有问题：比如我在你那句接一句。你说的多少是多少。只有说得人才知道哪个是多少，哪个是多+少了，甚至乎自己都不知道。这是语言表达本身就不够准确规范，让人都猜不透的，估计就不是难的问题了吧？",
                "这个属于语意消除歧义，以现阶段的计算能力，是很难的。",
                "我在Pyspark中引用jieba，用jieba.load_userdict('xxx.txx')加载了自己的词典，但是发现RDD.map操作jieba的时候，该词典没有加载成功。于是，我定义了一个jieba_instance = jieba.Tokernice()，通过传参的方式将定义个一个jieba实例传递到RDD的Map操作函数里面。操作如下:\nss = rdd_data.map(lambda x:judge_disease(x,column,disease_connect,table_name,jieba_instance ))\njudge_disease方法如下：\ndef judge_disease(x,column,disease_connect,table_name,jieba_instance ):\n    arr = []\n    if not column:\n        return (x,None,table_name)\n    seg = jieba_instance .cut(x[column.encode('utf-8')])\n    for item in seg:\n        item = item.encode('utf-8')\n        if disease_connect.has_key(item):\n            arr.append(unicode(item,'utf-8'))\n    arr = list(set(arr))\n    res = (x,\"$$\".join(arr).split(\"$$\"),table_name)\n\n    return res\n\n报如下错误：\n‘UnpicklingError: NEWOBJ class argument has NULL tp_new’\n求助：首先，我该如何通过传参的方式，将一个jieba实例传入函数？\n另外，有碰到相关问题的其它解决方法吗？",
                "机器运行环境：\njieba + python2.7+spark1.6\n在用jieba load user dict的时候，发现该dict加载后是以python 字典的数据类型存在于spark的driver里面，但是worker进程无法共享这段内存。同时，我也发现jieba分词调用的源数据是jieba.cache和FREQ（dictionary）。cache文件当set_dictionary的时候，是生成jieba.xxxxxxxxxxxx.cache，该文件在worker进程也无法访问。所以，spark里面，运用常规方法无法访问jieba自己的词典。\n所以，特提供自己的意见，看能否在版本中进行修改。\n1.可以直接加载python字典类型的函数，这样，我可以将该字典在spark里面共享，然后在spark的worker进程独立加载。\n2.增加一种模式，使得在类似spark这种方式里面，可以直接选择如何生成jieba.cache，比如，可以直接将add_word或者load_userdict新增的词，放到该jieba.cache里面。\n以上是我个人的浅见，希望继续交流。",
                "不用传jieba实例，在judge_disease中初始化jieba实例呢？",
                "@snowlord 那相当于每次都得新建jieba实例，对spark的map操作来说，太耗费资源和时间。",
                "请问，楼主解决了吗？怎么解决的呢？ @liaicheng",
                "把默认的dict.txt替换成自己的dict.txt。",
                "我也尝试了这么做，但是dict.txt文件需要 词频 吧？ 我测试的，如果不添加词频，报错。\n那么 对于自己词典中的词，没有词频，楼主添加的默认的吗？ 比如 3？ 还是怎么解决的呢？ @liaicheng",
                "没有用过spark，估计是不是由于全局变量引起的一些问题？\n建议用jieba.Tokenizer 得到一个分词器实例，然后再调用相关方法。\njieba.Tokenizer(dictionary=DEFAULT_DICT) 新建自定义分词器，可用于同时使用不同词典。jieba.dt 为默认分词器，所有全局分词相关函数都是该分词器的映射。\n例子：https://github.com/fxsjy/jieba/blob/0243d568e9421ab7d3c75f49e9adfc230810e0a3/test/test_lock.py",
                "@fxsjy Spark中使用tokr.cut()会报can't pickle thread.lock objects错误\n@liaicheng 把默认的dict.txt替换成自己的dict.txt，你的意思是jieba.set_dictionary(\"dict\\dict.txt\")\njieba.initialize() 这样？但是我试了还是不行，每次在map中使用jieba时，还是会执行Building prefix dict from the default dictionary ……",
                "请问我是将jieba分词包装到worker上了，这么做的话会有什么问题吗？效率会低吗？？然后用udf函数去解决这个问题\nsplit_precision = udf(lambda value:[token for token in (jieba.cut(value.encode('utf-8'), cut_all=False))],ArrayType(StringType()))\nsplit_result = df.withColumn(\"tokens\"+'_'+column, split_precision(col(column)))\n最后我想问一个问题，如果我想增加某个词的权重以让该词能够分出来的话，我在driver的程序里写了jieba.add_word(df_dict.iloc[i][0],freq=10000)\n但最后没有将词分出来，猜想是没有将词加载到worker的词库里，所以没有将词分出来。。那么该怎么去做呢？",
                "我也遇到這個問題，我的解法是：\n\n把 /usr/local/lib/python2.7/dist-packages/jieba/ 裡面的 dict.txt 替換成自己的字典\n把 /tmp/ 裡面的  jieba.cache 清掉\n\n然後就可以了，參考看看",
                "有解决方案了吗@liaicheng",
                "我把版本为调低到0.36就不报‘UnpicklingError: NEWOBJ class argument has NULL tp_new’ 了",
                "同样遇到pyspark加载用户自定义词典，未生效，有什么解决方案吗？",
                "16年的bug，到现在都没解决么",
                "我也遇到這個問題，我的解法是：\n\n把 /usr/local/lib/python2.7/dist-packages/jieba/ 裡面的 dict.txt 替換成自己的字典\n把 /tmp/ 裡面的  jieba.cache 清掉\n\n然後就可以了，參考看看\n\n这个cache需要去每一台机器上清理么？",
                "问题：jieba分词器spark分布式环境中加载词典失效(不会报错，但是不会加载词典分出想要的词，这个也是我用了一年的pyspark+jieba之后才突然发现自己用错了一年)\n尝试过的方法：\n\n修改默认词典dict.txt。\n失败，测试发现jieba并不加载自己的默认词典！！！\n将jieba作为参数传入柯里化的udf。\n失败，jieba中有线程锁，无法序列化。\n新建一个jieba的Tokenizer实例，作为参数传入udf或者作为全局变量。\n失败原因同2\n使用.rdd.mapPartiton算子, 在每个partiton中加载词典。\n失败原因同2\n在udf中通过jieba.dt.initialized判断是否需要加载词典\n成功。\n原因：通过本地测试（这样executor日志也会痛driver日志一起打印出来），\n发现，结巴会在每个partiton中分别初始化一个默认分词器。\njieba采用延迟加载机制，在没有使用jieba去分词或加载词典时，\njieba中的默认分词器，jieba.dt不会初始化。\njieba.dt.initialized属性在初始化后会从False变为True，\n所以可以依据这个来判断jieba是否初始化。从而决定是否加载词典。\n\nimport jieba\nfrom pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.types import ArrayType, StringType\n\nspark = SparkSession.builder.appName(\"word2vec_cluster\").getOrCreate()\n\ndf = spark.createDataFrame([\n    (0, '不用加水光？'),\n    (0, '不用加水光？'),\n    (0, '不用加水光？'),\n    (1, '加水光多少钱啊？'),\n    (1, '加水光多少钱啊？'),\n    (1, '加水光多少钱啊？'),\n    (2, '单纯水光多少'),\n    (2, '单纯水光多少'),\n    (2, '单纯水光多少'),\n], [\"id\", \"content\"])\ndf.show()\n\njieba.load_userdict(\n    '/Users/chant/sy/dwh/utils/static/soyoung.item.jieba.dic')\n\n\ndef _wrong_tokenize(content):\n    \"\"\"错误的分词函数。此udf不会加载词典，\"不用加水光\",\n    其中的\"水光\"会被切开，结果为[不用, 加水, 光, ？]\n\n    :param content: {str} 要分词的句子\n    :return: list[word, word, ...]\n    \"\"\"\n    return [i for i in jieba.cut(content)]\n\n\nwrong_tokenize = F.udf(_wrong_tokenize, ArrayType(StringType()))\ndf.select(wrong_tokenize('content').alias('words')).show()\nprint('df 的partition 数为：', df.rdd.getNumPartitions())\n输出如下，“水光”一词被切开了，说明加载词典失效了。但是惊喜地发现jieba在每个partiton中只初始化了一次。\n+---+--------+\n| id| content|\n+---+--------+\n|  0|  不用加水光？|\n|  0|  不用加水光？|\n|  0|  不用加水光？|\n|  1|加水光多少钱啊？|\n|  1|加水光多少钱啊？|\n|  1|加水光多少钱啊？|\n|  2|  单纯水光多少|\n|  2|  单纯水光多少|\n|  2|  单纯水光多少|\n+---+--------+\n\n[Stage 2:\u003e                                                          (0 + 1) / 1]Building prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\nLoading model cost 1.588 seconds.\nPrefix dict has been built succesfully.\n[Stage 3:\u003e                                                          (0 + 3) / 3]Building prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\nLoading model cost 5.942 seconds.\nPrefix dict has been built succesfully.\n[Stage 3:===================\u003e                                       (1 + 2) / 3]Loading model cost 6.045 seconds.\nPrefix dict has been built succesfully.\n[Stage 3:=======================================\u003e                   (2 + 1) / 3]Loading model cost 6.320 seconds.\nPrefix dict has been built succesfully.\n+--------------------+\n|               words|\n+--------------------+\n|      [不用, 加水, 光, ？]|\n|      [不用, 加水, 光, ？]|\n|      [不用, 加水, 光, ？]|\n|[加水, 光, 多少, 钱, 啊, ？]|\n|[加水, 光, 多少, 钱, 啊, ？]|\n|[加水, 光, 多少, 钱, 啊, ？]|\n|      [单纯, 水, 光, 多少]|\n|      [单纯, 水, 光, 多少]|\n|      [单纯, 水, 光, 多少]|\n+--------------------+\ndf 的partition 数为： 4\nimport jieba\nfrom pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.types import ArrayType, StringType\n\nspark = SparkSession.builder.appName(\"word2vec_cluster\").getOrCreate()\n\ndf = spark.createDataFrame([\n    (0, '不用加水光？'),\n    (0, '不用加水光？'),\n    (0, '不用加水光？'),\n    (1, '加水光多少钱啊？'),\n    (1, '加水光多少钱啊？'),\n    (1, '加水光多少钱啊？'),\n    (2, '单纯水光多少'),\n    (2, '单纯水光多少'),\n    (2, '单纯水光多少'),\n], [\"id\", \"content\"])\ndf.show()\n\n\ndef _tokenize(content):\n    \"\"\"此udf在每个partiton中加载一次词典，\n    \"不用加水光\", 其中的\"水光\"会被保留，结果为[不用, 加, 水光, ？]\n\n    :param content: {str} 要分词的内容\n    :return: list[word, word, ...]\n    \"\"\"\n    if not jieba.dt.initialized:\n        # 词典中有\"水光\"这个词\n        jieba.load_userdict(\n            '/Users/chant/sy/dwh/utils/static/soyoung.item.jieba.dic')\n    return [i for i in jieba.cut(content)]\n\n\ntokenize = F.udf(_tokenize, ArrayType(StringType()))\ndf.select(tokenize('content').alias('words')).show()\nprint('df 的partition 数为：', df.rdd.getNumPartitions())\n\n+---+--------+\n| id| content|\n+---+--------+\n|  0|  不用加水光？|\n|  0|  不用加水光？|\n|  0|  不用加水光？|\n|  1|加水光多少钱啊？|\n|  1|加水光多少钱啊？|\n|  1|加水光多少钱啊？|\n|  2|  单纯水光多少|\n|  2|  单纯水光多少|\n|  2|  单纯水光多少|\n+---+--------+\n\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\n[Stage 2:\u003e                                                          (0 + 1) / 1]Loading model cost 0.804 seconds.\nPrefix dict has been built succesfully.\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\n[Stage 3:\u003e                                                          (0 + 3) / 3]Loading model cost 1.204 seconds.\nPrefix dict has been built succesfully.\nLoading model cost 1.200 seconds.\nPrefix dict has been built succesfully.\nLoading model cost 1.224 seconds.\nPrefix dict has been built succesfully.\n+------------------+\n|             words|\n+------------------+\n|    [不用, 加, 水光, ？]|\n|    [不用, 加, 水光, ？]|\n|    [不用, 加, 水光, ？]|\n|[加, 水光, 多少钱, 啊, ？]|\n|[加, 水光, 多少钱, 啊, ？]|\n|[加, 水光, 多少钱, 啊, ？]|\n|      [单纯, 水光, 多少]|\n|      [单纯, 水光, 多少]|\n|      [单纯, 水光, 多少]|\n+------------------+\n\ndf 的partition 数为： 4\n@fxsjy @liaicheng 至此，这个issue应该可以关闭了，jieba没有问题，saprk也没有问题，但是spark+jieba就产生了问题。建议作者大大把下面这个udf的写法，添加到官方的readme中，毕竟这个bug不会报错，难以感知，有很多想我一样的小白，错误地使用了一年的pyspark+jieba却没有发现问题。\ndef _tokenize(content):\n    \"\"\"此udf在每个partiton中加载一次词典\"\"\"\n    if not jieba.dt.initialized:\n        jieba.load_userdict('user_dict.txt')\n    return [i for i in jieba.cut(content)]\n\n\ntokenize = F.udf(_tokenize, ArrayType(StringType()))",
                "This is an awesome answer. You could write a sample program and request a\nmerge.\n\nActually this is referring to understanding of spark behavior and your\nsample is valuablly helpful.\n…\nOn Wed, Dec 5, 2018, 10:27 Chant00 ***@***.*** wrote:\n\n 问题：jieba分词器spark分布式环境中加载词典失效(不会报错，但是不会加载词典分出想要的词，这个也是我用了一年的pyspark+jieba之后才突然发现自己用错了一年)\n 尝试过的方法：\n\n    1. 修改默认词典dict.txt。\n    失败，测试发现jieba并不加载自己的默认词典！！！\n    2. 将jieba作为参数传入柯里化的udf。\n    失败，jieba中有线程锁，无法序列化。\n    3. 新建一个jieba的Tokenizer实例，作为参数传入udf或者作为全局变量。\n    失败原因同2\n    4. 使用.rdd.mapPartiton算子, 在每个partiton中加载词典。\n    失败原因同2\n    5. 在udf中通过jieba.dt.initialized判断是否需要加载词典\n    成功。\n    原因：通过本地测试（这样executor日志也会痛driver日志一起打印出来），\n    发现，结巴会在每个partiton中分别初始化一个默认分词器。\n    jieba采用延迟加载机制，在没有使用jieba去分词或加载词典时，\n    jieba中的默认分词器，jieba.dt不会初始化。\n    jieba.dt.initialized属性在初始化后会从False变为True，\n    所以可以依据这个来判断jieba是否初始化。从而决定是否加载词典。\n\n import jiebafrom pyspark.sql import SparkSession, functions as Ffrom pyspark.sql.types import ArrayType, StringType\n\n spark = SparkSession.builder.appName(\"word2vec_cluster\").getOrCreate()\n\n df = spark.createDataFrame([\n     (0, '不用加水光？'),\n     (0, '不用加水光？'),\n     (0, '不用加水光？'),\n     (1, '加水光多少钱啊？'),\n     (1, '加水光多少钱啊？'),\n     (1, '加水光多少钱啊？'),\n     (2, '单纯水光多少'),\n     (2, '单纯水光多少'),\n     (2, '单纯水光多少'),\n ], [\"id\", \"content\"])\n df.show()\n\n jieba.load_userdict(\n     '/Users/chant/sy/dwh/utils/static/soyoung.item.jieba.dic')\n\n def _wrong_tokenize(content):\n     \"\"\"错误的分词函数。此udf不会加载词典，\"不用加水光\",    其中的\"水光\"会被切开，结果为[不用, 加水, 光, ？]    :param content: {str} 要分词的句子    :return: list[word, word, ...]    \"\"\"\n     return [i for i in jieba.cut(content)]\n\n\n wrong_tokenize = F.udf(_wrong_tokenize, ArrayType(StringType()))\n df.select(wrong_tokenize('content').alias('words')).show()print('df 的partition 数为：', df.rdd.getNumPartitions())\n\n 输出如下，“水光”一词被切开了，说明加载词典失效了。但是惊喜地发现jieba在每个partiton中只初始化了一次。\n +---+--------+| id| content|\n +---+--------+|  0|  不用加水光？||  0|  不用加水光？||  0|  不用加水光？||  1|加水光多少钱啊？||  1|加水光多少钱啊？||  1|加水光多少钱啊？||  2|  单纯水光多少||  2|  单纯水光多少||  2|  单纯水光多少|\n +---+--------+\n\n [Stage 2:\u003e                                                          (0 + 1) / 1]Building prefix dict from the default dictionary ...\n Loading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\n Loading model cost 1.588 seconds.\n Prefix dict has been built succesfully.\n [Stage 3:\u003e                                                          (0 + 3) / 3]Building prefix dict from the default dictionary ...\n Loading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\n Building prefix dict from the default dictionary ...\n Loading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\n Building prefix dict from the default dictionary ...\n Loading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\n Loading model cost 5.942 seconds.\n Prefix dict has been built succesfully.\n [Stage 3:===================\u003e                                       (1 + 2) / 3]Loading model cost 6.045 seconds.\n Prefix dict has been built succesfully.\n [Stage 3:=======================================\u003e                   (2 + 1) / 3]Loading model cost 6.320 seconds.\n Prefix dict has been built succesfully.\n +--------------------+|               words|\n +--------------------+|      [不用, 加水, 光, ？]||      [不用, 加水, 光, ？]||      [不用, 加水, 光, ？]||[加水, 光, 多少, 钱, 啊, ？]||[加水, 光, 多少, 钱, 啊, ？]||[加水, 光, 多少, 钱, 啊, ？]||      [单纯, 水, 光, 多少]||      [单纯, 水, 光, 多少]||      [单纯, 水, 光, 多少]|\n +--------------------+\n df 的partition 数为： 4\n\n import jiebafrom pyspark.sql import SparkSession, functions as Ffrom pyspark.sql.types import ArrayType, StringType\n\n spark = SparkSession.builder.appName(\"word2vec_cluster\").getOrCreate()\n\n df = spark.createDataFrame([\n     (0, '不用加水光？'),\n     (0, '不用加水光？'),\n     (0, '不用加水光？'),\n     (1, '加水光多少钱啊？'),\n     (1, '加水光多少钱啊？'),\n     (1, '加水光多少钱啊？'),\n     (2, '单纯水光多少'),\n     (2, '单纯水光多少'),\n     (2, '单纯水光多少'),\n ], [\"id\", \"content\"])\n df.show()\n\n def _tokenize(content):\n     \"\"\"此udf在每个partiton中加载一次词典，    \"不用加水光\", 其中的\"水光\"会被保留，结果为[不用, 加, 水光, ？]    :param content: {str} 要分词的内容    :return: list[word, word, ...]    \"\"\"\n     if not jieba.dt.initialized:\n         # 词典中有\"水光\"这个词\n         jieba.load_userdict(\n             '/Users/chant/sy/dwh/utils/static/soyoung.item.jieba.dic')\n     return [i for i in jieba.cut(content)]\n\n\n tokenize = F.udf(_tokenize, ArrayType(StringType()))\n df.select(tokenize('content').alias('words')).show()print('df 的partition 数为：', df.rdd.getNumPartitions())\n\n +---+--------+| id| content|\n +---+--------+|  0|  不用加水光？||  0|  不用加水光？||  0|  不用加水光？||  1|加水光多少钱啊？||  1|加水光多少钱啊？||  1|加水光多少钱啊？||  2|  单纯水光多少||  2|  单纯水光多少||  2|  单纯水光多少|\n +---+--------+\n\n Building prefix dict from the default dictionary ...\n Loading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\n [Stage 2:\u003e                                                          (0 + 1) / 1]Loading model cost 0.804 seconds.\n Prefix dict has been built succesfully.\n Building prefix dict from the default dictionary ...\n Loading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\n Building prefix dict from the default dictionary ...\n Loading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\n Building prefix dict from the default dictionary ...\n Loading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\n [Stage 3:\u003e                                                          (0 + 3) / 3]Loading model cost 1.204 seconds.\n Prefix dict has been built succesfully.\n Loading model cost 1.200 seconds.\n Prefix dict has been built succesfully.\n Loading model cost 1.224 seconds.\n Prefix dict has been built succesfully.\n +------------------+|             words|\n +------------------+|    [不用, 加, 水光, ？]||    [不用, 加, 水光, ？]||    [不用, 加, 水光, ？]||[加, 水光, 多少钱, 啊, ？]||[加, 水光, 多少钱, 啊, ？]||[加, 水光, 多少钱, 啊, ？]||      [单纯, 水光, 多少]||      [单纯, 水光, 多少]||      [单纯, 水光, 多少]|\n +------------------+\n\n df 的partition 数为： 4\n\n @fxsjy \u003chttps://github.com/fxsjy\u003e @liaicheng\n \u003chttps://github.com/liaicheng\u003e\n 至此，这个issue应该可以关闭了，jieba没有问题，saprk也没有问题，但是spark+jieba就产生了问题。建议作者大大把下面这个udf的写法，添加到官方的readme中，毕竟这个bug不会报错，难以感知，有很多想我一样的小白，错误地使用了一年的pyspark+jieba却没有发现问题。\n\n def _tokenize(content):\n     \"\"\"此udf在每个partiton中加载一次词典\"\"\"\n     if not jieba.dt.initialized:\n         jieba.load_userdict('user_dict.txt')\n     return [i for i in jieba.cut(content)]\n\n\n tokenize = F.udf(_tokenize, ArrayType(StringType()))\n\n —\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n \u003c#387 (comment)\u003e, or mute\n the thread\n \u003chttps://github.com/notifications/unsubscribe-auth/AA0SqpifJt8gBxevt5TSMZ-cvH2UuJz7ks5u1y8PgaJpZM4JkN9I\u003e\n .",
                "问题：jieba分词器spark分布式环境中加载词典失效(不会报错，但是不会加载词典分出想要的词，这个也是我用了一年的pyspark+jieba之后才突然发现自己用错了一年)\n尝试过的方法：\n\n修改默认词典dict.txt。\n失败，测试发现jieba并不加载自己的默认词典！！！\n将jieba作为参数传入柯里化的udf。\n失败，jieba中有线程锁，无法序列化。\n新建一个jieba的Tokenizer实例，作为参数传入udf或者作为全局变量。\n失败原因同2\n使用.rdd.mapPartiton算子, 在每个partiton中加载词典。\n失败原因同2\n在udf中通过jieba.dt.initialized判断是否需要加载词典\n成功。\n原因：通过本地测试（这样executor日志也会痛driver日志一起打印出来），\n发现，结巴会在每个partiton中分别初始化一个默认分词器。\njieba采用延迟加载机制，在没有使用jieba去分词或加载词典时，\njieba中的默认分词器，jieba.dt不会初始化。\njieba.dt.initialized属性在初始化后会从False变为True，\n所以可以依据这个来判断jieba是否初始化。从而决定是否加载词典。\n\nimport jieba\nfrom pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.types import ArrayType, StringType\n\nspark = SparkSession.builder.appName(\"word2vec_cluster\").getOrCreate()\n\ndf = spark.createDataFrame([\n    (0, '不用加水光？'),\n    (0, '不用加水光？'),\n    (0, '不用加水光？'),\n    (1, '加水光多少钱啊？'),\n    (1, '加水光多少钱啊？'),\n    (1, '加水光多少钱啊？'),\n    (2, '单纯水光多少'),\n    (2, '单纯水光多少'),\n    (2, '单纯水光多少'),\n], [\"id\", \"content\"])\ndf.show()\n\njieba.load_userdict(\n    '/Users/chant/sy/dwh/utils/static/soyoung.item.jieba.dic')\n\n\ndef _wrong_tokenize(content):\n    \"\"\"错误的分词函数。此udf不会加载词典，\"不用加水光\",\n    其中的\"水光\"会被切开，结果为[不用, 加水, 光, ？]\n\n    :param content: {str} 要分词的句子\n    :return: list[word, word, ...]\n    \"\"\"\n    return [i for i in jieba.cut(content)]\n\n\nwrong_tokenize = F.udf(_wrong_tokenize, ArrayType(StringType()))\ndf.select(wrong_tokenize('content').alias('words')).show()\nprint('df 的partition 数为：', df.rdd.getNumPartitions())\n输出如下，“水光”一词被切开了，说明加载词典失效了。但是惊喜地发现jieba在每个partiton中只初始化了一次。\n+---+--------+\n| id| content|\n+---+--------+\n|  0|  不用加水光？|\n|  0|  不用加水光？|\n|  0|  不用加水光？|\n|  1|加水光多少钱啊？|\n|  1|加水光多少钱啊？|\n|  1|加水光多少钱啊？|\n|  2|  单纯水光多少|\n|  2|  单纯水光多少|\n|  2|  单纯水光多少|\n+---+--------+\n\n[Stage 2:\u003e                                                          (0 + 1) / 1]Building prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\nLoading model cost 1.588 seconds.\nPrefix dict has been built succesfully.\n[Stage 3:\u003e                                                          (0 + 3) / 3]Building prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\nLoading model cost 5.942 seconds.\nPrefix dict has been built succesfully.\n[Stage 3:===================\u003e                                       (1 + 2) / 3]Loading model cost 6.045 seconds.\nPrefix dict has been built succesfully.\n[Stage 3:=======================================\u003e                   (2 + 1) / 3]Loading model cost 6.320 seconds.\nPrefix dict has been built succesfully.\n+--------------------+\n|               words|\n+--------------------+\n|      [不用, 加水, 光, ？]|\n|      [不用, 加水, 光, ？]|\n|      [不用, 加水, 光, ？]|\n|[加水, 光, 多少, 钱, 啊, ？]|\n|[加水, 光, 多少, 钱, 啊, ？]|\n|[加水, 光, 多少, 钱, 啊, ？]|\n|      [单纯, 水, 光, 多少]|\n|      [单纯, 水, 光, 多少]|\n|      [单纯, 水, 光, 多少]|\n+--------------------+\ndf 的partition 数为： 4\nimport jieba\nfrom pyspark.sql import SparkSession, functions as F\nfrom pyspark.sql.types import ArrayType, StringType\n\nspark = SparkSession.builder.appName(\"word2vec_cluster\").getOrCreate()\n\ndf = spark.createDataFrame([\n    (0, '不用加水光？'),\n    (0, '不用加水光？'),\n    (0, '不用加水光？'),\n    (1, '加水光多少钱啊？'),\n    (1, '加水光多少钱啊？'),\n    (1, '加水光多少钱啊？'),\n    (2, '单纯水光多少'),\n    (2, '单纯水光多少'),\n    (2, '单纯水光多少'),\n], [\"id\", \"content\"])\ndf.show()\n\n\ndef _tokenize(content):\n    \"\"\"此udf在每个partiton中加载一次词典，\n    \"不用加水光\", 其中的\"水光\"会被保留，结果为[不用, 加, 水光, ？]\n\n    :param content: {str} 要分词的内容\n    :return: list[word, word, ...]\n    \"\"\"\n    if not jieba.dt.initialized:\n        # 词典中有\"水光\"这个词\n        jieba.load_userdict(\n            '/Users/chant/sy/dwh/utils/static/soyoung.item.jieba.dic')\n    return [i for i in jieba.cut(content)]\n\n\ntokenize = F.udf(_tokenize, ArrayType(StringType()))\ndf.select(tokenize('content').alias('words')).show()\nprint('df 的partition 数为：', df.rdd.getNumPartitions())\n+---+--------+\n| id| content|\n+---+--------+\n|  0|  不用加水光？|\n|  0|  不用加水光？|\n|  0|  不用加水光？|\n|  1|加水光多少钱啊？|\n|  1|加水光多少钱啊？|\n|  1|加水光多少钱啊？|\n|  2|  单纯水光多少|\n|  2|  单纯水光多少|\n|  2|  单纯水光多少|\n+---+--------+\n\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\n[Stage 2:\u003e                                                          (0 + 1) / 1]Loading model cost 0.804 seconds.\nPrefix dict has been built succesfully.\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /var/folders/px/cyksvc795bb2qr4rg29s4y5m0000gn/T/jieba.cache\n[Stage 3:\u003e                                                          (0 + 3) / 3]Loading model cost 1.204 seconds.\nPrefix dict has been built succesfully.\nLoading model cost 1.200 seconds.\nPrefix dict has been built succesfully.\nLoading model cost 1.224 seconds.\nPrefix dict has been built succesfully.\n+------------------+\n|             words|\n+------------------+\n|    [不用, 加, 水光, ？]|\n|    [不用, 加, 水光, ？]|\n|    [不用, 加, 水光, ？]|\n|[加, 水光, 多少钱, 啊, ？]|\n|[加, 水光, 多少钱, 啊, ？]|\n|[加, 水光, 多少钱, 啊, ？]|\n|      [单纯, 水光, 多少]|\n|      [单纯, 水光, 多少]|\n|      [单纯, 水光, 多少]|\n+------------------+\n\ndf 的partition 数为： 4\n@fxsjy @liaicheng 至此，这个issue应该可以关闭了，jieba没有问题，saprk也没有问题，但是spark+jieba就产生了问题。建议作者大大把下面这个udf的写法，添加到官方的readme中，毕竟这个bug不会报错，难以感知，有很多想我一样的小白，错误地使用了一年的pyspark+jieba却没有发现问题。\ndef _tokenize(content):\n    \"\"\"此udf在每个partiton中加载一次词典\"\"\"\n    if not jieba.dt.initialized:\n        jieba.load_userdict('user_dict.txt')\n    return [i for i in jieba.cut(content)]\n\n\ntokenize = F.udf(_tokenize, ArrayType(StringType()))\n\n想请问你这个是分布式集群环境做的吗？",
                "@Chant00\n用此方法成功了~~感谢！\nsc.addPyFile('file_path/userdict.txt') \ndef seg_sentence(sentence, stopwords): \nsentence = re.sub(\"[A-Za-z0-9\\!\\%\\[\\]\\,\\。\\@]\", \"\", sentence) \nif not jieba.dt.initialized:\n jieba.load_userdict(\"userdict.txt\") \nsentence_seged = jieba.cut(sentence.strip()) \noutstr = [] \nfor word in sentence_seged:\n if word not in stopwords: \nif (word != '\\t') or (word != \"，\"): \noutstr.append(word) \nreturn outstr",
                "我想把“尿β２微球蛋白值”或者“PGI/PG”作为一个词拆分，我用了add_word和suggest_freq方法，但是仍然被拆开了，这种词的拆分能实现吗？",
                "因为你的词里面 β２ 和 / 不在中文正则匹配中，所以即使你添加了这两个词，在读取字典之前，就已经被拆分了：\nre_han_default = re.compile(\"([\\u4E00-\\u9FA5a-zA-Z0-9+#\u0026\\._]+)\", re.U)\nre_han_cut_all = re.compile(\"([\\u4E00-\\u9FA5]+)\", re.U)\n\n你可以根据自己的需要来修改 re_han_default，例如：\n\u003e\u003e\u003e import re\n\u003e\u003e\u003e import jieba\n\u003e\u003e\u003e jieba.re_han_default = re.compile(r'([\\u4e00-\\u9fa5a-zA-Z0-9+#\u0026\\._/β２]+)', re.UNICODE)\n\u003e\u003e\u003e jieba.add_word(\"尿β２微球蛋白值\")\n\u003e\u003e\u003e list(jieba.cut(u\"尿β２微球蛋白值\"))\n['尿β２微球蛋白值']",
                "我在linux和windows下的测试结果都如下\n`\n\n\n\nimport jieba\nimport re\njieba.re_han_default = re.compile(r'([\\u4e00-\\u9fa5a-zA-Z0-9+#\u0026._/β２]+)', re.UNICODE)\njieba.add_word(\"尿β２微球蛋白值\")\nlist(jieba.cut(u\"尿β２微球蛋白值\"))\n[u'\\u5c3f', u'\\u03b2', u'\\uff12', u'\\u5fae', u'\\u7403', u'\\u86cb', u'\\u767d', u'\\u503c']\n`\n版本的问题吗?\n@wangbin"
            ]
        },
        {
            "time": "Sep 20, 2016",
            "title": "，开兄控欧拉5555566555555555，mbvcxzzxz/",
            "contents": [
                "在使用pseg.cut来获取分词词性的时候，发现“云计算”、“石墨烯”这些词语的词性居然是x，x我看介绍通常是一些标点符号，为什么会出现这种情况，还有其他词语也会有这种情况吗？",
                "因为云计算，石墨烯是新词，词典里找不到，估计就默认设置成词性x了，你需要自己自定义词典并且加入这些新词的词性。",
                "import jieba.posseg as pseg\nwords = pseg.cut(text)\n\n\n\nresult = jieba.tokenize(text)\n发现对text的分词有些不同，比如“抄的”在前面就是一个词，第二种就是2个词.请问哪里出了问题？",
                "我也发现这个问题了，没人回复啊",
                "@jiffies , poseg和jieba.cut使用的模型不一样，所以分词的效果有差别。",
                "哪个好呢？",
                "我在使用jieba分词时，遇到了如下情况：\n待分词句子：奥布瑞·普拉扎（Aubrey Plaza），1984年6月26日出生于美国特拉华州威尔明顿，演员。\n分词结果：\njieba.cut:\n奥/布/瑞/·/普拉/扎/（/Aubrey/ /Plaza/）/，/1984/年/6/月/26/日出/生于/美国/特拉华州/威尔/明/顿/，/演员\nposseg.cut:\n奥 nr 布 nr 瑞 ns · x 普拉 nrt 扎 v （ x Aubrey eng   x Plaza eng ） x ， x 1984 eng 年 m 6 eng 月 m 26 eng 日出 v 生于 v 美国 ns 特拉华州 ns 威尔 nrt 明 a 顿 q ， x 演员 n\n出生这个词一直无法正确分出来，我发现词典中已经包含了该词：出生 1979 v，而日出频率低于出生：日出 394 v；然后我尝试自己再一次将“出生”添加到词典，以及关闭HMM，均没有作用，请问这是什么问题呢，谢谢！",
                "找到了原因，日出的频率是394，出生频率是1979，生于的频率是4690，导致分为日出/生于\n解决办法：\njieba.del_word('日出')\njieba.add_word('出生于')\njieba.add_word('日出')",
                "在统计词与邻近词的连通数的时候，为什么只统计后面的词，按道理说前后的词都要统计的吧？",
                "No description provided."
            ]
        },
        {
            "time": "Mar 9, 2018",
            "title": "加入了字典后在本地环境下测试没问题，但使用了flask的web框架后似乎就不行了",
            "contents": [
                "我看到CRF分词算法的介绍，http://blog.csdn.net/ifengle/article/details/3849852\n感觉还行。不知道jieba分词是怎么看CRF的？或者已经用了CRF?",
                "从效果上看，CRF可以有更好的切分效果。百度已经在工程上应用了。",
                "@chenweican 真的吗？感觉crf资料不是很多啊。",
                "Building prefix dict from the default dictionary ...\nLoading model from cache /var/folders/t8/p9fyd38n58v35j4fxrndsyyw0000gn/T/jieba.cache\nLoading model cost 0.357 seconds.\nPrefix dict has been built succesfully.",
                "jieba有官方的QQ群之类的吗？还有jieba有考虑文本摘要的问题么，最近在做这方面的。",
                "[2018-02-02 12:00:00,000] ERROR in app: Exception on /api/getWordFreq [GET]\nTraceback (most recent call last):\nFile \"e:\\app\\python35\\lib\\site-packages\\flask\\app.py\", line 1982, in wsgi_app\nresponse = self.full_dispatch_request()\nFile \"e:\\app\\python35\\lib\\site-packages\\flask\\app.py\", line 1615, in full_dis\natch_request\nreturn self.finalize_request(rv)\nFile \"e:\\app\\python35\\lib\\site-packages\\flask\\app.py\", line 1630, in finalize\nrequest\nresponse = self.make_response(rv)\nFile \"e:\\app\\python35\\lib\\site-packages\\flask\\app.py\", line 1740, in make_res\nonse\nrv = self.response_class.force_type(rv, request.environ)\nFile \"e:\\app\\python35\\lib\\site-packages\\werkzeug\\wrappers.py\", line 921, in f\nrce_type\nresponse = BaseResponse(*_run_wsgi_app(response, environ))\nFile \"e:\\app\\python35\\lib\\site-packages\\werkzeug\\wrappers.py\", line 59, in _r\nn_wsgi_app\nreturn _run_wsgi_app(*args)\nFile \"e:\\app\\python35\\lib\\site-packages\\werkzeug\\test.py\", line 923, in run_w\ngi_app\napp_rv = app(environ, start_response)\nTypeError: 'dict' object is not callable\n127.0.0.1 - - [02/Feb/2018 12:00:00] \"GET /api/getWordFreq HTTP/1.1\" 500 -��\n是因为没法取到字典的绝对路径吗？��字典放在站点根目录，加载时写的是： jieba.load_userdict(\"dict.txt\")\n�"
            ]
        },
        {
            "time": "May 13, 2015",
            "title": "对于jieba分词的一个疑问",
            "contents": [
                "我尝试对恒大相关的文章进行分词，结果错误率奇高，结果如下\n'''\n按照原计划，凯赛尔在西班牙学习三年后就将回国，但对于志向高远的“凯撒”来说，他更希望凭借未来三年的表现能留在西班牙继续深造，从而拉开自己的职业生涯，“**/随恒大/足校在西班牙学习三年后，我希望能留在这里，并开启自己的职业生涯\n保监会再发狠/招恒大/人寿被禁止投资股票\n这些/对恒大/概念股的影响有多大\n然后在10月底三季报披露散户看/到恒大/**买了4.95%是不是要举牌了，许老板就把货都卖给你们了\n'''\n我将自定义词典中”恒大“的词频调整到了10000都没有任何变化，尝试jieba.suggest_freq('恒大', True)也没有用；尝试将HMM关掉，结果恒大这个词会被一直分成”恒/大“。",
                "能不能再切之前，直接添加：\njieba.add_word(\"恒大\")",
                "Environment:\n\njieba v0.39\n\nCode:\nimport jieba\njieba.add_word(\"现代汉语文本切分与词性标注规范Ｖ1.0\");\nseg_list = jieba.cut(\"北大计算语言学研究所从 1992 年开始进行汉语语料库的多级加工研究。第一步是对原\\n\" +\n                \"始语料进行切分和词性标注。1994 年制订了《现代汉语文本切分与词性标注规范Ｖ1.0》。\")\nprint(','.join(seg_list))\n\noutput:\n北大,计算,语言学,研究所,从, ,1992, ,年,开始,进行,汉语,语料库,的,多级,加工,研究,。,第一步,是,对,原,\n,始,语料,进行,切分,和,词性,标注,。,1994, ,年,制订,了,《,现代汉语,文本,切分,与,词性,标注,规范,Ｖ,1.0,》,。",
                "根据个人需要修改jieba包init.py中几个正则表达式，使其支持数字特殊字符。re_han_default = re.compile(“(.+)”, re.U)",
                "from pyhanlp import *\n\nsegment = HanLP.newSegment('感知机')\nCustomDictionary.insert('现代汉语文本切分与词性标注规范Ｖ1.0')\nprint(segment.analyze(\"北大计算语言学研究所从1992年开始进行汉语语料库的多级加工研究。第一步是对原\" +\n                      \"始语料进行切分和词性标注。1994年制订了《现代汉语文本切分与词性标注规范Ｖ1.0》。\"))\n\n[北大/j 计算/v 语言学/n 研究所/n]/nt 从/p 1992年/t 开始/v 进行/v 汉语/nz 语料库/n 的/u 多/m 级/q 加工/v 研究/v 。/w 第一/m 步/q 是/v 对/p 原始/a 语/Ng 料/v 进行/v 切分/vn 和/c 词性/n 标注/v 。/w 1994年/t 制订/v 了/u 《/w 现代汉语文本切分与词性标注规范Ｖ1.0/nz 》/w 。/w",
                "为什么在CUT里面使用的HMM是BMES 这种模型。而不是基于词性的分词？\n我感觉基于词性的效果可能比BMES好一些。\n是使用过传统的词性的HMM效果不好，还是其他原因。在这个地方有点困惑希望作者能解惑。\n如果使用过传统的词性效果不太好，我就不去尝试了。目前觉得BMES 的分词能力可能达不到需要的精度。"
            ]
        },
        {
            "time": "Dec 12, 2013",
            "title": "jieba分词的学习能力如何体现",
            "contents": [
                "为了更好匹配使用了自定义词典，因为有些涉及到了产品型号，如5w-30，需要切成一个词，但是现在切出来还分成3个单词。修改了init中的正则\nre_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#\u0026._-\\ ]+)\", re.U)，还是不能成功，请问有方法吗？",
                "@jianjianxin\n正则表达式写错了吧",
                "No description provided.",
                "def setLogLevel(log_level):\nglobal logger\ndefault_logger.setLevel(log_level)\nlogger 是否应该是 default_logger?",
                "No description provided.",
                "各位，新接触这块领域，多多指教。\n对于jieba的学习新词的能力，是会自动讲本次分词过程中的新词自动加入默认的词典，还是其他方式体现。\n谢谢。\nKane",
                "刚详细看了下几个问题列表，对https://github.com/fxsjy/jieba/issues/7中提到的回复也看了下，\n基本知道jieba的新词学习能力跟hmm有关系，也试了下finalseg的新词学习功能：\n姚晨和老凌离婚了\n张绍刚发道歉信网友不认可\n结果：\n/ 姚晨/ 和/ 老凌/ 离婚/ 了/\n/ 张绍/ 刚发/ 道歉信/ 网友/ 不/ 认可/\n基本知道jieba默认分词是打开了新词学习的，想问下：\n1、姚晨这个例子，是否可以讲学习到的新词自动补充到默认词典中？\n1、第二个错误，需要自己补充词典来解决这个错误么？\n望不吝赐教，谢谢。",
                "@xmkane , 现在结巴分词并不能很好地处理一些歧义case，解决办法暂时只有加词典条目。 自定义词典如何添加可以参考wiki，另外git repository中的最新版本也支持调用add_word加词条   #122 。\n另外，你举的【张绍刚发道歉信网友不认可】这个例子可以用jieba分词子模块posseg来试一试，它的新词识别能力比较强，但是速度要慢一些。\n张绍刚/nr 发/v 道歉信/vn 网友/n 不/d 认可/v\nhttp://jiebademo.ap01.aws.af.cm/  （选择“显示词性”）",
                "@fxsjy ,我最近也在jieba，感觉很好，但也不清楚jieba的新词发现机制，简单测试了一下，貌似新词发现能力和文本长度没有关系？我以前了解过基于统计的新词发现方法，比如考虑词的内聚性和自由度，这就文本越多分词越准确，但我感觉jieba使用的不是这个原理？可以稍微说一下jieba的新词发现机制么？非常感谢呢～～"
            ]
        },
        {
            "time": "May 7, 2014",
            "title": "jieba的词性标注结果与ictclas标准不一致",
            "contents": [
                "这个简单的测试程序，会产生exception：\nimport jieba\ns = \"女装\"\na = jieba.cut(s, cut_all=False)\nprint \"/\".join(a)\n错误是：\nUnicodeEncodeError: 'gbk' codec can't encode character u'\\u016e' in position 0: illegal multibyte sequence\n我debug发现原因是 \"女装\" 用utf-8解码时并不会产生exception，但其实解码后的东西是乱七八糟的：u'\\u016e\\u05f0' 后面用这个字符串去分词了。我不知道为什么这个词这么特殊。\n一个解决办法是提供一个参数，人工指定编码，这样就肯定不会错了。",
                "在mac os下用Python 2.7.9测试没有出错，返回结果还是“女装”这个词。",
                "@alexwwang 我的.py是GBK编码的。你的脚本头加上# -- coding: gb18030 -- 试试？",
                "s 是 str 类型，gb18030 编码（根据源代码的编码而定）；你的命令行环境是 'gbk' 编码的。\n因此，在分词时，由于 s 是 str 类型，程序先进行尝试 utf-8 解码（解码错误再尝试 gbk），结果！解码成功，为Ůװ两个字符，尝试 print 的分词结果为 Ů/װ。但由于 gbk 无法编码这两个字符（码表里没有），只好异常。\n因此，要避免出这种状况：\n\ns = u\"女装\"\n尽量在源代码中使用 UTF-8 编码",
                "@gumblex 谢谢。我上面的代码只是PoC，实际上是从一个GBK文件逐句读出然后分词，产生的异常。所以不能用您建议的办法来避免了。目前我的做法是先decode再传给jieba. 不过因为jieba号称不管编码是GBK或者unicode都可以自动支持，所以我觉得这个还应该是个bug。",
                "做文本处理的该用 Unicode 类型就用 Unicode 类型，毕竟分词针对的是字符，不是用来负责判断这到底是 GBK 还是 Unicode 编码。",
                "No description provided.",
                "个人在做简历分类，请问大家，有适合的中文文本分类的停用词词典么",
                "A possible one if you really need it in NLP task: https://github.com/kn45/seg-jb/blob/master/segjb/stopwords.dat",
                "https://github.com/hankcs/HanLP/blob/master/data/dictionary/stopwords.txt",
                "我参看了ictclass关于词性标注的介绍[1]：标点符号都应该标注为以“w”开头的字符串，而jieba目前将它们标记为“x”，即认为是“字符串”。\n代码：\nlist(pseg.cut(u\"今天的任务有四项：写程序、看电影和跑步。\"))\n结果：\n[今天/t, 的/uj, 任务/n, 有/v, 四项/m, ：/x, 写/v, 程序/n, 、/x, 看/v, 电影/n, 和/c, 跑步/n, 。/x]\n请问这是bug么？\n[1] http://ictclas.org/docs/ICTPOS3.0%E6%B1%89%E8%AF%AD%E8%AF%8D%E6%80%A7%E6%A0%87%E8%AE%B0%E9%9B%86.doc",
                "另外，jieba还使用了标签“j”，而“j”并没有出现在ictclas的标准中。",
                "请问uj中的j表示什么含义？"
            ]
        },
        {
            "time": "Oct 14, 2018",
            "title": "地名部分词性不准确",
            "contents": [
                "具体指的是prob_*.py文件里面的数据\n是不是对训练的样本的每个字都分别贴上‘BMES’的标签？然后再统计出三个概率表？",
                "模型的数据是如何生成的？ #7",
                "当我尝试使用“pip install jieba\"命令来安装它时，我无法正确安装，日志中显示为:\"error: could not create '/usr/local/lib/python2.7/dist-packages/jieba': Permission denied\",请问，我应该怎样来解决这个问题？？？使用的系统是Ubuntu14.04版本！",
                "Chmod file with privilege\nSent from my iPhone\n\nOn 22 Nov 2015, at 20:53, hippieZhou notifications@github.com wrote:\nWhen I use \"pip install jieba\",I could't install it ,the log shows that:\"error: could not create '/usr/local/lib/python2.7/dist-packages/jieba': Permission denied\",what should i do ???\n—\nReply to this email directly or view it on GitHub.",
                "Thank you !",
                "求助；\npostgresql添加jieba扩展，提示找不到jieba的 module ，我的Postgresql的版本是8.4，看到github上有pg_jieba的项目，要求postgresql的版本都是9.1以上，请问Jieba对Postgresql有版本限制吗？紧急求助，谢谢！",
                "部分地名词性为nr，比如“孙吴县”。建议按照民政部最新的行政区划表更新词库。\n2018行政区划代码"
            ]
        },
        {
            "time": "Jan 13, 2014",
            "title": "关于jieba分词提取权重最大的关键词的示例代码问题。",
            "contents": [
                "您好,\n最近想透过R做断字断词及语意情感分析, 找了不少分词辞典及情感辞典\n看到网上另一个词库分享, 但是里面的字段格式不太能理解, 不知道各位是否可以指点一下呢?\nDownload URL:\nhttp://down.51cto.com/data/269758\n档案字段格式长这样:\n1\t\t扭在\tnz\t6ff026e67cc327c2\t\t\t2\t930\t1\t0\t3\n2\t\t拟在\tnz\t3ad73d9dc29b7c54\t\t\t2\t10092\t0\t0\t3\n3\t\t捻针\tnz\t52w76148h1f9cei9\t\t\t2\t308\t1\t0\t3\n4\t\t怒发冲冠\tnfcg\t9jue6c3a96b5eoif\t\t\t4\t9313\t1\t0\t3\n5\t\t农副产品\tnfcp\tadc3aa31df8f47dd\t\t\t4\t7450\t1\t0\t3\n6\t\t女房东\tnfd\t78foi563e45ga896\t\t\t3\t7108\t1\t0\t3\n7\t\t暖风机\tnfj\tbbe96g73c89c3298\t\t\t3\t5116\t1\t0\t3\n8\t\t年富力强\tnflq\t6df5a2e8ba64c9a3\t\t\t4\t13740\t1\t0\t3\n9\t\t逆耳忠言\tnezy\t8h65g473e5e5g52e\t\t\t4\t2285\t1\t0\t3\n10\t\t难分难解\tnfnj\t47a6ce306f3i3d2w\t\t\t4\t7382\t1\t0\t3\n11\t\t难分难舍\tnfns\t7i3eb71865g69aa5\t\t\t4\t6718\t1\t0\t3\n12\t\t闹翻天\tnft\tcbe4d1c47ie345a2\t\t\t3\t2694\t1\t0\t3\n13\t\t女服务员\tnfwy\ta9cc81f8f08fac43\t\t\t4\t12386\t1\t0\t3\n14\t\t逆反心理\tnfxl\ta3i3ba1d2a8ed348\t\t\t4\t6096\t1\t0\t3\n15\t\t农副业\tnfy\tc1969cd63ic682bb\t\t\t3\t5468\t1\t0\t3\n16\t\t年复一年\tnfyn\tfd18eb2b7afbc1ed\t\t\t4\t27804\t1\t0\t3",
                "如 我的需求是\"\"号里面的话不分词",
                "比如微博的表情文字是   [泪]\n我想拿到这个表情统计个数，请问可以怎么做？",
                "需求：\n要求匹配出内容的指定的一些关键词（我自己的词库里的词），忽略jieba里千千万万不相关的词，对于我来说，这些词没有用。\n实现：\n我想的是，把jieba默认的词库替换掉。可是，我看文档里没有提到如何替换jieba的词库\n还有什么好的办法实现吗？\n谢谢各位",
                "你可以用jieba的自定义字典实现准确识别\n\n中铁建资产管理有限公司 高正  18601064889\n…\n 在 2018年10月24日，11:11，Bakkan Hwang ***@***.***\u003e 写道：\n\n 需求：\n 要求匹配出内容的指定的一些关键词（我自己的词库里的词），忽略jieba里千千万万不相关的词，对于我来说，这些词没有用。\n\n 实现：\n 我想的是，把jieba默认的词库替换掉。可是，我看文档里没有提到如何替换jieba的词库\n 还有什么好的办法实现吗？\n\n 谢谢各位\n\n —\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub, or mute the thread.",
                "jieba有实现延时加载机制\njieba 采用延迟加载，import jieba 和 jieba.Tokenizer() 不会立即触发词典的加载，一旦有必要才开始加载词典构建前缀字典。如果你想手工初始 jieba，也可以手动初始化。\nimport jieba\njieba.initialize()  # 手动初始化（可选）\n在 0.28 之前的版本是不能指定主词典的路径的，有了延迟加载机制后，你可以改变主词典的路径:\njieba.set_dictionary('data/dict.txt.big')\n例子： https://github.com/fxsjy/jieba/blob/master/test/test_change_dictpath.py",
                "本人接触jieba跟python不久，关于返回权重最大关键词参照了给出的源码 https://github.com/fxsjy/jieba/blob/master/test/extract_tags.py 现在不出效果好像卡在了这一行“usage:    python extract_tags.py [filename] -k [top k]” 。。不太了解这些参数的含义，本人上网查阅未找到解决方法。请教一下有朋友能不能大致讲解一下这个源码的意思，最好能简单结合一下实际案例，如：\nencoding=utf-8\nimport jieba\nseg_list = jieba.cut(\"我来到北京清华大学\",cut_all=True)\nprint \"Full Mode:\", \"/ \".join(seg_list) #全模式\nseg_list = jieba.cut(\"我来到北京清华大学\",cut_all=False)\nprint \"Default Mode:\", \"/ \".join(seg_list) #精确模式\nseg_list = jieba.cut(\"他来到了网易杭研大厦\") #默认是精确模式\nprint \", \".join(seg_list)\nseg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\") #搜索引擎模式\nprint \", \".join(seg_list)\n不胜感激！"
            ]
        },
        {
            "time": "Oct 6, 2017",
            "title": "为什么不能用自己的语料训练新的HMM模型",
            "contents": [
                "例如（￣▽￣），(｀・ω・´)等颜文字表情。\n在处理的时候，即使添加 add_word 或者 load_userdict ，结果都会被分开成：\n（   ￣   ▽   ￣   ）",
                "java版本的分词Ansj和HanLP都支持用自己的语料再训练，为什么结巴分词这么久也不支持？"
            ]
        },
        {
            "time": "Jun 18, 2018",
            "title": "为什么对gen_pfdict并行化没有加速效果呢",
            "contents": [
                "Jieba对中文名的支持不是很好啊,例如：\n中国商务部国际贸易谈判副代表俞建华表示，2001年在上海，中方成功举办了APEC第九次领导人非正式会议。时隔13年，中方非常荣幸再次成为APEC东道主。中国政府高度重视这一盛事，目前各项准备工作已全面展开。\n分词为\n中国/ns 商务部/nt 国际/n 贸易谈判/n 副/b 代表/n 俞/zg 建华/nz 表示/v ，/x 2001/m 年/m 在/p 上海/ns ，/x 中方/f 成功/a 举办/v 了/ul APEC/eng 第九次/m 领导人/n 非正式/b 会议/n 。/x 时隔/n 13/m 年/m ，/x 中方/f 非常/d 荣幸/nr 再次/d 成为/v APEC/eng 东道主/nr 。/x 中国政府/nt 高度重视/l 这/r 一/m 盛事/n ，/x 目前/t 各项/r 准备/v 工作/vn 已/d 全面/n 展开/v 。/x\n其中人名“俞建华”被分成两个词了\"俞/zg 建华/nz ”,有计划改进吗？",
                "如果只用posseg中的viterbi方法进行分词，“中国商务部国际贸易谈判副代表俞建华表示” 被分得非常完美\n中国商务部/nt, 国际/n, 贸易/vn, 谈判/vn, 副/b, 代表/n, 俞建华/nr, 表示/v",
                "db中有数据条“酒”，“白酒”，\"红酒\"\n搜索“酒\" ,只能出现一条记录”酒\"\n希望能同时出现“白酒”,\"红酒\"",
                "这种问题不归这里管吧",
                "在遍历文件每一行那里，使用threading多线程处理，无论是静态分配还是动态分配（不同块大小），都没有直接串行的效率高，词典规模增大三倍后仍是这样。\n这是为什么呢？",
                "python 有一个全局多线程锁，对于计算密集型任务应该使用多进程。需要加速可以试试这个repo https://github.com/deepcs233/jieba_fast/tree/master/jieba_fast。"
            ]
        },
        {
            "time": "May 22, 2015",
            "title": "ValueError: math domain error",
            "contents": [
                "",
                "返回值是列表的形式，如何用jieba的词性标注给列表中的每个关键词标注词性。",
                "参数控制 withFlag=True",
                "非中英文的詞，如Citroën會被切成Citro+ë+n。請問可以如何避免？\n看似系統將ë等特殊字看作符號。我有機會修改source code，將原本的英數字集加入此些特殊字嗎？目前看到相關的設定是jieba/posseg/init.py檔案的re_skip_detail = re.compile(\"([.0-9]+|[a-zA-Z0-9]+)\")。\n或是有其它建議？\n謝謝。",
                "运行test目录下的demo.py时出现了\n1. 分词\nBuilding prefix dict from /usr/local/lib/python2.7/dist-packages/jieba/dict.txt ...\nLoading model from cache /tmp/jieba.cache\nLoading model cost 0.000440835952759 seconds.\nPrefix dict has been built succesfully.\nFull Mode: 我/ 来/ 到/ 北/ 京/ 清/ 华/ 大/ 学\nTraceback (most recent call last):\nFile \"demo.py\", line 22, in \nprint(\"Default Mode: \" + \"/ \".join(seg_list))  # 默认模式\nFile \"/usr/local/lib/python2.7/dist-packages/jieba/init.py\", line 274, in cut\nfor word in cut_block(blk):\nFile \"/usr/local/lib/python2.7/dist-packages/jieba/init.py\", line 199, in cut_DAG\ncalc(sentence, DAG, route=route)\nFile \"/usr/local/lib/python2.7/dist-packages/jieba/__init.py\", line 144, in calc\nlogtotal = log(total)\nValueError: math domain error\n这是因为什么？\n这是用python2还是python3写的？",
                "词典好像没成功加载，删掉 /tmp/jieba.cache，下载最新版试试。\n最新版 Python 2/3 均兼容"
            ]
        },
        {
            "time": "Jun 7, 2018",
            "title": "通过文件名的形式加载自定义用户词典时，若省略掉词频和词性，每一个单词后面必须要加一个空格，不然会报错",
            "contents": [
                "使用extract_tags函数时，提示函数没有 allowPOS 参数？？？\n\u003e from jieba import analyse\n\u003e s = '精确模式，试图将句子最精确地切开，适合文本分析；#此模式为默认模式'\n\u003e analyse.extract_tags(s,allowPOS=('n','nv'))\nTraceback (most recent call last):\nFile \"\", line 1, in \nanalyse.extract_tags(s,allowPOS=('n','nv'))\nTypeError: extract_tags() got an unexpected keyword argument 'allowPOS'",
                "+1",
                "我也遇见了这个问题,昨天还好好的,可以运行,没有改动,今天就不行了,去掉这个参数就可以运行.",
                "希望再提供一些信息:\n\n在 jupyter notebook遇见此问题,配置信息如下:\n-- MaC 10.13.4, Python 3.6.5,\n但是在终端中是没有问题,可以运行.",
                "+1",
                "自定义词典里写上“歼-10”了，也没有起作用。",
                "添加到自定义词库",
                "同问！大家有什么办法没呢？",
                "一些不字开头的词语在短句中，为啥被extract_tags后\"不\"字就不见了，将\"不\"与后面词语分开，这样提取的词的意思都变了，不合理吧\n比如\n\n不合适\n不适合\n不太合适\n不太适合\n不满意\n不想买\n不想要",
                "不知道是不是bug\n\n得出结果，通过pip 安装的包为旧版0.39  请作者更新一下",
                ""
            ]
        },
        {
            "time": "Mar 30, 2014",
            "title": "4英寸，7.5ml，这种词是否有办法辨识？",
            "contents": [
                "根据不同的参数名称调用不同的自定义词典和stopwords来分词,自定义词典和stopwords全部初始化加载到内存中,实际分词的时候根据参数来替换dict和stopwords的变量值,这样能实现吗?",
                "No description provided.",
                "我是用正则表达式处理的，new_sentence = re.sub(r'[^\\u4e00-\\u9fa5]', ' ', old_sentence) 然后再进行分词的, \\u4e00-\\u9fa5这个是utf-8中，中文编码的范围",
                "@cbzhuang  非常谢谢你的回复！ 我用了这个，不知道可对。#169",
                "Actually, CJK characters are encoded together so there's no critical range for Chinese characters. A punctuation dict could be used to do the filtering.",
                "关于jieba.load_userdict(dic)这个加载自定义词典，有如下疑问，烦请解答：\n\n加载了自定义词典会不会覆盖默认的词典\n2.如果我有多个txt的自定义词典，可否加载多个，机制是加载的多个并存，还是说后加载的覆盖先加载的？\n3.如问题2，如果是后加载的覆盖先加载的，那么我就需要把多个txt先合并再加载，如果是并存的那么我可以运行多次jieba.load_userdict(dic)来加载多个词典，反正会并存。\n4.之所以有如上的问题，是因为我从搜狗词库里面下载了很多日常用语secl词典\n\n非常感谢，开发人员能解答下。",
                "從這段code來看，他是去呼叫add_word() 方法把字典裡的字一個一個載入，所以應該不存在覆蓋問題，也應該可以多次載入\nhttps://github.com/fxsjy/jieba/blob/master/jieba/__init__.py#L392",
                "load_userdict最后也是调用的是add_word()方法，不会覆盖",
                "在应用时，往往被切分为4, 英寸，7.5, ml",
                "当前我希望获得这种连词的时候，是直接判断量词前是否有数词。有数词自动连接。\n但我觉得这是个dirty trick，需要原生支持。",
                "对于这种量词我所呆过公司的做法一般是将这类词当成特殊词语对待，就像品牌有专门的品牌词典和品牌同义词等。所以量词也被当成一个特殊的环节来用特殊的方法对待。一般是用词典和正则。\n个人愚见，期待更好的回答。\n发自我的 iPhone\n\n在 31 Mar 2014，01:13，\"geekan, FSE(Full StackOverflow Engineer)\" notifications@github.com 写道：\n当前我希望获得这种连词的时候，是直接判断量词前是否有数词。有数词自动连接。\n但我觉得这是个dirty trick，需要原生支持。\n—\nReply to this email directly or view it on GitHub.",
                "这种量词的可能不是很常见，有一类是AT\u0026T、T恤等常见的品牌或者是中英标点符号共同成词，尽管userdict里有这个词，也无法正确分割。",
                "@aszxqw 正则是对的，但会加倍扫描次数\n我猜合并到库里会有比较好的性能",
                "@tuang 这是另外一个问题，我也遇到了。\n你开一个issue？",
                "@tuang 我开了一个issue，你看需要怎么补充一下",
                "@geekan新开的issue在哪？",
                "@tuang #145"
            ]
        },
        {
            "time": "Apr 29, 2015",
            "title": "想拆出一个带有中文和英文的词",
            "contents": [
                "输入： hello world\n输出：\n……        start           end\nhello      0                5\nworld     5                10\n这个offset位置标识其实忽略了原文中 hello 和 world 中的空格位置。这样在做高亮显示场景时是错误的。",
                "请问有没有带标点符号的词语切分方法，比如我需要保留逗号切割“智者千慮，必有一失”，有没有可能完整保留逗号？\n已经试过自定义词典和add单个词语，似乎效果并不明显。\n请问大家有没有什么好的想法？",
                "可以考虑一下最大匹配算法",
                "类似的问题还有，如何保持空格分隔的词，比如\"Steve Jobs\".",
                "我想把“超敏C反应蛋白”作为一个词拆分，我用了add_word和suggest_freq方法，但是仍然被拆开了，这种词的拆分能实现吗？",
                "我测试 add_word 是可以的啊？不知道是不是我理解的不对？\n\u003e\u003e\u003e list(jieba.cut(\"超敏C反应蛋白是什么？\"))\n['超敏', 'C', '反应', '蛋白', '是', '什么', '？']\n\u003e\u003e\u003e jieba.add_word(\"超敏C反应蛋白\")\n\u003e\u003e\u003e list(jieba.cut(\"超敏C反应蛋白是什么？\"))\n['超敏C反应蛋白', '是', '什么', '？']",
                "你理解的是对的，真心感谢，因为这类的词比较多，我是用python读文件的形式挨个添加的，不知道是不是这个出了问题导致的",
                "您好，谢谢您的回答，但是我刚才试了一下，我这里还是不行，不知道你是用的什么版本，我这个是linux系统，python环境，O(∩_∩)O~\n张柳-云平台研发部\n发件人： wangbin\n发送时间： 2015-04-29 10:42\n收件人： fxsjy/jieba\n抄送： liuliu818\n主题： Re: [jieba] 想拆出一个带有中文和英文的词 (#253)\n我测试 add_word 是可以的啊？不知道是不是我理解的不对？\n\n\n\nlist(jieba.cut(\"超敏C反应蛋白是什么？\"))\n['超敏', 'C', '反应', '蛋白', '是', '什么', '？']\njieba.add_word(\"超敏C反应蛋白\")\nlist(jieba.cut(\"超敏C反应蛋白是什么？\"))\n['超敏C反应蛋白', '是', '什么', '？']\n\n\n\n—\nReply to this email directly or view it on GitHub.\nConfidentiality Notice: The information contained in this e-mail and any accompanying attachment(s)\nis intended only for the use of the intended recipient and may be confidential and/or privileged of\nNeusoft Corporation, its subsidiaries and/or its affiliates. If any reader of this communication is\nnot the intended recipient, unauthorized use, forwarding, printing,  storing, disclosure or copying\nis strictly prohibited, and may be unlawful.If you have received this communication in error,please\nimmediately notify the sender by return e-mail, and delete the original message and all copies from\nyour system. Thank you.",
                "我也是 Linux 系统， Python 3.4.3， jieba 0.36.2"
            ]
        },
        {
            "time": "Mar 28, 2016",
            "title": "jieba.cut()和jieba.posseg.cut()的分词结果不一致？",
            "contents": [
                "比如要分词的字符串：http://192.168.0.121/login.view\n因为：re_num = re.compile(\"[.0-9]+\")这样写，\n分词的结果为：\nlogin eng\n. m\nview eng\n\".\"被判断成数字，建议修改一下这个正则",
                "这个“.”  很有必要被当做数字，信我。",
                "单个的英文单词不能通过取关键词取到吗？类似于c这个单词，即使把c加入到自定义的词典也不行啊",
                "tfidf算法抽取，w就是待抽取的单词，如果单词长度小于2,就不处理；\nif len(w.strip()) \u003c 2 or w.lower() in self.stop_words:\n    continue\n\ntextrank算法抽取，只有单词长度大于等于2,才会进行处理；\ndef pairfilter(self, wp):\n    return (wp.flag in self.pos_filt and len(wp.word.strip()) \u003e= 2\n            and wp.word.lower() not in self.stop_words)\n\n两个算法，都会较短的单词（字符数少于2）不进行抽取；",
                "@zhbzz2007 很感谢",
                "对同一段文本使用上述两个函数进行分词，分词结果不一样？\n如：弄正\n在jieba.cut()中分成了”弄“和”正“\n在jieba.posseg.cut()中则保持”弄正“"
            ]
        },
        {
            "time": "Oct 30, 2018",
            "title": "jieba分词是否支持依存句法分析？",
            "contents": [
                "自定义词典中加入以下标点符号的词性标注：\n! 50000 w\n\" 50000 w\n但是jieba无法正确分出词性，统一被识别为非语素x\n\n\n\nwords = pseg.cut('南京\"中山陵\"位于紫金山!')\nfor w in words: print w.word,w.flag\n...\n南京 ns\n\" x\n中山陵 ns\n\" x\n位于 v\n紫金山 nr\n! x",
                "如题",
                "re_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#\u0026._]+)\", re.U)\n既然对C++, C#多了特判, 为什么不加入 objective-c特判",
                "如题，多谢！"
            ]
        },
        {
            "time": "Sep 21, 2018",
            "title": "使用 add_word 和 suggest_freq 后，词语还是被拆分",
            "contents": [
                "如何解决向字典添加“leap motion”这种中间带空格的关键词",
                "@microcao , 目前不行，得改代码。 词典里面的分割符就是空格，此外还要改一处代码里面的正则表达式，把空格也作为成词的合法字符。",
                "@fxsjy 能帮忙搞一下带空格的英文单词的实现吗？感谢！",
                "同问。@fxsjy",
                "@geekan 这会带来另一个问题。如果“leap motion”作为一个词的话，那么就要考虑：搜索引擎模式下leap和motion怎样切分出来？",
                "@anderscui 不了解这里的逻辑，不过这个看起来像是一个通用的问题：\n\n长短词同时存在时，搜索引擎模式能否全切分？我觉得是必须要全切分的。",
                "@geekan 对，所以我在考虑，能否先按照现有方式切分，然后根据一个配置文件将包含空格的词merge，而不是把这样的词直接加入自定义词典。",
                "@anderscui 不是很明白，为啥要单独定义配置文件呢？\n是因为搜索引擎模式对空格有特殊处理吗？",
                "jieba.add_word(\"石墨烯\",100,\"nr\")\njieba.add_word(\"凱特琳\",100,\"nr\")\njieba.add_word(\"莫那娄氏\",10,\"n\")\njieba.del_word(\"自定义词\")\n不添加词典，利用add_word和del_word仍然不起作用\n先开启并行，不起作用。如果先添加词，再进行并行就可以。。。。还真是bug\njieba.enable_parallel()",
                "#547",
                "以下示例中 words 是默认词典中所有的词，feqs 是对应的词频：\nfreqs[words.index('北京市')]  # 3392\nfreqs[words.index('人民政府')]  # 15227\nfreqs[words.index('北京市人民政府')]  # 135\n\nlist(jieba.cut('北京市人民政府发布了一条政策'))\n#  ['北京市人民政府', '发布', '了', '一条', '政策']\n如上，北京市 和 人民政府 的词频要比 北京市人民政府 高出很多，但是 北京市 和 人民政府 却没有被切开，这是为什么呢？",
                "注意源码中calc()函数：\n\ndef calc(self, sentence, DAG, route): \nN = len(sentence)\n\nroute[N] = (0, 0) \n\nlogtotal = log(self.total) \n\nfor idx in xrange(N - 1, -1, -1):\n\n    route[idx] = max((log(self.FREQ.get(sentence[idx:x + 1]) or 1) - logtotal + route[x + 1][0], x) for x in DAG[idx])\n\n\n最后一行的运算，实际上是log(单词词频) - log(全词词频) = log(单词词频 / 全词词频)，也即这个词与所有词词频之比。因此词数多时原概率会多除以一个分母即全词词频，概率上反而变小。此处并非单词词频简单相加。\n个人理解，欢迎交流。",
                "你能以我上面给的例子解释下吗？我看文档上说的是词频高就能被切开。\n多多指教。",
                "假设所有词词频之和为total（比如1000000）。当进行动态规划分词时，当进行“北”字子运算时，比较的是log(“北京市”-\u003e词频 / total) + log(\"人民政府\"-\u003e词频 / total) = log(\"北京市\"-\u003e词频 * “人民政府\"-\u003e词频 / total^2)，和log(\"北京市人民政府\"-\u003e词频 / total)。虽然“北京市”和“人民政府”词频相对高，但由于多除了一个全词词频之和的分母，概率运算的结果实际上是变小了的。据我的理解，实际上分词方法倾向于分割为字典中匹配的尽可能长的词。\n另外，此处用log函数而非乘除法直接运算似乎是为了解决浮点数对于此处概率极小时不够精确的情况。",
                "多谢指教，我再看看",
                "In [94]: s = '乌鲁木齐爱家超市南门店'\n\nIn [95]: jieba.cut(s)\nOut[95]: \u003cgenerator object Tokenizer.cut at 0x10e24fde0\u003e\n\nIn [96]: jieba.lcut(s)\nOut[96]: ['乌鲁木齐', '爱家', '超市', '南', '门店']\n\nIn [97]: jieba.add_word('南门')\n\nIn [98]: jieba.lcut(s)\nOut[98]: ['乌鲁木齐', '爱家', '超市', '南', '门店']\n\nIn [99]: jieba.suggest_freq('南门', True)\nOut[99]: 833\n\nIn [100]: jieba.lcut(s)\nOut[100]: ['乌鲁木齐', '爱家', '超市', '南', '门店']\n\nIn [101]: jieba.lcut(s, HMM=False)\nOut[101]: ['乌鲁木齐', '爱家', '超市', '南', '门店']\n期望结果\n['乌鲁木齐', '爱家', '超市', '南门', '店']",
                "In [94]: s = '乌鲁木齐爱家超市南门店'\n\nIn [95]: jieba.cut(s)\nOut[95]: \u003cgenerator object Tokenizer.cut at 0x10e24fde0\u003e\n\nIn [96]: jieba.lcut(s)\nOut[96]: ['乌鲁木齐', '爱家', '超市', '南', '门店']\n\nIn [97]: jieba.add_word('南门')\n\nIn [98]: jieba.lcut(s)\nOut[98]: ['乌鲁木齐', '爱家', '超市', '南', '门店']\n\nIn [99]: jieba.suggest_freq('南门', True)\nOut[99]: 833\n\nIn [100]: jieba.lcut(s)\nOut[100]: ['乌鲁木齐', '爱家', '超市', '南', '门店']\n\nIn [101]: jieba.lcut(s, HMM=False)\nOut[101]: ['乌鲁木齐', '爱家', '超市', '南', '门店']\n期望结果\n['乌鲁木齐', '爱家', '超市', '南门', '店']\n\n@4ft35t 设置的频率还是太低了，使用jieba.add_word(\"南门\", freq=1000)设置高一点就可以了。"
            ]
        },
        {
            "time": "Dec 2, 2013",
            "title": "请教一下jieba的词典的格式是怎样的？",
            "contents": [
                "\u003ctype 'exceptions.IOError'\u003e [Errno 5] Input/output error   for term in terms:\\n', '  File \"/usr/local/lib/python2.7/dist-packages/jieba/init.py\", line 255, in cut\\n    for word in cut_block(blk):\\n', '  File \"/usr/local/lib/python2.7/dist-packages/jieba/init.py\", line 195, in cut_DAG\\n    DAG = get_DAG(sentence)\\n', '  File \"/usr/local/lib/python2.7/dist-packages/jieba/__init.py\", line 115, in wrapped\\n    initialize(DICTIONARY)\\n', '  File \"/usr/local/lib/python2.7/dist-packages/jieba/init.py\", line 65, in initialize\\n    print \u003e\u003e sys.stderr, \"Building Trie..., from \" + abs_path\\n'\n难道是由于后台运行后，console退出了，导致print出差？",
                "请问：\n结巴分词中用户自定义的字典受默认字典词频的影响是和单个词频的值有关么，例如我自定义词典中增加一个词“汉武帝”，词频为30 ，而默认词典中“汉”和“武帝”的词频都是100，那是不是说我这个词一定会被分成默认字典“汉/武帝”，而不是我自定义词表中的“汉武帝”，如果是这样的话，我后期自定义的字典如果批量加入，我怎么定词频的值合适，还是我需要对一类词进入默认词表中先检查一下，在定义自己的字典，不知道有没有什么好的办法，望回复，感谢！！",
                "我也有这个问题，请问您解决了吗？",
                "我解决了，你在用户自定义词典中不要规定词频，如下\n体外药物释放\n增效剂\n航道整治\n肌腱膜纤维肉瘤癌基因同系物A\n这样就会自动生成一个合适的词频，从而可以把这个词分出来",
                "@我也碰到了类似的问题，使用楼上的@grandmoi 的方法不写词频和词性会报下面这个错误\nIndexError: list index out of range\n不写词性，词频写多高都分不出词\n难不成真的要自己一个个加到字典里……",
                "@grandmoi 我也有这个问题。我的自定义词典中定义了‘国债’，没有设置词频。然后输入‘中国债’，想分出来‘中’、‘国债’。但是实际输出还是‘中国’、‘债’",
                "结巴分词中 自定义词典里面的新词的tfidf是怎么算的呢？",
                "自问自答吧 ， 没有出现的词，idf是 所有词idf的平均是11左右。 tf就是这个句子中这个新词的词频。\n详细分析：http://www.cnblogs.com/zle1992/p/8822832.html",
                "自问自答吧 ， 没有出现的词，idf是 所有词idf的平均是11左右。 tf就是这个句子中这个新词的词频。\n详细分析：http://www.cnblogs.com/zle1992/p/8822832.html\n\n如果是平均值，为什么不是标准平均值，而是有差异呢？",
                "想找一些词典做些训练用，不知道有人使用过没？\n另外网上能找到类似的东西吗？例如名词，形容词词表一类的。",
                "参看搜狗的2006年的输入法词库"
            ]
        },
        {
            "time": "Jun 6, 2018",
            "title": "词性标注不正确",
            "contents": [
                "jieba.lcut('2017年10月5日或2017-10-03或12:21和12点30分还有十二点三十分')\n分出来\n['2017年', '10月', '5日', '或', '2017', '-', '10', '-', '03', '或', '12', ':', '21', '和', '12点', '30分', '还有', '十二点', '三十分']\n如何分成\n['2017年10月5日', '或', '2017-10-3', '或', '12:21', '和', '12点30分', '还有', '十二点三十分']",
                "把这些词汇加入到词典中\n\n发自我的vivo智能手机\n\nsugarZ \u003cnotifications@github.com\u003e编写：\n…\njieba.lcut('2017年10月5日或2017-10-03或12:21和12点30分还有十二点三十分')\n分出来\n['2017年', '10月', '5日', '或', '2017', '-', '10', '-', '03', '或', '12', ':', '21', '和', '12点', '30分', '还有', '十二点', '三十分']\n如何分成\n['2017年10月5日', '或', '2017-10-3', '或', '12:21', '和', '12点30分', '还有', '十二点三十分']\n—\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.",
                "之前也想到了,如果固定的几个还可以,但是非常大量的日期时间格式,没法全部加到字典,这方法太笨了.",
                "@sugarZ，请问lcut可以将“2017年”分词到一起吗，貌似我这边是“2017”，“年”",
                "@JiaWenqi 好像不行,我是加的自定义词典,把最近的年份都添加了进去,还有1-12月0-24点(钟),但是如果把时间也加上就太麻烦了,如果jieba支持特定格式分词配置就好了.",
                "目前想到一个方案，将待分词文本用时间正则进行分割后分段进行分词",
                "比如：“我来到北京清华大学”  分成： 我 / 来 / 到 / 北 / 京 / 清 / 华 / 大 / 学",
                "这种事儿也用分词么？\n\n2017-11-21 20:00 GMT+08:00 jasmineol \u003cnotifications@github.com\u003e:\n…\n 比如：“我来到北京清华大学” 分成： 我 / 来 / 到 / 北 / 京 / 清 / 华 / 大 / 学\n\n —\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n \u003c#552\u003e, or mute the thread\n \u003chttps://github.com/notifications/unsubscribe-auth/ABD5_ejuZPwUklkvnwSV_5J3028wkVO3ks5s4rtUgaJpZM4QluiD\u003e\n .",
                "这个网上随便一搜都是分字的\n比如把中文字分隔转为list",
                "补充一下，c++如何使用jieba分字，涉及到gbk，utf8这种编码的转换，所以想看下jieba有没有现成的",
                "这种直接获取编码就是一个词吧，之前看过汉子转拼音。",
                "设置一个空词典，关闭HMM新词联想，分出来全是一个个字的",
                "No description provided.",
                "我只能告诉你这玩意看得懂空格和标点……（",
                "用 https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl",
                "为什么\"超市\"被标记成动词了(v)",
                "HanLP可以标对"
            ]
        },
        {
            "time": "Oct 12, 2016",
            "title": "在关键字提取中withWeight返回的是权重值，如何返回关键字的词频",
            "contents": [
                "jieba.posseg.cut是可以分割词并得到tag类型，但是却把百分比分割成两个部分，tag都是'x'，jieba.cut就能准确分成\"2%\"，如果需要tag还需要百分比，应该怎么写",
                "同样遇到了这个问题",
                "同问",
                "已解决，见https://blog.csdn.net/liuhongyue/article/details/80498195",
                "No description provided.",
                "么么哒~",
                "withWeight返回的是权重值，如何返回关键字的词频，计数？\n虽然可以通过先分词，再通过关键词计数，但希望直接在\njieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=()) 函数中\n返回关键词出现的次数",
                "TF*IDF"
            ]
        },
        {
            "time": "Jan 6, 2014",
            "title": "大侠，jieba没有词性消歧么？",
            "contents": [
                "请问jieba可放上GAE吗？谢谢。",
                "不是什么问题，主要是把第一次生成的temp文件改一改。\nOn Wed, Oct 2, 2013 at 11:14 PM, vaemon notifications@github.com wrote:\n\n请问jieba可放上GAE吗？谢谢。\n—\nReply to this email directly or view it on GitHubhttps://github.com//issues/109\n.",
                "I think what this issue is saying is \"Can Jieba be used on Google App Engine?\" and jannson's response is \"well, creating the temp files doesn't work\".\nThat's also what I'm finding:\nTraceback (most recent call last):\n  File \"/base/data/home/apps/s~dokibo-dict/20161021t175549.396505411313804719/lib/jieba/__init__.py\", line 123, in initialize\n    self.tmp_dir or tempfile.gettempdir(), cache_file)\n  File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/tempfile.py\", line 45, in PlaceHolder\n    raise NotImplementedError(\"Only tempfile.TemporaryFile is available for use\")\nNotImplementedError: Only tempfile.TemporaryFile is available for use\n\nWhat approach could be used to fix this?",
                "@astromme\nMaybe you can make a patch to gae....tempfile.py\nlike this :  https://github.com/liantian-cn/jieba-gae/blob/master/lib/tempfile_gae.py\nThen just replace import tempfile for import tempfile_gae as tempfile , in the error of the file.\nlike this : https://github.com/liantian-cn/jieba-gae/blob/master/lib/jieba/__init__.py\nIt looks good now : https://jieba.liantian.me/\nnote:\nIf you replace tempfile.py directly, do not use tempfile_gae.py, the development service is not available.",
                "举例：\nsen = u\"我钟爱法国\"\n我/r 钟爱/nr 法国/ns\nsen = u\"我鍾愛法國 \"\n我/r 鍾/nr 愛/v 法國/ns\n有什么方法可以改善么？比如可以把简体字的词库改成繁体字词库么？",
                "改用dict.big.txt的繁體字典後, 並參考我修正的錯誤 #670",
                "发现在cut_all模式下，字符串中的特殊字符不能得到很好的返回，查看代码发现这两种模式下的正则表达式设定是不同的，想请教一下，为何两种模式下，设定的正则是不同的\n测试结果如图所示：\n精确模式\n\n全分割模式",
                "同遇到该问题，怎么回事",
                "大侠，你好，最近一直在研读jieba，好像没发现有词性消歧相关的处理。\n真的没有么？"
            ]
        },
        {
            "time": "Oct 22, 2016",
            "title": "词典分隔符用空格就不能添加含有空格的词了",
            "contents": [
                "可否加入情緒分析功能?\n目前有的情緒辭典有:\nNTU SD\nNTU Sentiment Dictionary\nhttps://docs.google.com/forms/d/e/1FAIpQLSe20EyOE3bp9cKT0gF6R4DodTHOmriIGegkGYa03oHYejhi9g/viewform?c=0\u0026w=1\n大连理工大学信息检索研究室\nhttp://ir.dlut.edu.cn/EmotionOntologyDownload",
                "同问.",
                "在基于TF-IDF进行特征提取时，因为文本背景是某一具体行业，不适合使用通用的IDF语料库，我觉得应该使用自定义的基于该行业背景的IDF语料库。请问如何生成自定义IDF语料库呢？\n我现在有的数据是几十万个该行业的文档，初步想法是：对每个文档分词去重，把所有文档分词结果汇集去重后形成一个分词集，然后对于分词集里的每一个词语w，按**f=log(该行业文档总数/(含有w的文档数量+1))**公式求出词语w的IDF值f，最后txt文件里每一行放一个(w, f)\n是这样吗？谢啦~",
                "还有2个问题：假设通用IDF语料库里有A B C三个词语及其idf值，我自定义IDF语料库里有A B D及其idf值，那么请问，在添加自定义的IDF语料库后：\n\n自定义IDF语料库里的A和B及其相应idf值就直接覆盖通用IDF语料库里的A和B吧？\n通用IDF语料库里原先的C及其idf值，现在还有吗？\n\n（其实问题只有就1个：添加自定义IDF语料库后，是整个文件替换，还是说只有那些重复的词语才被替换？）",
                "求助求助求助，没有朋友知道吗？？？",
                "我也想做一个词库,满足自己的需求,自带的字库里有很多类似一一二/一一分/一三六八之类意义不大的词",
                "line 是单个文档\nall_dict = {}\nfor line in lines:\n    temp_dict = {}\n    total += 1\n    cut_line = jieba.cut(line, cut_all=False)\n    for word in cut_line:\n        temp_dict[word] = 1\n    for key in temp_dict:\n        num = all_dict.get(key, 0)\n        all_dict[key] = num + 1\nfor key in all_dict:\n    w = key.encode('utf-8')\n    p = '%.10f' % (math.log10(total/(all_dict[key] + 1)))",
                "@M2shad0w 非常感谢！还有一个问题：\n假设通用IDF语料库里有A B C三个词语及其idf值，我自定义的IDF语料库里有A B D及其idf值，那么请问，在添加自定义的IDF语料库后：\n\n自定义IDF语料库里的A和B及其相应idf值就直接覆盖通用IDF语料库里的A和B吧？\n通用IDF语料库里原先的C及其idf值，现在还有吗？",
                "@siberiawolf61\n我看了一下 结巴库中 load idf path 的代码\nhttps://github.com/fxsjy/jieba/blob/master/jieba/analyse/tfidf.py#L65\nclass TFIDF(KeywordExtractor):\n\n    def __init__(self, idf_path=None):\n        self.tokenizer = jieba.dt\n        self.postokenizer = jieba.posseg.dt\n        self.stop_words = self.STOP_WORDS.copy()\n        self.idf_loader = IDFLoader(idf_path or DEFAULT_IDF)\n        self.idf_freq, self.median_idf = self.idf_loader.get_idf()\n\n...\n\nself.idf_loader = IDFLoader(idf_path or DEFAULT_IDF)\n\n应该是覆盖了，c值的 idf 也没有了",
                "@M2shad0w 好的，谢谢啊！",
                "感谢！",
                "您好；\n我看到您代码的时候，您的动态规划求频率最大是反向的（也就是从最后一个字符开始算），我自己写了一下，正向的频率最大（也就是从第一个字符开始算）我跑了跑，发现结果是一样的，请问一下这个可以证明吗？还是我操作失误了，还有就是不是说汉语的重点在后面吗？是因为这个用的反向频率最大吗？谢谢",
                "jieba 的词典是用空格作为词，词频，词性之间的分隔符的，但是当要在句子中添加“CH125 Spacy”这种词的时候就会识别不了。\n如果词典使用“\\t”是不是会相对较好？",
                "@rockyzhengwu, @fxsjy 您好, 請問您有解決嗎?我也想要自訂字典中加有空白的英文組合字，但都識別不出來，　都會拆成好幾個字..."
            ]
        },
        {
            "time": "Jul 15, 2014",
            "title": "关于标点符号",
            "contents": [
                "import jieba.analyse 失败 提示：ImportError: No module named 'jieba.analyse'; 'jieba' is not a package\n而在命令行中却可以导入，不知道什么原因\n环境：python3.5",
                "先确定你自己的文件名不叫 jieba.py",
                "好的 谢谢@gumblex",
                "比如遇到本身就有语义或者拼写的错误的中文：咳嗽-\u003e咳嗽等等\n想从语义和词性上通过字典来实现可以吗？",
                "没有的吧，据我使用的经验来说是没有的。\n这里评价体系比较难弄\n据我的使用经验而言我们是自己建立了一套转换体系",
                "你好，我很想知道，jieba如何去除标点符号",
                "纯Python实现，无需jieba\npunct = set(u''':!),.:;?]}¢'\"、。〉》」』】〕〗〞︰︱︳﹐､﹒\n﹔﹕﹖﹗﹚﹜﹞！），．：；？｜｝︴︶︸︺︼︾﹀﹂﹄﹏､～￠\n々‖•·ˇˉ―--′’”([{£¥'\"‵〈《「『【〔〖（［｛￡￥〝︵︷︹︻\n︽︿﹁﹃﹙﹛﹝（｛“‘-—_…''')\n# 对str/unicode\nfilterpunt = lambda s: ''.join(filter(lambda x: x not in punct, s))\n# 对list\nfilterpuntl = lambda l: list(filter(lambda x: x not in punct, l))",
                "楼上的回答看上去好酷炫。",
                "吊到无法直视。谢了！\n------------------ 原始邮件 ------------------\n发件人: \"Yanyi Wu\";notifications@github.com;\n发送时间: 2014年7月19日(星期六) 下午5:46\n收件人: \"fxsjy/jieba\"jieba@noreply.github.com;\n抄送: \"发如雪止\"734133872@qq.com;\n主题: Re: [jieba] 关于标点符号 (#169)\n楼上的回答看上去好酷炫。\n—\nReply to this email directly or view it on GitHub.",
                "我覺得可以幫 jieba 增加一個 jieba.trimPunct(content) 的 method 讓有需要的人可以使用",
                "不好意思，想請問一下類似的問題\n由於我目前在處理網頁資料，爬取下來的內容會有一些雜訊，類似\n\u0026nbsp;和\u0026gt;\n\n我該如何將其濾掉呢？",
                "不好意思，现在才回复，我觉得你可以先做一遍文本过滤再用jieba分词。\n可以先把里面的标点符号过滤掉。\nacemoon0301@163.com\n发件人： 2153030\n发送时间： 2014-12-25 02:06\n收件人： fxsjy/jieba\n抄送： AcemoonMa\n主题： Re: [jieba] 关于标点符号 (#169)\n不好意思，想請問一下類似的問題\n由於我目前在處理網頁資料，爬取下來的內容會有\" \"、\"\u003e\" 等雜訊\n我該如何將其濾掉呢？\n—\nReply to this email directly or view it on GitHub.",
                "@AcemoonMa 不好意思，因為抓取的是html裡的內容，裡面的\" \"、\"\u003e\"是以字串呈現\n空白是 \"\u0026+n+b+s+p+;\" 將加號都去除，然而用gumblex大的方法會將所有的\"\u0026\",\"n\",\"b\",\"s\",\"p\",\";\"都濾掉\n昨天我找到了一個方法，可以濾掉字串裡的字串，應該也可以運用在分詞前的濾掉停用字\nhttp://stackoverflow.com/questions/6116978/python-replace-multiple-strings\nimport re\n\nrep = {\"condition1\": \"\", \"condition2\": \"text\"} # define desired replacements here\n\n# use these three lines to do the replacement\nrep = dict((re.escape(k), v) for k, v in rep.iteritems())\npattern = re.compile(\"|\".join(rep.keys()))\ntext = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)\nFor example:\n\u003e\u003e\u003e pattern.sub(lambda m: rep[re.escape(m.group(0))], \"(condition1) and --condition2--\")\n'() and --text--'\n還沒有實際測試，測試過後跟大家回報",
                "@2153030 HTML 的话最好用 BeautifulSoup 之类的解析库预处理提取字符串（如它的 stripped_strings），再进行分词等自然语言方面的操作。",
                "如果是html解析有很多库都可以提取你需要的text,tag,attribute这些数据啊。\n至于网页本身的架构也是可以获取完整的。\n例如lxml,beautifulsoap以及python自带的库。\n如果你获取后的数据中仍有这些字符，你可以考虑直接写一个字符集合，然后用最基础的循环过滤出来啊。\n或者直接用unicode编码过滤，把除了中文，英文，数字以外的都过滤掉就可以了。\nacemoon0301@163.com\n发件人： 2153030\n发送时间： 2014-12-26 15:36\n收件人： fxsjy/jieba\n抄送： AcemoonMa\n主题： Re: [jieba] 关于标点符号 (#169)\n@AcemoonMa 不好意思，因為抓取的是html裡的內容，裡面的\" \"、\"\u003e\"是以字串呈現\n空白是 \"\u0026+n+b+s+p+;\" 將加號都去除，然而用gumblex大的方法會將所有的\"\u0026\",\"n\",\"b\",\"s\",\"p\",\";\"都濾掉\n昨天我找到了一個方法，可以濾掉字串裡的字串，應該也可以運用在分詞前的濾掉停用字\nhttp://stackoverflow.com/questions/6116978/python-replace-multiple-strings\nimport re\nrep = {\"condition1\": \"\", \"condition2\": \"text\"} # define desired replacements here\nuse these three lines to do the replacement\nrep = dict((re.escape(k), v) for k, v in rep.iteritems())\npattern = re.compile(\"|\".join(rep.keys()))\ntext = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)\nFor example:\n\n\n\npattern.sub(lambda m: rep[re.escape(m.group(0))], \"(condition1) and --condition2--\")\n'() and --text--'\n—\nReply to this email directly or view it on GitHub.",
                "你如果有常用的社交或者通讯软件，你可以发软件名和ID，我加你好友，共同探讨，邮件不太方便。\nacemoon0301@163.com\n发件人： 2153030\n发送时间： 2014-12-26 15:36\n收件人： fxsjy/jieba\n抄送： AcemoonMa\n主题： Re: [jieba] 关于标点符号 (#169)\n@AcemoonMa 不好意思，因為抓取的是html裡的內容，裡面的\" \"、\"\u003e\"是以字串呈現\n空白是 \"\u0026+n+b+s+p+;\" 將加號都去除，然而用gumblex大的方法會將所有的\"\u0026\",\"n\",\"b\",\"s\",\"p\",\";\"都濾掉\n昨天我找到了一個方法，可以濾掉字串裡的字串，應該也可以運用在分詞前的濾掉停用字\nhttp://stackoverflow.com/questions/6116978/python-replace-multiple-strings\nimport re\nrep = {\"condition1\": \"\", \"condition2\": \"text\"} # define desired replacements here\nuse these three lines to do the replacement\nrep = dict((re.escape(k), v) for k, v in rep.iteritems())\npattern = re.compile(\"|\".join(rep.keys()))\ntext = pattern.sub(lambda m: rep[re.escape(m.group(0))], text)\nFor example:\n\n\n\npattern.sub(lambda m: rep[re.escape(m.group(0))], \"(condition1) and --condition2--\")\n'() and --text--'\n—\nReply to this email directly or view it on GitHub.",
                "@gumblex @AcemoonMa 謝謝兩位大大的提示\n這樣看起來似乎讓實作方便些，我去研究一下BeutifulSoup\n@AcemoonMa 郵件已發",
                "楼主 python菜鸟问个简单但急需回答的问题  请问你这个怎么用呢？能写个例子吗？",
                "我想我知道啦  多谢楼主。",
                "import re\n\ndef delete_punctuation(text):\n    \"\"\"删除标点符号\"\"\"\n    text = re.sub(r'[^0-9A-Za-z\\u4E00-\\u9FFF]+', ' ', text)\n    return text\n\n这个满足你的要求吗？"
            ]
        },
        {
            "time": "Apr 1, 2016",
            "title": "添加自定义词典不起作用",
            "contents": [
                "This simple sentence:\n今天下午三点提醒我开会,\nwhen I am using posseg but pos this sentence, got this result:\n[pair('今天下午', 'nr'), pair('三点', 'm'), pair('提醒', 'v'), pair('我', 'r'), pair('开会', 'v')]\n\nObviously, this not we want, 今天下午 should be 't' rather than 'nr'.\nEven I am using user_dict to separate 今天 and 下午， no result.",
                "这个jieba词性烂的很，不建议用，应该是训练数据问题",
                "当然跟分词效果也有很大关系，你这个明显是分词不对，应该是今天/下午",
                "真的有这么烂？？",
                "Is it possible to use the jieba's tokenizer in TfidfVectorizer? [http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html].\nsklearn.feature_extraction.text.TfidfVectorizer asks for a callable tokenizer, and I am wondering which function in jieba can be passed here.\nI would like to cluster or classify some Chinese documents.",
                "jieba.cut()\nor\njieba.cut_for_search()\njieba_1_分词",
                "tfidf_vectorizer = TfidfVectorizer(tokenizer=jieba.cut, lowercase=False, stop_words=stopwords)",
                "在词典中添加a/d,d/a分词的结果仍然是三部分，不能形成一个词，\n权重调了也没有用，估计是特殊字符的问题。",
                "与这个类似：#363"
            ]
        },
        {
            "time": "Sep 25, 2018",
            "title": "添加自定义词 为何切不出来呢",
            "contents": [
                "写了一个程序，使用py2exe制作可执行文件，但是制作完成后，点击执行文件，出现这样的错误，IOError: [Errno 2] No such file or directory: dist\\library.zip\\jieba\\finalseg\\prob_start.py'，而且手动添加这个文件，错误依旧，希望能够得到你的解答，谢谢。",
                "支持这样 cut = jieba.lcut(text.txt) 输入文件的方式吗？试了下，说 NameError: name 'text' is not defined，可这文件同目录下面有的。",
                "lcut 里的是字符串，把文件读进来再用",
                "应该可以通过 添加 用户词典，然后判断词是否在 词典里吧？\n但是添加用的是jieba.load_userdict\n所以如果我要用if word in 这个dict叫什么呢？。。。。",
                "添加了一个dict但是一直乱码啊，好奇怪啊，utf-8格式，别的用同样方法添加的停用词都不会乱码。一旦我想要匹配  输出词和dict就会输出乱码",
                "用这个，定义主词库，jieba.set_dictionary('dict.txt.big')\n自定义的字典，用Notepad++创建，别用win的记事本",
                "这个是添加用户词典里的词，从而可以被识别和区分，但是并不是只输出词典里有的词吧？\n\n\nSent from Mail Master\n在2016年12月20日 19:15，mycrystalgirl 写道:\n\n用这个，定义主词库，jieba.set_dictionary('dict.txt.big')\n自定义的字典，用Notepad++创建，别用win的记事本\n\n—\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.",
                "你都已经定义了主词典，当然所有分词结果都是都主词典抽取的，所以只输出主词典有的词。\n记住几点\n1.记得别用关键词提取，而是用分词功能jieba.cut\n2.关闭HMM\n即便这样，还是匹配出字符串里任何的英文和数字字符串，所以\n3.修改init.py把里面的eng正则破坏掉，我另一个帖子讲的\n我也是刚刚用结巴分词，需求跟你的一样，才摸索出的。",
                "你说需要破坏init.py\n“”修改init.py把里面的eng正则破坏掉，我另一个帖子讲的“”\n能不能把你的帖子地址分享一下。我也遇到了非常类似的问题。谢谢",
                "HMM=false",
                "jieba.add_word(\"β细胞\", freq=True, tag='n')\na = '揭晓！胰岛β细胞功能可以恢复吗?'\nWORD = jieba.lcut(a,cut_all=False)\nprint ([word for word in WORD])\n['揭晓', '！', '胰岛', 'β', '细胞', '功能', '可以', '恢复', '吗', '?']",
                "freq的值设置高一点？",
                "freq的值设置高一点？\n\n谢谢，查到了 好像是和特殊字符有关 修改了源代码"
            ]
        },
        {
            "time": "Apr 3, 2014",
            "title": "AT\u0026T和T-shirt这类词无法识别，也无法通过加字典来解决",
            "contents": [
                "请问打开新词发现后，发现的新词可以导出吗？",
                "同问，pynlpir有一个get_newword用法，可以print出新词，结巴有没有类似功能？",
                "textrank中的rank()方法参考了pagerank的思路，并且在程序中iterate 10次。但是对于jieba案例，可否考虑把阻尼因子设定为1，基于这一点，可以推断出每一个节点的weight就是这个点的连线数，没有必要iterate 10反复迭代了。修改方法如下：\n    for n, out in self.graph.items():\n        ws[n] = wsdef\n        outSum[n] = sum((e[2] for e in out), 0.0)\n\n    # this line for build stable iteration\n    # sorted_keys = sorted(self.graph.keys())\n    # for x in xrange(10):  # 10 iters\n    #     for n in sorted_keys:\n    #         s = 0\n    #         for e in self.graph[n]:\n    #             s += e[2] / outSum[e[1]] * ws[e[1]]\n    #         ws[n] = (1 - self.d) + self.d * s\n    ws=outSum\n    (min_rank, max_rank) = (sys.float_info[0], sys.float_info[3])",
                "",
                "@fxsjy @tuang\n不知道是否有好的解决方法？",
                "目前除了加自定义词典，还不能自动识别。我先做个记号，纳入开发计划。",
                "AT\u0026T是可以的啊",
                "= =",
                "T恤也可以啊。http://jiebademo.ap01.aws.af.cm/",
                "嗯嗯，对的。打开方式问题。感谢耐心指正！",
                "@fxsjy T-shirt 和 google+ 貌似还识别不了"
            ]
        },
        {
            "time": "May 18, 2018",
            "title": "AttributeError: module 'jieba' has no attribute 'lcut'",
            "contents": [
                "import jieba\nTraceback (most recent call last):\nFile \"\", line 1, in\nFile \"/Users/mengbin/jython2.7.0/Lib/site-packages/jieba/init.py\", line 15, in\nfrom ._compat import *\nImportError: No module named _compat",
                "请问下同义词该怎么处理呢？",
                "遇到了一个奇怪的情况，我在Ubuntu16.04下通过pip安装了jieba0.39，可以正常使用，但后来可能又装了一些第三方的库，然后jieba.lcut就报错了，显示没有这个方法，由于后来装的第三方库比较多，正在生产环境上使用，不能直接卸载，感觉这个问题是因为某个第三方库中引入了jieba旧版本的代码，以至于我现在只要使用jieba，用的旧的代码，但通过pip list查询，jieba的版本还是0.39，请问应该怎样解决？谢谢~",
                "我也遇到这个问题了，我是因为同时装了jieba3k，换成jieba就可以了，jieba.lcut只有在jieba中可以使用",
                "执行python脚本时报错，AttributeError: module 'jieba' has no attribute 'lcut'，代码如下：\n#对句子经行分词，并去掉换行符\ndef tokenizer(text):\n''' Simple Parser converting each document to lower-case, then\nremoving the breaks for new lines and finally splitting on the\nwhitespace\n'''\ntext = [jieba.lcut(document.replace('\\n', '')) for document in text]\nreturn text"
            ]
        },
        {
            "time": "Sep 29, 2016",
            "title": "jieba.load_userdict()的问题",
            "contents": [
                "想要有个词语与词语的关系，用来做分析？或者可以一同开发？请指点下，或我留个邮箱？",
                "词语与词语间关系 是基于词性还是完全不考虑词性呢",
                "基于的，比如钱包，近义词是钱夹，属于：皮革制品，纺织品，配件。。。",
                "@c2h2 几乎不可能的，即使有准确率也不高，因为这问题我觉得还是主要靠人工review维护一个同义词词典。虽然这个应用的需求很强烈。",
                "@c2h2 ， 试一试Google的word2vec ?",
                "在使用 load_userdict() 时，并不会主动关闭文件，导致程序警告\nResourceWarning: unclosed file \u003c_io.BufferedReader name='dict.txt'\u003e\n\n不知道没有关闭文件的意义是什么？还有后续操作嘛？",
                "比如\"中断\" \"满格\" 等会莫名奇妙的识别为ns,ns词性是地名词性，这也太假了",
                "我按照规范建立了自己的字典，因为业务需要，我创建了两个字典，分别名字为a.txt ，b.txt 这两个文件分表包括了两个数据表格里面的数据，自己测试了一下不能如果采用以下的方法加载两个文件，好像不能同时生效\n\n\nimport jieba\njieba.load_userdict('a.txt')\njieba.load_userdict('a.txt')\n\n\n请问结巴能否同时加载两个用户字典呢？？",
                "今天又遇到了问题，我在我的flask web app 中使用了jieba的加载自定义字典功能，然后用下面的命令启动\n\n\ngunicorn -w 4 -p gevent -b 0.0.0.0:9999 --reload run:app\n发现jieba连续不断的吐出下面的提示，我觉得应该是gunicorn开启了多个线程导致了这个问题，我想请教下，该如何解决？\n\n\nloading model from cache /tmp/jieba.cache\nloading model cost 2.44023799896 seconds.\nTrie has been built succesfully.\n[2016-09-29 17:05:37 +0000] [32528] [INFO] Booting worker with pid: 32528\nBuilding Trie..., from /root/py27/lib/python2.7/site-packages/jieba/dict.txt\nloading model from cache /tmp/jieba.cache\nloading model cost 2.28571200371 seconds.\nTrie has been built succesfully.\n[2016-09-29 17:06:06 +0000] [32556] [INFO] Booting worker with pid: 32556\nBuilding Trie..., from /root/py27/lib/python2.7/site-packages/jieba/dict.txt\nloading model from cache /tmp/jieba.cache\nloading model cost 2.27150511742 seconds.\nTrie has been built succesfully.\n[2016-09-29 17:06:10 +0000] [32560] [INFO] Booting worker with pid: 32560\nBuilding Trie..., from /root/py27/lib/python2.7/site-packages/jieba/dict.txt\nloading model from cache /tmp/jieba.cache",
                "gunicorn会fork多个进程，但是jieba是lazy加载词典的。你可以在import jieba后，调用一下jieba.initialize()。 这样就不会多次加载了。",
                "同样也是jieba load_dict的问题，我发现我自己在词典中添加了一个词并设定了参数比如：萌萌哒 50 a，但是使用posseg分词的结果却是 萌萌哒 x，这是版本问题还是其他设定的问题？",
                "@fxsjy\n具体的代码用到了这几个部分\nimport jieba\njieba.initialize()\nimport os\nif os.path.exists('cbi360.txt'):\njieba.load_userdict('cbi360.txt')\nimport jieba.posseg as peg\n其中 cbi360.txt是我的自己的字典，而且我还用刀了jieba.posseg 的方法，请问这个具体的顺序是怎么样的啊？"
            ]
        },
        {
            "time": "Oct 10, 2016",
            "title": "cache文件在分词里起了什么作用？可否有无cache的模式",
            "contents": [
                "jieba.analyse.textrank()方法提取英文文本的关键词没有输出数据，但是jieba.analyse.tfidf()方法则可以处理英文文本。中文文本两个都可以处理。so ,问题是，，，",
                "你看看源代码就知道了，tokenizer和postokenizer的区别。",
                "wiki里“对Python中文分词模块结巴分词算法过程的理解和分析”跳转到黄网。",
                "哈哈哈哈哈哈哈\n\n2017-11-02 11:37 GMT+08:00 kercker \u003cnotifications@github.com\u003e:\n…\n wiki里“对Python中文分词模块结巴分词算法过程的理解和分析”跳转到黄网。\n\n —\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n \u003c#541\u003e, or mute the thread\n \u003chttps://github.com/notifications/unsubscribe-auth/ABD5_fZrOpJSHG1DaFGVx-iT7X4t9A95ks5syTkJgaJpZM4QPIGV\u003e\n .",
                "6666666666老铁网站发出来呀",
                "又骗我点击增加csdn浏览量。。。。。。。。。。。。。。。。。",
                "jieba.0.38\n\n不加载自定义词时，分词正常\n\nfrom jieba import posseg as pseg\npseg.cut(u\"为何 MOTO 在华销售的绝大多数\")\n\n为何/r MOTO/eng 销售/vn\n\n\n加载自定义词时，分词异常\n\nfrom jieba import posseg as pseg\njieba.load_userdict(\"???\")\n\npseg.cut(u\"为何 MOTO 在华销售的绝大多数\")\n\n为何/r M/x OTO/x 销售/vn \n\n自定义词典中含有 OTO",
                "希望在生产环境用结巴，但生产环境对于获取绝对路径、产生cache文件等操作有沙盒控制。想请问cache文件在jieba中是否有不可替代的作用，我可否修改代码做一个不用cache的版本（略降低效率也可）？"
            ]
        },
        {
            "time": "May 16, 2015",
            "title": "求助",
            "contents": [
                "文档中写到，词频省略时使用自动计算能保证分出该词的词频。\n例如，“吉林省延边朝鲜族自治州”，期望分词为“吉林省”和“延边朝鲜族自治州”。添加自定义词典并省略词频，并没有成功分出期望的结果。\nIn [126]: jieba.lcut('吉林省延边朝鲜族自治州',  HMM=False)\nOut[126]: ['吉林省延边朝鲜族自治州']\n请指点！",
                "jieba.del_word('吉林省延边朝鲜族自治州')",
                "@Silencezjl 这样一个一个del_word也不是办法...似乎得完全自定义词典",
                "Does anyone have a list of the POS tags, and what they stand for?  They're supposed to be compatible with ictclas.  Thanks.",
                "想请教tfidf部分是如何进行分词的？能自定义分词字典么，自定义删除一些词汇\ntags = jieba.analyse.extract_tags(content, topK=topK, withWeight=withWeight)",
                "呃 后两个问题文档中就有解答",
                "import jieba.analyse\nFile \"/usr/local/lib/python2.7/dist-packages/jieba/analyse/init.py\", line 7, in \nfrom .textrank import textrank\nFile \"/usr/local/lib/python2.7/dist-packages/jieba/analyse/textrank.py\", line 6, in \nimport collections\nFile \"/usr/lib/python2.7/collections.py\", line 10, in \nfrom keyword import iskeyword as _iskeyword\nImportError: cannot import name iskeyword",
                "参见 #207"
            ]
        },
        {
            "time": "Oct 19, 2016",
            "title": "当数据量大时，extract_tags 方法非常慢",
            "contents": [
                "您好，发现结巴对URL的识别好像不太好，，，，要是从微博中提取出URL这个有办法么？谢谢",
                "识别URL不是分词应该处理的问题呀，使用正则表达式匹配？",
                "就是我有一个给定的关键词库，然后新来一篇文档，从词库里面找出几个词语作为这篇文档的关键词。",
                "目前我就在做类似的工作，效果挺不错。\n你的这种情况，可以这么做。\n\n先将文档按句子切分成多个句子，然后计算关键词库中的每个关键词在这篇文档中的句子集合；\n接着计算关键词库中两两关键词的相似性（可以用Jaccard相似性度量），这样构成了一个相似性矩阵；\n接着对相似性矩阵进行特征分解，然后对特征值进行归一化；\n对归一化的特征值从大到小排序，并累计求和（cumsum），选取前\u003c=0.8的特征值对应的关键词作为这篇文档的关键词\n\n以上是一个基本的版本，直接用，效果一般。因此，需要考虑关键词的tf-idf。我是这么做的，在上面的步骤3时，对特征值进行tfidf加权。最后实验结果很好。\n以上，楼主可以试试。",
                "@MacQing 非常感谢，我试一下。",
                "您好，当我用 jieba.analyse.extract_tags 方法提取关键词时，当数据量比较大的时候，这个方法运行时间太长了，请问有没有并行加速的措施呢？  @fxsjy",
                "同问。"
            ]
        },
        {
            "time": "Mar 11, 2015",
            "title": "How to use jieba in hadoop?",
            "contents": [
                "我在思考如何将爬虫抓取的文本结合jieba工具进行文本分析。\n在“终端”中一段一段地进行分词不大方便，不知道有没有同学做过这个整合？如果没有，请告诉我大概需要些什么，我想试着做一下（我目前只会matlab，哈哈都不好意思说了）\n谢谢～",
                "我用jieba来分析用户的评论，然后提取关键评论短语。",
                "@linkerlin yes~当有大量的用户评论需要统一处理的时候，要怎么做呢？",
                "@zihaolucky 另写一个脚本读取评论，调用jieba分词，作分析，输出结果，用matlab不知道怎么搞，用python很好搞。",
                "我也是用python写个脚本，分析微博内容数据，获取高频词汇\n在 2013年7月31日上午9:33，maxint64 notifications@github.com写道：\n\n@zihaolucky https://github.com/zihaolucky另写一个脚本读取评论，调用jieba分词，作分析，输出结果，用matlab不知道怎么搞，用python很好搞。\n—\nReply to this email directly or view it on GitHubhttps://github.com//issues/86#issuecomment-21835061\n.\n\n\n\nRegards\nBojan Liu\nMy Blog: http://newliu.com",
                "@maxint64 嗯～我是抓取知乎上的内容进行文本分析。抓取工具用的是GoSeeker，不知道两个要怎么配合起来才好。因为到了后面，如果抓取内容多的话，手工一个个来分词就不行了。\nmaxint64 是不是把文本文件读进来，然后返回结果也形成文本文件？我是想这样做，可以指点一下吗？",
                "@zihaolucky , 这个例子就是从一个文件读入文本，然后分词后输出到另外一个文件。 https://github.com/fxsjy/jieba/blob/master/test/test_file.py",
                "GoSeeker 是什么？\n没有Google到。。。",
                "文本分析需要分析哪些内容？\n爬虫是肯定需要的了，推荐你一个python写的爬虫:scrapy\n2013/7/30 Zihao Zheng notifications@github.com\n\n我在思考如何将爬虫抓取的文本结合jieba工具进行文本分析。\n在“终端”中一段一段地进行分词不大方便，不知道有没有同学做过这个整合？如果没有，请告诉我大概需要些什么，我想试着做一下（我目前只会matlab，哈哈都不好意思说了）\n谢谢～\n—\nReply to this email directly or view it on GitHubhttps://github.com//issues/86\n.",
                "@zihaolucky\n没用过GoSeeker，不过基本就是你说的那样做。\n也不一定非要是文本文件，什么格式方便你做分析就用什么格式。",
                "@linkerlin 嗷、是GooSeeker，一个Firefox的插件。",
                "@jannson thanks~似乎是很强大的一个工具。我会静下心来好好看代码的了，老是抗拒也不是个问题啊哈。",
                "@fxsjy 收到！学习一下，谢谢～",
                "我是对nlp刚研究的新手  想先从一段文字中进行分词 从而提取他的教育经历和工作经历.\n目前看来有些效果挺好 有些还一般 ,  应该是组织机构的库还不好.\n在 Jul 31, 2013，9:52，BojanLiu notifications@github.com 写道：\n\n我也是用python写个脚本，分析微博内容数据，获取高频词汇\n在 2013年7月31日上午9:33，maxint64 notifications@github.com写道：\n\n@zihaolucky https://github.com/zihaolucky另写一个脚本读取评论，调用jieba分词，作分析，输出结果，用matlab不知道怎么搞，用python很好搞。\n—\nReply to this email directly or view it on GitHubhttps://github.com//issues/86#issuecomment-21835061\n.\n\n\n\nRegards\nBojan Liu\nMy Blog: http://newliu.com\n—\nReply to this email directly or view it on GitHub.",
                "看来这个工具方便了很多人呐～",
                "@fxsjy 我成功啦～\n不过似乎有点问题。我把这条新闻存到txt文件中\n\n分词结果是这样的：\n\n当我再一次 run test_file.py a.txt 出现这样：\n\n主要是最后一段的问题",
                "@zihaolucky , 你的问题解决了么？ 我没太看懂截图要表示的意思。",
                "@fxsjy 抱歉我没有表达清楚。\n我有这样一则新闻：\n\n我打开'终端“，第一次run test_file.py a.txt的时候得到了log文件的结果是这样的：\n\n随后紧接着再执行一次，才得到了正确的结果：\n\n不知道这是什么问题呢？我必须得执行run test_file.py a.txt两次才能完整地分词。\n这个是我的执行记录：",
                "@zihaolucky ,  可能是文件没有close导致的。你看看利用的test_file.py和这一个一样么？https://github.com/fxsjy/jieba/blob/master/test/test_file.py",
                "import urllib2\nimport sys,time\nimport sys\nsys.path.append(\"../\")\nimport jieba\njieba.initialize()\n\nurl = sys.argv[1]\ncontent = open(url,\"rb\").read()\nt1 = time.time()\nwords = list(jieba.cut(content))\n\nt2 = time.time()\ntm_cost = t2-t1\n\nlog_f = open(\"1.log\",\"wb\")\nfor w in words:\n    print \u003e\u003e log_f, w.encode(\"utf-8\"), \"/\" ,\nprint 'cost',tm_cost\nprint 'speed' , len(content)/tm_cost, \" bytes/second\"\n\n我的错..我是不是把这个tets_file.py的文件替换成现在这个就可以了？\n嗯，确认可以了～谢谢你啊",
                "hi,\n我在jieba.dict.utf8这个字典里面等26556行发现下面的内容\n今天上午 3 nr\n今天下午 3 nr\nnr不是人名的标注吗？ 为什么这俩的标注是nr？",
                "RT",
                "Maybe a Java version of jieba.\n作者：piaolingxue 地址：https://github.com/huaban/jieba-analysis"
            ]
        },
        {
            "time": "May 22, 2015",
            "title": "自带词典中词与自定义词曲中词相同时，出现的问题！！",
            "contents": [
                "你好。\n在 /jieda/init.py 的 gen_trie(f_name) 函数中，目前只是简单地读出所有文件内容，然后以 \\n 切分，然后再对每一行切片。\n这样会有一个问题，当碰到一个空行时，程序就会出错（split(' ') 之后的三个变量赋值会不匹配），并且，这个错误处理起来还比较麻烦（事实上我改了一下默认的字典文件后就出错了，我也找不出字典哪里有问题）。\n我建议是不是对空行处理一下，比如：\ndef gen_trie(f_name):\n    lfreq = {}\n    trie = {}\n    ltotal = 0.0\n\n    with open(f_name, 'rb') as f:\n        for l in f:\n            l = l.decode('utf-8').strip()\n            if not l:\n                continue\n            word, freq, _ = l.split(' ')\n            freq = float(freq)\n            lfreq[word] = freq\n            ltotal+=freq\n            p = trie\n            for c in word:\n                if not c in p:\n                    p[c] ={}\n                p = p[c]\n            p['']='' #ending flag\n\n    return trie, lfreq,ltotal",
                "嗯，可以考虑。不过现在加载词典已经很慢了，我需要测试加上这些条件判断后，加载速度会不会下降。",
                "默认词典直接写成.py文件会不会快一些",
                "@xluer ，现在的机制是第一次从文本加载，然后会用marshal.dump把词典序列化到临时磁盘文件。以后再启动时，会比较dump文件和词典文本文件的时间戳，如果dump文件更新，就不会读取文本文件了。\n为什么用文本文件？-- 最初的想法是便于用于修改词典文件。",
                "很多专有词语中间是有空格的\n例如：OpenGL ES\n词典分隔符用空格不太合理，建议改成\\t",
                "因为多义词的原因，建议支持词典中支持多词性，方便后续设计更复杂的算法",
                "@fxsjy 你好，感谢你给我们这么好用的工具！！\n在使用时，发现一个小问题，不知道怎么去解释。\n描述：\njieba自带词库(dict.txt)中有'云南'，我在自定义词典(user_dict.txt)中也加入'云南'。导入自定义词典，然后分词结果就变成如下这样了：\n\n\n\nfor i in pg.cut('云南旅游之地质公司'):\nprint i.word,i.flag\n\n\n\n云 ns\n南 ns\n旅游 vn\n之 u\n地质 n\n公司 n\n即 '云南‘被拆成单字了。",
                "@bobo1732 , 请问你用的版本是多少？",
                "@fxsjy 版本是:0.36.2\nPS:好速度！！",
                "@fxsjy 貌似我已经发现的类似问题，#210\n“因为add_word这个实现没有去更改已经加载了的词的概率，而total已经发生变化了。”\n是这个原因吗？",
                "上面的#210问题应该已经解决。最新开发版无论有没有加“云南”自定义词，都分成：\n云南旅游 nz\n之 u\n地质 n\n公司 n\n\n删掉“云南旅游”，无论有没有加“云南”自定义词，都分成：\n云南 ns\n旅游 vn\n之 u\n地质 n\n公司 n\n\n不知道是什么问题",
                "@gumblex 问题是：在自定义词典中加入‘云南’后，‘云南’被拆成‘云’、‘南’。\n你用的是哪个版本？",
                "GitHub 上的，我直接用 add_word 测试，与自定义词典等效。可能是自定义词典中其他词的频率影响了“云南”的分词。",
                "@gumblex 可以肯定：不是自定义词典中词频影响问题.\n因为我只要将自定义词典中‘云南’这个词删除后，一切都好了。。",
                "我测试下来，用默认dict.txt，“云南 1000000”能正常分出“云南 ns”；用dict.txt.small，能正常分出“云南 ns”，“云南 1”就分出了“云 ns/南 ns”。所以我觉得问题出在词典上。你有没有手动指定词频？",
                "@gumblex 哦，我自定义词典中的词的词频就是这个词的长度，云南只有 2",
                "那真不幸，能分出“云南”的最低词频是3。请参考 suggest_freq 函数。0.36 版及以上支持自动计算词频，在自定义字典里现在不必（瞎）指定词频了。",
                "@gumblex 哦，基本明白了，谢谢！"
            ]
        },
        {
            "time": "Dec 4, 2013",
            "title": "如何控制精确分词的粒度，尽可能小",
            "contents": [
                "个人感觉拆分开来更合理。\n另外jieba.posseg.cut 可以选择为full mode 或者索引引擎模式吗",
                "同求full mode",
                "No description provided.",
                "比如：\ns = u'''出租 珠江新城 13楼独立90方 2房2 全配套 月8400元''' cut = jieba.cut(s) print ','.join(cut)\n结果是\n出租, ,珠江新城, ,13,楼,独立,90,方, ,2,房,2, ,全,配套, ,月,8400,元\n有可能把13楼、90方、2房、2房2、月8400元给单独划分出来吗？",
                "可以,但需要你自己单独提供词库\n\nlfol \u003cnotifications@github.com\u003e 于2018年9月1日周六 下午4:36写道：\n…\n 比如：\n s = u'''出租 珠江新城 13楼独立90方 2房2 全配套 月8400元''' cut = jieba.cut(s) print\n ','.join(cut)\n\n 结果是\n 出租, ,珠江新城, ,13,楼,独立,90,方, ,2,房,2, ,全,配套, ,月,8400,元\n\n 有可能把13楼、90方、2房、2房2、月8400元给单独划分出来吗？\n\n —\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n \u003c#665\u003e, or mute the thread\n \u003chttps://github.com/notifications/unsubscribe-auth/AaXy0hBiwe3XSNWR8GFIFhy4w56w2teuks5uWkb_gaJpZM4WWIsV\u003e\n .",
                "from pyhanlp import *\n\nsegment = HanLP.newSegment().enableNumberQuantifierRecognize(True)\nsentences = [\n    \"十九元套餐包括什么\",\n    \"九千九百九十九朵玫瑰\",\n    \"壹佰块都不给我\",\n    \"９０１２３４５６７８只蚂蚁\",\n    \"牛奶三〇〇克*2\",\n    \"ChinaJoy“扫黄”细则露胸超2厘米罚款\",\n]\nfor sentence in sentences:\n    print(segment.seg(sentence))\n[十九元/mq, 套餐/n, 包括/v, 什么/ry]\n[九千九百九十九朵/mq, 玫瑰/n]\n[壹佰块/mq, 都/d, 不/d, 给/p, 我/rr]\n[９０１２３４５６７８只/mq, 蚂蚁/n]\n[牛奶/nf, 三〇〇克/mq, */w, 2/m]\n[ChinaJoy/nx, “/w, 扫黄/vi, ”/w, 细则/n, 露/v, 胸/ng, 超/v, 2厘米/mq, 罚款/vi]",
                "請問我們想分詞出來的結果是兩個英文組合字, 在自建詞庫裡有\"Apple book\", 但分詞出來後還是\"Apple\"和 \"book\"分開的兩個字, 請問要如何處理才能達到我們要的結果? 謝謝",
                "我們希望看到的是\"Apple book\", 兩個字組合成的詞, 請各位大俠開導一下,謝謝 @fxsjy",
                "求大神進來看一下唄... @felixonmars  , 好像都沒人進來看???奇怪???",
                "請問這個討論群是不是關了?都沒有人回覆耶????",
                "这个问题得从底层改（最近没人想来填坑的样子",
                "能透過改這個檔案來達成目的嗎?  init.py",
                "可以",
                "@gumblex, @fxsjy  , 請教大俠如何達成? 如何改? 拜託指導一下, 大大的感謝",
                "如何控制精确分词的粒度，尽可能小"
            ]
        },
        {
            "time": "Sep 6, 2018",
            "title": "研究发现，5-HT功能活动降低与抑郁症患者的抑郁心境、食欲减退、失眠、昼夜节律紊乱、内分泌功能紊乱、性功能障碍、焦虑不安、不能应付应激、活动减少等密切相关；而5-HT功能增高与躁狂症的发病有关。",
            "contents": [
                "Traceback (most recent call last):\nFile \"\", line 1, in \nFile \"/opt/anaconda3/lib/python3.6/site-packages/jieba/init.py\", line 119, in wrapped\nreturn fn(*args, **kwargs)\nFile \"/opt/anaconda3/lib/python3.6/site-packages/jieba/init.py\", line 307, in load_userdict\nword, freq = tup[0], tup[1]\nIndexError: list index out of range",
                "运行demo出现这个。。。\nBuilding prefix dict from /Library/Python/2.7/site-packages/jieba/dict.txt ...\nLoading model from cache /var/folders/hv/rnxvykcd4dq98fm44qqh7_sm0000gn/T/jieba.cache\nLoading model cost 0.584 seconds.\nPrefix dict has been built succesfully.\nTraceback (most recent call last):\nFile \"test_userdict.py\", line 6, in \njieba.load_userdict('userdict.txt') #载入字典及字典文件名称\nFile \"/Library/Python/2.7/site-packages/jieba/init.py\", line 381, in load_userdict\nf.name, lineno, line))\nValueError",
                "使用：\njieba.add_word(\"5-HT功能\", freq=True, tag='n')\njieba.add_word(\"食欲减退\", freq=True, tag='n')\njieba.add_word(\"昼夜节律紊乱\", freq=True, tag='n')\n然后分词处理：\nprint(\"/\".join(jieba.cut(sta, HMM=True)))\n发现结果为：\n研究/发现/，/5/-/HT/功能/活动/降低/与/抑郁症/患者/的/抑郁/心境/、/食欲减退/、/失眠/、/昼夜节律紊乱/、/内分泌/功能/紊乱/、/性功能\n障碍/、/焦虑不安/、/不能/应付/应激/、/活动/减少/等/密切相关/；/而/5/-/HT/功能/增高/与/躁狂症/的/发病/有关/。\n预计结果是：\n研究/发现/，/5-HT功能/活动/降低/与/抑郁症/患者/的/抑郁/心境/、/食欲减退/、/失眠/、/昼夜节律紊乱/、/内分泌/功能/紊乱/、/性功能\n障碍/、/焦虑不安/、/不能/应付/应激/、/活动/减少/等/密切相关/；/而/5-HT功能/增高/与/躁狂症/的/发病/有关/。\n“5-HT功能”，为什么没有用？",
                "因为有“-”，所以对两边的数据进行了切分",
                "那怎么解决这个问题。因为有些专业名里面包含这种“-”\n\n\n\nliwy@futongdf.com.cn\n\n发件人： LuoZe\n发送时间： 2018-09-06 21:22\n收件人： fxsjy/jieba\n抄送： lwy1111111; Author\n主题： Re: [fxsjy/jieba] 研究发现，5-HT功能活动降低与抑郁症患者的抑郁心境、食欲减退、失眠、昼夜节律紊乱、内分泌功能紊乱、性功能障碍、焦虑不安、不能应付应激、活动减少等密切相关；而5-HT功能增高与躁狂症的发病有关。 (#666)\n因为有“-”，所以对两边的数据进行了切分\n—\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.",
                "from pyhanlp import *\n\nCustomDictionary.insert('5-HT功能', '医疗术语 10')\nprint(HanLP.segment('研究发现，5-HT功能活动降低与抑郁症患者的抑郁心境'))\n[研究/vn, 发现/v, ，/w, 5-HT功能/医疗术语, 活动/vn, 降低/v, 与/cc, 抑郁症/nhd, 患者/n, 的/ude1, 抑郁/a, 心境/n]",
                "谢谢，但是我问的是结巴的解决方法！",
                "谢谢，但是我问的是结巴的解决方法！\n\n之前有帖子写了，特殊字符，可以考虑修改\njieba.re_han_default = re.compile(r'([\\u4e00-\\u9fa5a-zA-Z0-9+#\u0026._%/-]+)', re.UNICODE)"
            ]
        },
        {
            "time": "Mar 13, 2018",
            "title": "使用词典一定能把词典中的词分开吗？",
            "contents": [
                "一个很奇怪的问题\n我的代码如下：\nimport sys\nreload(sys)\nsys.setdefaultencoding(\"utf-8\")\nsys.path.append(\"../\")\nimport jieba\njieba.setLogLevel(60)\njieba.load_userdict(\"dict.txt\")\nimport jieba.analyse\ntest_sent = sys.stdin.read()\ntags = jieba.analyse.extract_tags(test_sent, topK=6)\nprint \",\".join(tags)\n保存为 test.py\n在 用户根目录执行的时候 执行成功，成功提取出结果\n我将test.py 和 dict.txt 复制到 /data/www/目录下执行 就会报错\nTraceback (most recent call last):\nFile \"./keyword.py\", line 9, in \nimport jieba.analyse\nFile \"/usr/local/lib/python2.7/dist-packages/jieba/analyse/init.py\", line 9, in \nfrom textrank import textrank\nFile \"/usr/local/lib/python2.7/dist-packages/jieba/analyse/textrank.py\", line 5, in \nimport collections\nFile \"/usr/lib/python2.7/collections.py\", line 10, in \nfrom keyword import iskeyword as _iskeyword\nFile \"/data/www/spider/keyword.py\", line 12, in \ntags = jieba.analyse.extract_tags(test_sent, topK=6)\nAttributeError: 'module' object has no attribute 'analyse'\n我安装jieba的时候 使用 pip安装\npip install jieba\n真的搞不懂原因了，求解",
                "你的 keyword.py (原 test.py) 与标准库重名了\n./keyword.py 引用 jieba.analyse 引用 textrank 引用 collections 引用 keyword，然后本目录的模块优先搜索，就用你的 keyword.py 了。\n解决方法：改名",
                "谢了我之前的时候 经过测试发现确实是这个问题。。非常感谢",
                "能否对已经分词的文本单独进行词性标注呢",
                "照 jieba 的做法实现一个：\nimport re\n\nre_skip_internal = re.compile(\"(\\r\\n|\\s)\")\nre_num = re.compile(\"[\\.0-9]+\")\nre_eng = re.compile(\"[a-zA-Z0-9]+\")\n\nclass POSSimpleTagger:\n\n    def __init__(self, dictfile):\n        self.word_tag_tab = {}\n        with open(dictfile, \"rb\") as f:\n            for lineno, line in enumerate(f, 1):\n                try:\n                    line = line.strip().decode(\"utf-8\")\n                    if not line:\n                        continue\n                    word, _, tag = line.split(\" \")\n                    self.word_tag_tab[word] = tag\n                except Exception:\n                    raise ValueError(\n                        'invalid POS dictionary entry in %s at Line %s: %s' % (f_name, lineno, line))\n\n    def tagpos(self, words, HMM=True):\n        for w in words:\n            if re_skip_internal.match(w):\n                yield (w, 'x')\n            elif re_num.match(w):\n                yield (w, 'm')\n            elif re_eng.match(w):\n                yield (w, 'eng')\n            else:\n                yield (w, self.word_tag_tab.get(w, 'x'))\n注意字符串要用 unicode(py2), str(py3)。",
                "使用suggest_freq或者add_word、load_userdict是否能保证词典中的词一定分出来呢？因为有很多医疗术语，如果不能保证的话，感觉会有很多词会分错。\n已经有个词典，能否优先根据词典先按照正向最大匹配（先匹配词典中最长的词，再考虑长度短的词），然后再把词典中没有的词进行分词呢？\n@fxsjy"
            ]
        },
        {
            "time": "Jan 12, 2016",
            "title": "在一个句子中出现两个相同的组合的词",
            "contents": [
                "大工程啊 现在没有好用的中文引擎",
                "我也想问这个问题，Coreseek是基于sphinx的。。",
                "那几个分词太让人纠结了",
                "@fxsjy 老大考虑下",
                "基于 TextRank 算法的关键词抽取\njieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。\njieba.analyse.TextRank() 新建自定义 TextRank 实例\n算法论文： TextRank: Bringing Order into Texts\n基本思想:\n将待抽取关键词的文本进行分词\n以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图\n计算图中节点的PageRank，注意是无向带权图\n我在使用textrank的时候想调整滑动窗口为2,上面说窗口大小默认为5,但找不到span在哪里,求帮助!",
                "搞掂了,原来要新建一个对象,然后改属性,例如:\ntext = jieba.analyse.TextRank()\ntext.span = 2",
                "No description provided.",
                "比如某个句子的分词效果，结巴的结果和预期有出入\n我想把人工标注的结果加入到结巴，如何？",
                "例如这样一句话：\n我要听我要这首歌\n我把 我要 这个词加到自定义词典中，但是开始的我和要也会 切分成 我要,\n我想要的是 我/要/听/我要/这首歌这个要如何处理？"
            ]
        },
        {
            "time": "Mar 31, 2016",
            "title": "是否可以用清华THULAC使用的语料库来训练jieba的模型？",
            "contents": [
                "抽取句子中的序列规则，因为词性标注得太细的原因，很多规则冗余，请问可以修改源码的哪一部分？或者直接有这个功能么？",
                "如果我输入 “15路公交” 或“150平米以上的房子”\n现在的切割是 15/公交 150/平米/以上/的/房子\n我自定义词典里词频调成 0 或100都不行，谢谢",
                "同问\n能不能加入正则规则？",
                "能否考虑，先切分再用某种方法合并？",
                "清华最近放出来的THULAC看起来很赞的样子，文档中提到了训练模型所用的语料库，官方团队的资源获取能力实在是太厉害了。不知道可否和清华方面沟通一下，用他们的语料库来训练jieba的模型。\nTHULAC\n\n1.THULAC工具包提供的模型是如何得到的？\nTHULAC工具包随包附带的分词模型Model_1以及分词和词性标注模型Model_2是由人民日报语料库训练得到的。这份语料库中包含已标注的字数约为一千二百万字。\n同时，我们还提供更复杂、完善和精确的分词和词性标注联合模型Model_3和分词词表。该模型是由多语料联合训练训练得到（语料包括来自多文体的标注文本和人民日报标注文本等）。这份语料包含已标注的字数约为五千八百万字。由于模型较大，如有机构或个人需要，请填写“doc/资源申请表.doc”，并发送至 thunlp@gmail.com ，通过审核后我们会将相关资源发送给联系人。",
                "😄 说实话并没有觉得很赞……"
            ]
        },
        {
            "time": "Apr 3, 2018",
            "title": "3只鸡 被分成了 [3,只鸡]",
            "contents": [
                "这个问题是出现在“全模式”下（cut_all=True），而在“精确模式”下则没有上面的问题。\n我在全模式下输出分词结果，发现list列表前后多了两个空值。",
                "Fixed. please check:   90ab511",
                "@jaffwu , 另外，jieba已经内置了和whoosh的集成功能。 请看：https://github.com/fxsjy/jieba/blob/master/test/test_whoosh.py",
                "无论是否翻墙，均无法访问，也ping不到",
                "@Pzoom522 这个链接已经崩了很久了。",
                "数词+量词+单字名词 很多都有这个问题……",
                "jieba是默认把字母与数字同汉字分开的"
            ]
        },
        {
            "time": "Mar 1, 2017",
            "title": "jieba 分词 extract_tags 遗漏分词结果",
            "contents": [
                "现在cut方法有了词性标注，但cut_for_search方法好像没有词性标注。我现在在用Jieba做搜索引擎，想要个词性标注，以便获取不同的词性，不知道能否添加上？另外，能否有份关于词性说明的文档，即说明每个词性符号代表着哪种词性？",
                "我也觉得应该返回词性标注，因为有时词性是很重要的。最好再加个方法来获取单个词的词性。 @xgfone 要找的词性意思，可以去 中科院的分词网站去下载 《ICTPOS3.0汉语词性标记集》我已经下载了一份，很是全面。",
                "因为ICTCLAS的旧文档已经不知去向。\n而且ICT POS 3.0已经不同了。\n谢谢\n我有找到过一个网站的说明，不知上面所说的关于jieba分词的词性标注集是否还适用\n\nhttp://www.hankcs.com/nlp/corpus/several-revenue-segmentation-system-used-set-of-source-tagging.html",
                "for i in jieba.analyse.extract_tags('我爱上海的水'):\nprint i\n这个例子里面只能输出上海\n我，爱，水，都没了？ 请问为什么呀",
                "seg_list = jieba.cut('我爱上海的水',cut_all=True)\nprint(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式"
            ]
        },
        {
            "time": "Mar 21, 2018",
            "title": "千与千寻和浪客剑心居然是成语",
            "contents": [
                "请问dict.txt 词库是否都来自搜狗的2006版免费词库? #24",
                "No description provided.",
                "你没看代码吗？",
                "哎，作者就是不爱写注释，哎。",
                "@ilqxejraha 谢谢您，可能是我表达有误，我是想问词汇库里面的词汇和词频是人工统计的吗？还是通过其它的方法。",
                "统计过来的。最后得到了就是这么一个模型。",
                "你在源码中看到词频的使用了吗？",
                "具体的统计词频，词频的作用可能体现在，一个词存在多个意思。\n比如英语中，经常有一个词会有很多个意思。\n比如出现一个词，并且，这个词有很多种解释，这时候词频可能会对词意的选择有一定帮助。\n具体的其他算法hmm的我还没看。",
                "@KevinDotW 据说是基于人民日报的语料库，我也想知道怎么才能创建自己的词典",
                "使用jieba.pooseg.cut发现千与千寻和浪客剑心的词性标记都为i（成语）。我将信将疑的查了下发现这两个词都仅作为动漫/电影名词出现。\n查看后发现，dict.txt中千与千寻和浪客剑心的标记都是3 i。我认为改成3 n会不会比较合适呢？毕竟dict.txt中大部分动漫名词都被标记为n。"
            ]
        },
        {
            "time": "Feb 8, 2014",
            "title": "头发长长了，变成了长头发。",
            "contents": [
                "No description provided.",
                "用 del_word('男性') 直接将“男性”这个词删掉？",
                "每一次提取一句話的關鍵詞的速度平均都在6s左右如何優化這個過程",
                "错误输出例：\n\u003e\u003e\u003e list(pseg.cut('你们啊，naïve！'))\n[pair('你们', 'r'), pair('啊', 'zg'), pair('，', 'x'), pair('na', 'eng'), pair('ï', 'x'), pair('ve', 'eng'), pair('！', 'x')]\n\u003e\u003e\u003e list(pseg.cut('傻“bī”的“bī”，究竟是什么字？'))\n[pair('傻', 'a'), pair('“', 'x'), pair('b', 'x'), pair('ī', 'x'), pair('”', 'x'), pair('的', 'uj'), pair('“', 'x'), pair('b', 'x'), pair('ī', 'x'), pair('”', 'x'), pair('，', 'x'), pair('究竟', 'd'), pair('是', 'v'), pair('什么', 'r'), pair('字', 'n'), pair('？', 'x')]\n要对范围有个大概感觉的话，可以看看 https://en.wikipedia.org/wiki/Latin_script_in_Unicode 里面长得不像符号的东西。",
                "提交一个小bug\ndemo的分词结果是：\n头发/n 长长/n 了/ul ，/x 变成/v 了长/v 头发/n 。/x\n结果好像不是太好。\n请问是否能用自定义词库或者什么方便纠正？"
            ]
        },
        {
            "time": "Aug 2, 2017",
            "title": "AttributeError: 'int' object has no attribute 'decode'",
            "contents": [
                "如圖所示\n\n我是用簡體中文維基百科的語料，經過處理之後把符號換成 space，再將整個文本當成一個當成一個 string\n餵進去。\n下面是我的代碼\nimport time\n\nstart_time = time.time()\n\njieba.enable_parallel(16)\njieba_words = {word for word in jieba.cut_for_search(text_processed, HMM=True) if len(word) \u003e 1 and len(word) \u003c= 7}\n\nprint(\"Segmentizing took {} seconds.\".format(time.time() - start_time))\n\n謝謝幫忙！",
                "上图是我的数据前一部分，我的目的是对 titles 一列进行分词，分词的代码如下。现在遇到的问题是AttributeError: 'int' object has no attribute 'decode'，所以我认为是 titles 中有 int 所致，所以添加了一个判断条件，但是代码执行的结果依旧是报之前的错。请问这是什么原因？\ndef jiebait(text):\n    seglist = jieba.cut(text, cut_all = True)\n    fenci = []\n    for word in seglist:\n        if (not isinstance(word, int)) and (len(word) \u003e= 2):\n            fenci.append(word)\n    # 如用搜索引擎模式：\n    #seglist = jieba.cut_for_search(text)\n    return ' '.join(fenci)",
                "def jiebait(text):\n    seglist = [str(w) for w in jieba.cut(text, cut_all = True)]\n    fenci = []\n    for word in seglist:\n        if (not isinstance(word, int)) and (len(word) \u003e= 2):\n            fenci.append(word)\n    # 如用搜索引擎模式：\n    #seglist = jieba.cut_for_search(text)\n    return ' '.join(fenci)",
                "AttributeError                            Traceback (most recent call last)\n in ()\n2     result_line = \"\"\n3     # segment\n----\u003e 4     seg_list = [str(w) for w in jieba.cut(line, cut_all = False)]\n5     # remove special character\n6     temp = re.sub(\"[\\s+.!_,$%^(+\"')]+|[+——()?【】“”！，。？、~@#￥%……\u0026（）]+\", \"\",(\"/\".join(seg_list)))\n in (.0)\n2     result_line = \"\"\n3     # segment\n----\u003e 4     seg_list = [str(w) for w in jieba.cut(line, cut_all = False)]\n5     # remove special character\n6     temp = re.sub(\"[\\s+.!_,$%^(+\"')]+|[+——()?【】“”！，。？、~@#￥%……\u0026（）]+\", \"\",(\"/\".join(seg_list)))\nD:\\Anaconda\\lib\\site-packages\\jieba_init_.py in cut(self, sentence, cut_all, HMM)\n280             - HMM: Whether to use the Hidden Markov Model.\n281         '''\n--\u003e 282         sentence = strdecode(sentence)\n283\n284         if cut_all:\nD:\\Anaconda\\lib\\site-packages\\jieba_compat.py in strdecode(sentence)\n35     if not isinstance(sentence, text_type):\n36         try:\n---\u003e 37             sentence = sentence.decode('utf-8')\n38         except UnicodeDecodeError:\n39             sentence = sentence.decode('gbk', 'ignore')\nAttributeError: 'int' object has no attribute 'decode'",
                "你的 text 本身是不是 int",
                "是的，本身是int，但是seg_list = [str(w) for w in jieba.cut(line, cut_all = False)]这一步，应该都转成string了",
                "解决了 我这边数据转码的问题 不好意思。。。感谢"
            ]
        },
        {
            "time": "Mar 5, 2016",
            "title": "如何自定义IDF文件",
            "contents": [
                "No description provided.",
                "字典是中文分词的核心数据。分词的好坏，往往由字典的正确率和是否与时俱进决定。设想中，应该有这么一个机制，允许Jieba的使用者一起来贡献字典的更新。\n1、比如，让Jieba的垂直行业用户，可以把针对他们的行业已经tain过的字典上传上来，分享给大家\n2、再比如，Jieba自己的字典，是否可以通过对于当前一段时间互联网的最新热词/短语的流行潮流进行自我更新？",
                "单开一个 repo，专门存放分享上来的字典？\n通过 pull request 来分享、更新\n不过这样要有人来维护这个 repo，要确保字典质量，要花精力的啊",
                "@anjianshi 众筹词典？",
                "众筹是大家出钱然后由某个人 / 组织负责提供、维护字典吗？不知道好不好操作 :)",
                "导入自定义词库可以满足个人需求。",
                "在抽取关键词 时，需用到IDF_big 文件，请根据需要用到的语料库，生成自己的IDF文件，用于计算TFIDF"
            ]
        },
        {
            "time": "Oct 1, 2018",
            "title": "请问如何定义如“建设银行”=“建行”这类，以便在统计或者模糊匹配的时候被定义为同一个单词？",
            "contents": [
                "我想知道jieba里初始的dict是如何生成的？怎么统计的词频？得到dict的过程是从语料中从零开始训练生成，还是说也是借助了新华字典这种初始的字典作为辅助原料？还有就是词频是如何进行统计的？",
                "#393",
                "同问",
                "问题1：\n在jieba首页说明中显示自定义词典词频和词性不必须，但是我使用textrank，更改词典会报错！\n请问，二者是必须的吗？\n问题2：\n词频和词性对于textrank算法有何影响？",
                "代码入下：\ncontent=\"环境设施都很好，服务周到！\"\naspect_content_array = jieba.analyse.extract_tags(content, topK=50)\nprint \",\".join(aspect_content_array)\n抽取结果：\n服务周到,设施,环境\n我想得到“好”这个关键词，但是抽取不出来\n另外能否在抽关键词的时候，把“服务周到” 抽取成 “服务”和“周到”两个词",
                "建议为jieba.posseg增加cut_all选项，允许采用全模式分词状态下再进行词性标注以得到更全的结果",
                "我也需要啊，现在还没有这个api？",
                "rt"
            ]
        },
        {
            "time": "Oct 20, 2018",
            "title": "TFIDF算法要取多少topK",
            "contents": [
                "代码如下：\n#encoding=utf-8\nimport jieba\nimport jieba.posseg as posseg\n\njieba.load_userdict(\"/data/paper/keywords/user_dict.txt\")\n\nwords = posseg.cut(\"在现代社会中，免洗消毒液的使用越来越普遍，消毒液中所含的肉豆蔻酸异丙酯、丙二醇和乙醇等成分可增强皮肤的渗透性。\")\n\nfor w in words:\n    print('%s %s' % (w.word, w.flag))\n\n报错\nBuilding prefix dict from /usr/lib/python2.7/site-packages/jieba/dict.txt ...\nLoading model from cache /tmp/jieba.cache\nLoading model cost 0.296606063843 seconds.\nPrefix dict has been built succesfully.\nTraceback (most recent call last):\nFile \"./jieba-test.py\", line 5, in \njieba.load_userdict(\"/data/paper/keywords/user_dict.txt\")\nFile \"/usr/lib/python2.7/site-packages/jieba/init.py\", line 119, in wrapped\nreturn fn(_args, *_kwargs)\nFile \"/usr/lib/python2.7/site-packages/jieba/init.py\", line 312, in load_userdict\nadd_word(_tup)\nFile \"/usr/lib/python2.7/site-packages/jieba/init.py\", line 116, in wrapped\nreturn fn(_args, **kwargs)\nTypeError: add_word() takes at most 3 arguments (4 given)\n之前还工作的好好的。user_dict.txt 类似于这样的：\n酞 10 n\n 酞酐 20 n\n 酞磺胺噻唑 50 n\n 酞磺醋胺 40 n\n 酞基 20 n\n 酞己炔酯 40 n\n 酞腈 20 n\n 酞菁 20 n\n 酞菁二(吡啶)铁(II)络合物 150 n\n 酞菁镉 30 n\n 酞菁蓝 30 n\n 酞菁锂 30 n\n 酞菁镁 30 n\n 酞菁染料 40 n\n 酞菁素艳蓝 50 n\n 酞菁锡(II) 70 n\n 酞菁亚铁 40 n\n 酞菁铟 30 n\n 酞菁银 30 n\n 酞类染料 40 n\n 酞嗪 20 n\n 酞醛 20 n\n 酞酸 20 n\n 酞酸丁苄酯 50 n\n 酞酸二苯酯 50 n\n 酞酸二丙酯 50 n\n 酞酸二环己酯 60 n\n 酞酸二甲酯 50 n\n 酞酸二壬酯 50 n\n 酞酸二辛酯 50 n\n 酞酸二乙酯 50 n\n 酞酸二异癸酯 60 n\n 酞酮酸 30 n\n 酞醯胺醯基 50 n\n 酞醯基 30 n\n 酞醯亚胺基 50 n\n 酞酰胺 30 n\n 酞酰氯 30 n\n 酞亚胺 30 n\n\n其中优先级是按词语长度用脚本自动生成的，给予长词更高的优先级。",
                "我发现了。似乎是自定义词库本身的问题",
                "如题",
                "同問，請教 @fxsjy",
                "同問，請教 @fxsjy",
                "请问有人解决了吗？",
                "我改了一下，目前支持词库中的符号和空格匹配了 https://github.com/WalkerWang731/jieba",
                "关键词提取无论如何总能得到一些结果，在语料非常多的时候，人工标记不太具备可行性，那么有哪些推荐的算法用于评估提取内容的准确性和相关性呢？",
                "如題，TFIDF算法一般來說topK這個參數要取多少比較合適?我目前使用的題材是金庸的小說，目前正在切神鵰俠侶這本。"
            ]
        },
        {
            "time": "Mar 8, 2017",
            "title": "自定义词库问题",
            "contents": [
                "jieba.possseg.cut的分词不能支持和jieba.cut一样的可选cut_all参数，这样两种模式下分词结果不一样。",
                "除了函数的替换外，基本上没有改动别处的代码。经过大量测试分词的结果与您的结果相同，开启HMM的普通模式时间缩短了60%左右，关闭HMM的普通模式时间缩短了50%左右。\n  请问我可以把这些独立成一个仓库吗，经完整的测试后再request你的readme",
                "我想，你可以把这些c++的程序封装成一个静态 so文件，然后可以增加到这个项目里面，然后py脚本直接调用你的c++so 代码，这样可以提高效率。也可以让这个项目更加的完美。",
                "增加一个 c++的模块在里面，然后，py 脚本函数调用弄一个开关，选择是用py代码还是用c++代码。",
                "*unix下编译.so没啥问题，windows下目前兼容还不好还在调试",
                "windows就不要支持了，你可以弄一个说明，就说，c++代码库只支持linux系统，不对windows提供。\nwindows就让他们用py代码得了。反正又不跑服务端.",
                "用 except ImportError 这种吧，你先 fork 到自己仓库，作者估计也不怎么管了。",
                "已经封装好，详情见https://github.com/deepcs233/jieba_fast\n可使用 pip install jieba_fast安装",
                "安装出错啊，报错少文件。\ngcc: 错误：source/jieba_fast_functions_wrap_py3_wrap.c：没有那个文件或目录\ngcc: 致命错误：没有输入文件\n编译中断。",
                "@george-sq 已经fix，有问题建议到issues里问",
                "我使用set_dictionary自定义词库 但是发现每个词语后面都需要添加词语，词频，词性\n在README中介绍到   词频，词性 和词性是可以省略的 检查发现在__init__ 文件77行对每一行进行分词后报错 请问怎么解决有人知道吗",
                "jieba.load_userdict 加载词典不起作用\njieba.add_word 一个一个的加载起作用",
                "@qxde01 我也遇到了这个问题。",
                "@1000sprites\n   dict_words=pd.read_csv(dictfile)\n   n=dict_words.shape[0]\n   for i in range(0,n):\n            jieba.add_word(dict_words.word[i],int(dict_words.freq[i]),'nz')",
                "@qxde01 thanks so much."
            ]
        },
        {
            "time": "Jul 5, 2018",
            "title": "如何动态加载自定义词典",
            "contents": [
                "你好我在我的一个小项目里使用了结巴分词，首先对您的努力表示感谢\n我在使用py2exe打包我的程序的时候，其他的模块都没有出现问题，打包能够成功，但在运行的时候一直显示\nIOError: [Errno 2] No such file or directory: 'E:\\python\\links\\romeo\\dist\\l\nibrary.zip\\jieba\\finalseg\\prob_start.py'\n我单独写了一个.py文件，只有一句话：\nimport jieba\nseg=jieba.cut('hello jieba')\n打包，但运行的时候也是上面的那个结果。\n\n我不知道您是否打包过使用了结巴的程序，如果有，您遇到过这样的问题吗？\n或者您知道问题可能出现在哪吗？\n谢谢啦~",
                "@lisnb , 我没有用过py2exe。看错误信息，'E:\\python\\links\\romeo\\dist\\l\nibrary.zip\\jieba\\finalseg\\prob_start.py' 这个py文件被打包在zip里面了，所以读不出来。等会我装个py2exe试一试",
                "@fxsjy  嗯，非常感谢您的回复\n您试过之后应该就会看到，所有的依赖项都会被打包在一个zip文件里（如果没有其他设置的话）\n也有可能是py2exe的问题，那么您用过其他的什么打包的工具打包过使用了结巴的程序吗？",
                "@lisnb ，试了一下，确实不行。 路径问题不好处理啊，数据文件应该怎么放？",
                "@fxsjy\n网上是这么说的，不知道你问的是不是这个\nfrom distutils.core import setup\nimport glob\nimport py2exe\nsetup(console=[\"myscript.py\"],\ndata_files=[(\"bitmaps\",\n[\"bm/large.gif\", \"bm/small.gif\"]),\n(\"fonts\",\nglob.glob(\"fonts*.fnt\"))],\n)\n说明：data_files选项将创建一个子目录dist\\bitmaps，其中包含两个.gif文件；一个子目录dist\\fonts，其中包含了所有的.fnt文件。\n我在想的是，好像分词的字典没有被收进包里，为什么代码文件也没有呢，好奇怪。我以前也没有打包过，我也研究一下...",
                "问题还在，我尝试手工把prob_start.py插入到zip包中，可这也不是解决方法，而且不管用。",
                "@yangboz , 这个问题解决了，你git pull更新一下。 我用cxfree试验成功了。\n测试程序： hello.py\n#encoding=utf-8\nimport jieba\njieba.set_dictionary(\"./dict.txt\")\njieba.initialize()\ns=\"我研究生命起源。\"\nprint \" \".join(jieba.cut(s))\ndict.txt 放在hello.py 同级目录。",
                "pull下来安装最新的jieba 代码后，引用“jieba.set_dictionary(\"./dict.txt\")jieba.initialize()” 路径错误信息是：jieba\\posseg..dict.txt;\n不用“jieba.set_dictionary(\"./dict.txt\")jieba.initialize()” 后的错误信息是：jieba\\posseg\\dict.txt;",
                "@yangboz , sorry，昨天的更新没有解决posseg读词典的路径问题，麻烦你再pull一下试一试。\n下面是我的测试程序：\n#encoding=utf-8\nimport jieba\njieba.set_dictionary(\"./dict.txt\") #相对路径\n#jieba.set_dictionary(\"c:/tmp/dict.txt\")  #也支持绝对路径\njieba.initialize()\n\nfrom jieba import posseg\n\ns=\"我研究生命起源。\"\nprint \" \".join(jieba.cut(s))\nfor w in posseg.cut(s):\n    print w.word,w.flag",
                "pull到最新代码，最后还有提示提示 File \"genericpath.pyc\", line 54,in getmtime\nWindowError: [Error 3] \" \"xxx\\dist\\library.zip\\jieba\\dict.txt\"\n我用的是 py2exe，和cxtree应该差不多。",
                "@yangboz , 必须调用jieba.set_dictionary明确指出词典的路径，否则会在默认位置寻找dict.txt。但是被py2exe打包之后就找不到了。",
                "pull到最新的，代码有看到有变化，pseg_cut 有过滤一些空白词汇了，但是最后打包成exe的时候，同时“明确指出词典的路径”后，原来utf-8编码到exe运行的时候成了“cp936”了，还不确定是py2exe的问题。",
                "车库咖啡会场现场解决了:-)\n总结如下：\nstr()操作中文字符+操作，python转换尝试寻找系统默认的编码;\ndict定义中文需要\"\\u\";\n文件io输出也需要\"encode('utf-8)\", py2exe中文问题解决.",
                "@yangboz ， 昨晚很有意思，大牛们在上面讲，咱俩在调程序。",
                "你好，这个问题解决了吗？我也碰到相同的问题，很懊恼，jieba官网上说，支持py2exe的，但是制作后，还是不行",
                "在上次来源大会现场作者帮助下，解决了，麻烦你贴下你的错误信息！\nSend from Yangbo's iPhone.\n\nOn 2013年10月23日, at 15:17, AlgorithmFan notifications@github.com wrote:\n你好，这个问题解决了吗？我也碰到相同的问题，很懊恼，jieba官网上说，支持py2exe的，但是制作后，还是不行\n—\nReply to this email directly or view it on GitHub.",
                "你好，按照你上面说的方法，加上\njieba.set_dictionary(\"dict.txt\")\njieba.initialize()\n仍然出现下面的错误。\nTraceback (most recent call last):\nFile \"mainWidget.py\", line 6, in \nFile \"docKeyword.pyc\", line 13, in \nFile \"jieba\\posseg__init__.pyc\", line 61, in \nFile \"jieba\\posseg__init__.pyc\", line 20, in load_model\nIOError: [Errno 2] No such file or directory: 'E:\\crawWenkuBC\\dist\\library.zip\\jieba\\dict.txt'",
                "看错误信息还是这个dict.txt的路径问题，这个文档一直说的很晦涩，你先参考下我这打包程序中的路径 https://github.com/yangboz/hairy-avenger/tree/master/KingOfProgrammer/src/Q2",
                "谢谢，已经解决，犯了低级错误，应该将\njieba.set_dictionary(\"dict.txt\")\njieba.initialize()\n直接放在import jieba后面，然后再进行import jieba.posseg as pseg,这样才可以在程序中修改dict.txt的路径，再次表示感谢。",
                "Building prefix dict from C:\\Users\\game\\Desktop\\qt\\test\\dist\\dict.txt ...\nLoading model from cache c:\\users\\game\\appdata\\local\\temp\\jieba.ua7197607c9829a7854ca3e54b4005544.cache\nLoading model cost 0.355 seconds.\nPrefix dict has been built succesfully.\nTraceback (most recent call last):\nFile \"test.py\", line 7, in \nfrom jieba import analyse\nFile \"jieba\\analyse_init_.pyo\", line 9, in \nFile \"jieba\\analyse\\tfidf.pyo\", line 65, in init\nFile \"jieba\\analyse\\tfidf.pyo\", line 42, in init\nFile \"jieba\\analyse\\tfidf.pyo\", line 47, in set_new_path\nIOError: [Errno 2] No such file or directory: 'C:\\Users\\game\\Desktop\\qt\\test\\dist\\library.zip\\jieba\\analyse\\idf.txt\nidf.txt还是不行\n`# -- coding: utf-8 --\nimport sys\nsys.path.append(\"../\")\nimport jieba\njieba.set_dictionary(\"./dict.txt\")\njieba.initialize()\nfrom jieba import analyse\njieba.analyse.set_idf_path(\"./idf.txt\")\nif name == 'main':\nseg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\nprint(\"Default Mode: \" + \"/ \".join(seg_list))  # 默认模式\ns = \"此外，公司拟对全资子公司吉林欧亚置业有限公司增资4.3亿元，增资后，吉林欧亚置业注册资本由7000万元增加到5亿元。吉林欧亚置业主要经营范围为房地产开发及百货零售等业务。目前在建吉林欧亚城市商业综合体项目。2013年，实现营业收入0万元，实现净利润-139.13万元。\"\nfor x, w in jieba.analyse.extract_tags(s, withWeight=True):\n    print('%s %s' % (x, w))`\n\n很简单的代码",
                "还是不行哈",
                "我自定义了一个词典，有一些包含“-”的词，比如 “SK-II”。加载这个词库时候发现只有这类词语没法生效，会被分为 SK，-，II,而不是SK-II。请问这个问题应该如何解决",
                "同问，自定义词典中有5%,但分词会分开，词性标注的会分开 但正常分词不会被分开",
                "找到解决方法了\n修改jieba/init.py代码中的re_han_default 正则表达式为如下值：\nre_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#\u0026\\._%\\-]+)\", re.U)",
                "Can solve this: #692",
                "在一个已经在运行的进程中，我想能够自动加载自定义词典以实现实时对关键词的提取。之前我已经尝试过直接实时判断本地词典文件是否有更新，更新则使用load_userdict来加载词典，但是这个方法好像不管用，新添加进去的关键词还是不能被识别，请问大佬们有什么办法破吗？",
                "CustomDictionary.reload() 可以重新加载词典，但是在加载期间， 分词功能不知道还能不能正常使用。",
                "@csyjgu 敢问大侠CustomDictionary是自定义词典？还是什么？有reload方法吗？",
                "我也有类似的问题，能否在一个工程中能否载入多个字典，然后根据方法需要动态选择使用哪个？",
                "我也有相同的疑問，能否動態選擇要加載哪一種詞典呢?\n目前想到兩種做法，\n\n每次更換自定義字典時 使用 jieba.initialize() 初始化，再切換辭典，不過這個方法挺耗時的。\n創造多個 jieba實例對象分別載入自定義辭典，再依據需求選用不同的對象。\n\n雖然可行，但都不是最佳解！",
                "CustomDictionary.reload() 可以重新加载词典，但是在加载期间， 分词功能不知道还能不能正常使用。\n\n这是HanLP中的方法吧。",
                "可以用jieba.add_word 和 jieba.del_word 进行动态修改吧, 我现在的问题是web应用程序是多进程运行的，add和del之后只对当前处理请求的进程生效 。"
            ]
        },
        {
            "time": "Oct 20, 2016",
            "title": "python 无法添加单个词汇",
            "contents": [
                "也就是分析和统计语料生成词典文件、idf.txt文件的源代码，非常期待~~",
                "@lewsn2008 你看看作者其他的项目，已经分享了",
                "@piaolingxue thanks~",
                "@piaolingxue 是按个项目啊？能给个项目名不？",
                "您好，如何添加停用词，常见的停词表在哪里可以下载呢",
                "停用词表推荐这个：https://github.com/dongxiexidian/Chinese/blob/master/stopwords.dat",
                "多谢！",
                "@tukeyone  , here is the example for adding the stop words\n# build stop word list\ndef stopwordslist(filepath):\n    stopwords = [line.strip() for line in open(filepath, 'r').readlines()]\n    return stopwords\n\ndef seg_sentence(sentence):\n    sentence_seged = jieba.cut(sentence.strip())\n    # loading stop word list\n    stopwords = stopwordslist(\"your_stop_word_txt_file\")  \n    outstr = ''\n    for word in sentence_seged:\n        if word not in stopwords:\n            if word != '\\t':\n                outstr += word\n                outstr += \" \"\n    return outstr",
                "Thank you ！ this problem has been solved .",
                "In [4]: print '/'.join(jieba.cut(\"我来到清华大学\",cut_all=True))\n我/来到/清华/清华大学/华大/大学\nIn [5]: jieba.add_word('到清')\nIn [6]: print '/'.join(jieba.cut(\"我来到清华大学\",cut_all=True))\n我/来到/到清/清华/清华大学/华大/大学\nIn [7]: jieba.add_word('学')\nIn [8]: print '/'.join(jieba.cut(\"我来到清华大学\",cut_all=True))\n我/来到/到清/清华/清华大学/华大/大学\n希望高手能给予帮助!",
                "同问",
                "#408"
            ]
        },
        {
            "time": "Nov 1, 2016",
            "title": "jieba优化以支持spark-stream高效率分词",
            "contents": [
                "貌似代码里面没有从语料生成idf.txt的代码，是否遗漏？",
                "词典和idf.txt都是坐着事先对语料进行训练和分析得到的，不包含在这个工程里面。\n不过我也很想看到语料分析的代码，期待作者共享！",
                "如果大家对这个有兴趣，我愿意分享给大家。\n当时觉得idf.txt里面的统计不大好，所以自己想办法生成了一份。还有词料统计，用了最大熵的思路，生成了一分语料统计。主要是为了发现新词的。\n2013/8/7 lewsn2008 notifications@github.com\n\n词典和idf.txt都是坐着事先对语料进行训练和分析得到的，不包含在这个工程里面。\n不过我也很想看到语料分析的代码，期待作者共享！\n—\nReply to this email directly or view it on GitHubhttps://github.com//issues/87#issuecomment-22226557\n.",
                "@linkerlin , @lewsn2008 , 我找到了之前写的生成idf.txt的脚本，基本思路是对一些小说报刊语料进行分词，然后以段落为单位，统计idf.\nimport jieba\nimport math\nimport sys\nimport re\nre_han = re.compile(ur\"([\\u4E00-\\u9FA5]+)\")\n\nd={}\ntotal = 0\nfor line in open(\"yuliao_onlyseg.txt\",'rb'):\n    sentence = line.decode('utf-8').strip()\n    words = set(jieba.cut(sentence))\n    for w in words:\n        if w in jieba.FREQ:\n            d[w]=d.get(w,0.0) + 1.0\n    total+=1\n    if total%10000==0:\n        print \u003e\u003esys.stderr,'sentence count', total\n\nnew_d = [(k,math.log(v/total)*-1 ) for k,v in d.iteritems()]\n\nfor k,v in new_d:\n    print k.encode('utf-8'),v",
                "@fxsjy 非常感谢，作者真是无私的大牛啊，膜拜！",
                "谢谢！\n没看明白为何要 math.log(v/total)*-1",
                "@linkerlin , 也可以math.log(total/v)",
                "求语料数据，程序中的yuliao_onlyseg.txt还有吗？想跑一下学习学习，thks",
                "一个新的分词库：https://github.com/jannson/yaha ，仅与大家交流学习：\n\n提供了解决结巴分词库的姓名识别，后缀名识别，使用正则表达式等问题的思路；\n同时对提取关键字，ChineseAnalyzer进行了小小的优化；\n附加了 最大熵算法生成新词，自动摘要，比较两文本的相似度算法的实现。\n\n产生这个分词库的原因，是因为在我的一个小小的爬虫，搜索引擎上使用结巴分词库之后，发现了一些小问题优化之后形成的，本来想直接修改结巴代码并提交，但是有一些设计思路区别较大才弄的新的分词库，不是对结巴作者的不敬。\n最后感谢结巴库作者，里面的字典以及一些代码思路来自于结巴库。也希望大家以后能有更多交流。\nOn Tue, Aug 20, 2013 at 5:40 PM, lewsn2008 notifications@github.com wrote:\n\n求语料数据，程序中的yuliao_onlyseg.txt还有吗？想跑一下学习学习，thks\n—\nReply to this email directly or view it on GitHubhttps://github.com//issues/87#issuecomment-22933170\n.",
                "@jannson , 已关注。",
                "@lewsn2008 , 这个文件有200多MB，怎么发给你？",
                "能否发个dropbox链接，我也想下载一份。\nwuyanyi09@gmail.com\n发件人： Sun Junyi\n发送时间： 2013-08-21 17:13\n收件人： fxsjy/jieba\n主题： Re: [jieba] 关于idf.txt (#87)\n@lewsn2008 , 这个文件有200多MB，怎么发给你？\n—\nReply to this email directly or view it on GitHub.",
                "@aszxqw , @lewsn2008 ，试用了一下百度云盘，分享地址：http://pan.baidu.com/share/link?shareid=4094310849\u0026uk=1124369080",
                "if w in jieba.FREQ:\n        d[w]=d.get(w,0.0) + 1.0\n\njieba.FREQ现在已经不存在了，请问现在应该如何写if w in   ?",
                "Try `jieba.dt.FREQ`\n\n\nOn Feb 24, 2018 17:55, \"xunyl\" \u003cnotifications@github.com\u003e wrote:\n\n    if w in jieba.FREQ:\n        d[w]=d.get(w,0.0) + 1.0\n\njieba.FREQ现在已经不存在了，请问现在应该如何写if w in ?\n\n—\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\n\u003c#87 (comment)\u003e, or mute\nthe thread\n\u003chttps://github.com/notifications/unsubscribe-auth/AA0SqiLtUKC9m_qw6PFto87F-Vh10uWwks5tX9x8gaJpZM4A2-YO\u003e\n.",
                "形容词某些情况下变成了名词了 如下所示\n# 新 a\n\u003e\u003e\u003e pseg.lcut(\"新材料装备制造业的循环产业体系\")\n[pair('新', 'a'), pair('材料', 'n'), pair('装备', 'n'), pair('制造业', 'n'), pair('的', 'uj'), pair('循环', 'vn'), pair('产业', 'n'), pair('体系', 'n')]\n# 新 n\n\u003e\u003e\u003e pseg.lcut(\"青海格尔木工业园镁锂新材料产业基地\")\n[pair('青海', 'ns'), pair('格尔木', 'nr'), pair('工业园', 'n'), pair('镁锂', 'nz'), pair('新', 'n'), pair('材料', 'n'), pair('产业基地', 'n')]\n\n\n# 大 a\n\u003e\u003e\u003e pseg.lcut(\"大工业自由\")\n[pair('大', 'a'), pair('工业', 'n'), pair('自由', 'a')]\n# 大 n\n\u003e\u003e\u003e pseg.lcut(\"7个国家的大公司中\")\n[pair('7', 'm'), pair('个', 'm'), pair('国家', 'n'), pair('的', 'uj'), pair('大', 'n'), pair('公司', 'n'), pair('中', 'f')]\n\n有什么办法可以使其保持一致吗？",
                "参考hanlp",
                "可以參考我修正的錯誤 #670\n測試完 您上述的語句都正常\n(可能跟我修正的錯誤有關係)\n#'新', 'a'\n\u003e\u003e\u003e pseg.lcut(\"新材料装备制造业的循环产业体系\")\n[pair('新', 'a'), pair('材料', 'n'), pair('装备', 'n'), pair('制造业', 'n'), pair('的', 'uj'), pair('循环', 'vn'), pair('产业', 'n'), pair('体系', 'n')]\n#新,'n' -\u003e #'新', 'a'\n\u003e\u003e\u003e pseg.lcut(\"青海格尔木工业园镁锂新材料产业基地\")\n[pair('青海', 'ns'), pair('格尔木', 'nr'), pair('工业园', 'n'), pair('镁', 'n'), pair('锂', 'n'), pair('新', 'a'), pair('材料', 'n'), pair('产业基地', 'n')]\n\n#'大', 'a'\npseg.lcut(\"大工业自由\")\n[pair('大', 'a'), pair('工业', 'n'), pair('自由', 'a')]\n\n#'大', 'n' -\u003e #'大', 'a'\npseg.lcut(\"7个国家的大公司中\")\n[pair('7', 'm'), pair('个', 'm'), pair('国家', 'n'), pair('的', 'uj'), pair('大', 'a'), pair('公司', 'n'), pair('中', 'f')]",
                "参考Issue: #387\n如果直接在spark-stream的flatMap中执行jieba.cut()分词，则因“发现该dict加载后是以python字典的数据类型存在于spark的driver里面，但是worker进程无法共享这段内存。By @liaicheng ”，导致每次执行时多次“Building prefix dict from the default dictionary ……”，影响了分词性能。\n如果“建议用jieba.Tokenizer 得到一个分词器实例，然后再调用相关方法。By @fxsjy ”，则对tokr实例进行广播变量（sc.broadcast(tokr)）时，报“can't pickle thread.lock objects”的错误。\n考虑到spark是采用pickle进行持久化，参考pyahocorasick（https://pypi.python.org/pypi/pyahocorasick/ ）的实现方式，\nYou can also create an eventually large automaton ahead of time and pickle it to re-load later. Here we just pickle to a string. You would typically pickle to a file instead:\n\n\n\nimport cPickle\npickled = cPickle.dumps(A)\nB = cPickle.dumps(pickled)\nB.get('he')\n(0, 'he')\n\n\n\njieba也能否优化以更好地支持序列化从而进一步提高在spark中的执行效率？"
            ]
        },
        {
            "time": "Apr 4, 2014",
            "title": "词性标注的viterbi的line28和line29的判断条件一致",
            "contents": [
                "看了一下结果，感觉挺好的，想研究一下整个过程，能否提供一个简单的文档说明，代码都没有注释，python菜鸟请教。",
                "争取在一个月之类完善设计文档",
                "期待啊，想认真学习一下楼主的代码",
                "我最近正在研究爬虫来抓取数据，需要中文分词系统，也想研究一下分词系统。ICTCLAS中文分词系统竟然没有给出Python的接口，让人很无语！！！",
                "文档还未完成啊~~~~，自罚一杯",
                "今天发现一个网友写的分析文档，很感动，此文基本上揭示了结巴分词的原理：http://ddtcms.com/blog/archive/2013/2/4/69/jieba-fenci-suanfa-lijie/",
                "多谢，楼主的什么时候出来呢？呵呵",
                "@kingllon, 公司有很多码要砌，github的业务荒废有月余了，惭愧。大家可以先看ddtcms写的那个:-)",
                "粗略看了一遍，的却写总结的很好。细节需要精度，多谢提供资料。",
                "感谢作者的无私分享。不过好像已经打不开链接了。不知能不能重新分享一下",
                "对呀，分析文档的网页打不开了，好像是服务器不行了，不知道有没有新的链接",
                "分析文档的网页打不开，求新地址 ~~~谢谢谢谢谢谢",
                "一直在等待一个Ruby版本的jieba，求大神来开发一个！",
                "No description provided.",
                "我想要用jieba对content一列进行分词：\n\n结果总是出现“AttributeError: 'float' object has no attribute 'decode'，为了解决这个问题，把这个series变成str之后又无法loop，请问我应该怎么处理呢？谢谢！",
                "我如果猜测没错，你想写的代码应该是\nt_list = []\nt_list += list(jieba.cut(sample['content']))\n\n如果不是你应该把你具体哪句出错列出来，不过很可能是python 语法不熟造成的",
                "您好，谢谢您的热心回答！第一次在github发问题不是很清楚问法，给您造成麻烦啦～\n\n问题我已经解决啦，是我数据本身的问题，让我加入一个conditional loop就OK啦～谢谢您！\n\n祝好～\n\n2018-04-19 15:47 GMT+08:00 MoreFreeze \u003cnotifications@github.com\u003e:\n…\n 我如果猜测没错，你想写的代码应该是\n\n t_list = []\n t_list += list(jieba.cut(sample['content']))\n\n 如果不是你应该把你具体哪句出错列出来，不过很可能是python 语法不熟造成的\n\n —\n You are receiving this because you authored the thread.\n Reply to this email directly, view it on GitHub\n \u003c#617 (comment)\u003e, or mute\n the thread\n \u003chttps://github.com/notifications/unsubscribe-auth/AhgNuVNwsxwnvtduCq8KfQGDIgnVnRlhks5tqEEhgaJpZM4TPapa\u003e\n .",
                "为什么这两行的判断条件一致呢？\nif len(obs_states)==0: obs_states = prev_states_expect_next\nif len(obs_states)==0: obs_states = all_states"
            ]
        },
        {
            "time": "Mar 1, 2014",
            "title": "词性标注多字",
            "contents": [
                "使用jieba提取readme内容中的关键词，返回的结果是：\njieba,the,cut,to,list\n这个结果中是否可以将the，to剔除掉，有没有什么选项？",
                "@yukaizhao, 主要是idf.txt中只有中文词汇。我会加强这一块，需要分析英文语料。",
                "@fxsjy 我也碰到这个问题，但我试着添加了英文的idf数据，但是仍然有the,to这样的英文出现，能否添加stop words？",
                "@fay, @yukaizhao , 最新版本已经对关键词提取功能做了一些改进，也加了一些英文的stop words。 抽取效果Demo: http://jiebademo.ap01.aws.af.cm/extract",
                "试了一下，效果好多了，正好有件事儿请教，现在的master版本是稳定版本吗，我想换一个小点的词典，更新词典后发现和以前的jieba版本不兼容。",
                "@yukaizhao , master版本不是稳定版，但是应该没有兼容性的问题吧。出错信息是什么？",
                "确实是词典的格式不一样，新的词典添加词性，老词典没有词性\n老词典：\n一万三千五百斤 4\n新词典：\n她 134035 r\n请问哪儿可以下载最新的稳定版呀？",
                "https://pypi.python.org/pypi/jieba",
                "我自己在网上爬了1500个网页评论，训练了一个idf.txt，然后读取前面的名词，也就是字典里有 \"n\"的词，效果还很不错。有需要我可以给你。\nOn Thu, Aug 22, 2013 at 1:26 PM, Zoey Young notifications@github.comwrote:\n\n抽取关键词中含...\n—\nReply to this email directly or view it on GitHubhttps://github.com//issues/27#issuecomment-23069212\n.",
                "@jannson 求教怎么训练idf",
                "No description provided.",
                "版本 \u003c0.37 中 pair 对象不是 iterable，即不能像 tuple 一样直接拆开来。之前的测试代码相关部分是：\nfor w in words:\n    print('%s %s' % (w.word, w.flag))",
                "恩，已经解决，谢谢！！前后例子不太一致，导入词典的那个例子里用的是你上面写的例子。。。主页上用的是tuple",
                "导入词典的例子在哪里",
                "在用词性标注的时候如以下句子：\n前港督衛奕信在八八年十月宣布成立中央政策研究組\n词性标注后变成\n前/f 港督/n 衛/v 衛奕/z 信/n 在/p 八八年/m 十月/t 宣布/v 成立/v 中央/n 政策/n 研究組/n\n多出一个衛字\n单纯的用分词是OK的，不知道是不是我词性标注的选取的词性标注方法有问题。\n我用的是官网上的样例：\n\n\n\nimport jieba.posseg as pseg\nwords = pseg.cut(\"前港督衛奕信在八八年十月宣布成立中央政策研究組\")\nfor w in words:\n...    print w.word, w.flag",
                "@elfcool ， 多谢反馈，这是个bug，我刚刚修复了一下：9d4ac26\n你有空pull下来看看问题还存不存在。"
            ]
        },
        {
            "time": "Feb 8, 2017",
            "title": "默认的字典词性错误",
            "contents": [
                "jieba.__version__ = 0.34\n字典檔確定有「台中」這個詞。\n不論用何種模式，「台中」都會切成「台」「中」，但是正確應該不會被切開。\n「台北」、「台南」就不會被切開，這樣是正確的。\n\n\n請問這有問題有辦法解決嗎？？",
                "可以把台中加入自定义词典jieba.load_userdict",
                "好，我試試看，但是我好奇的是，「台中」這個字詞已經在字典裡面了，為何還是斷詞還是斷成「台」「中」？？",
                "\u003e\u003e\u003e jieba.FREQ['台中']\n3\n\u003e\u003e\u003e jieba.FREQ['台']\n16964\n\u003e\u003e\u003e jieba.FREQ['中']\n243191\n\u003e\u003e\u003e jieba.total\n60101967\n3/60101967 \u003c 16964×243191/601019672\n即，P(台中) \u003c P(台)×P(中)，台中词频不够导致概率较低\n在自定义词典调高词频即可，或者add_word用更高频率覆盖“台中”\n（这里是\u003e16964×243191/60101967，即≥69）",
                "我发现jieba会将“不合理”看做是一个词，而不是“不/合理”。\n另外，基于@gumblex 给出的计算，应该是可以将“不”和“合理”分开的，如下：\njieba.get_FREQ(u'不合理') # 470\njieba.get_FREQ(u'不') # 360331\njieba.get_FREQ(u'合理') # 3870\n词典中共有584429个词（dict.txt.big），计算：\n470/584429 \u003c 360331*3870/584429^2\n如果使用user dict，当把“合理”的词频调高后，可以分开。",
                "这个计算的分母是总频数(jieba.dt.total = 60101967)不是总词数，这样计算下来「不合理」确实高于其他分词。\n你可以手动拆分：jieba.suggest_freq(('不 ','合理'), True)，或直接删除 del_word。",
                "@gumblex ：\n明白了。另外一个问题是：\n1）在当前程序中调用suggest_freq，会影响后续的使用么？\n2）del_word只是对当前的分词起作用么？是否并没有将其在dict.txt中删除？",
                "@gumblex :\n厚着脸皮再问一个，POS标注中的分词结果是否和jieba.cut相同？",
                "suggest_freq 和 del_word 等仅对当前分词器实例 (jieba.Tokenizer) 起作用（旧版是仅对全局词典起作用），不会修改本地词典，相当于改了几个变量。\nPOS标注中的分词结果不一定相同，见 #212。",
                "怎么让 POS标注 支持 suggest_freq 和 del_word ?",
                "：将目标文本按行分隔后，把各行文本分配到多个python进程并行分词，然后归并结果，从而获得分词速度的可观提升\n基于python自带的multiprocessing模块，目前暂不支持windows",
                "You can try this. It works in Windows.\nfrom path import Path\nfrom multiprocessing import Pool\nimport argparse\nimport time\n\nLINE_PER_CORE = 5000\nNUM_CORE = 30\nFLOOR_COUNT = 10\nCEIL_COUNT = 200\n\nimport jieba\n\n\ndef process_one(_in):\n    r_list = []\n    for l in _in:\n        new_l = ' '.join(jieba.cut(l))\n        r_list.append(new_l.strip())\n    return r_list\n\n\ndef do(l_list, writer):\n    pool = Pool(NUM_CORE)\n    r_list=pool.map(process_one,[l_list[it:it+LINE_PER_CORE] for it in range(0,len(l_list),LINE_PER_CORE)])\n    pool.close()\n    pool.join()\n    for lr in r_list:\n        for line in lr:\n            writer.write(line + '\\n')\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-i\",\"--input\", help=\"input folder\", default=\".\")\n    parser.add_argument(\"-o\", \"--output\", help=\"output folder\", default=\"w_process\")\n    parser.add_argument(\"--LINE_PER_CORE\", help=\"# lines per core\", type=int, default=20000)\n    parser.add_argument(\"--NUM_CORE\", help=\"# of cores\", type=int, default=30)\n    parser.add_argument(\"--coding\", type=str, default=\"utf-8\")\n\n    args = parser.parse_args()\n    print(\"Args :\", args)\n    input_folder = args.input\n    output_folder = args.output\n    LINE_PER_CORE = args.LINE_PER_CORE\n    NUM_CORE = args.NUM_CORE\n    coding = args.coding\n\n    if not Path(output_folder).exists():\n        Path(output_folder).mkdir()\n    for f in Path(input_folder).files('*.txt'):\n        print(f.basename(), time.strftime('%Y-%m-%d %X', time.localtime()))\n        with open(output_folder + '/%s.output.txt' % (f.namebase,),'w', encoding='utf-8') as f_out:\n            with open(f.abspath(),'r', encoding='utf-8') as f_in:\n                l_list=[]\n                all_dict = {}\n                for l in f_in:\n                    if len(l_list)\u003cNUM_CORE*LINE_PER_CORE:\n                        l_list.append(l)\n                    else:\n                        do(l_list, f_out)\n                        print(f.basename(), time.strftime('%Y-%m-%d %X', time.localtime()))\n                        l_list=[]\n                if len(l_list)\u003e0:\n                    do(l_list, f_out)\n    print(time.strftime('%Y-%m-%d %X', time.localtime()))",
                "你好，我用jieba做一些实体识别，发现很多默认的词性都很奇怪\n宣传教育 nr\n不知道这个是否有办法能都改正下，或者有啥建议"
            ]
        },
        {
            "time": "Mar 24, 2014",
            "title": "结巴分词去重",
            "contents": [
                "http://jiebademo.ap01.aws.af.cm/\nnot work",
                "Hello, does anybody know from where the dictionary of traditional Chinese characters originated? Who created that dictionary or from where was it ported?",
                "请问，现在有去除相同分词的功能吗？是想用结巴来跑一个自己的中文字典，现在的重复词比较多，通过关键词也没办法很好控制，多谢。"
            ]
        },
        {
            "time": "Apr 4, 2014",
            "title": "golang jieba",
            "contents": [
                "你好，我最近在看结巴分词的算法，看到在判断切分组合的时候，利用的是动态规划来求出最大组合概率，我觉得这个方法挺好。\n不过，我有一个想法，不知道这样想对不对，还请指教。\n我是这样想的，因为之前已经形成了切分词图，我在想可不可以利用图论里的最短路径的相关算法来做判断。但是最短路径求得是权值最短，如果把概率作为权值的话，这样直接算就算的是最小概率了。之前我看到过一篇论文有提出一个解决这种问题的办法，利用最短路径算法来求最大权值。\n需要对权值做一定的转换。设边的权值为f（也就是词的频率或概率），取f的自然底数的对数的相反数，-ln(f)，作为新的权值w，即w=-ln(f)，利用这个权值w来求最短路径。\n可以看出：\n1，f越大，w越小\n2，min(w1+w2+w3+...+wi)对应的是max(ln(f1_f2_f3_...fi))，也就是说求出有新权值w的图的最短路径后，相应的那个路径的组合概率 P=f1_f2_f3..._fi 是最大的，也就是我们要找的\n3，由于f本来就是概率，所以0\u003cf\u003c1，因此w是非负数\n所以可以通过求最短路径的方法来得到最大组合概率。",
                "我感觉和我用的算法没有本质不同啊，最开始的时候我是用的概率相乘求最大概率路径，后来为了避免下溢，改为对log(p)求和来算最大概率路径。另外，由于是有向无环图，在有向无环图上求最长路径本来就是可以用动态规划的，不需要在转化为求最短路径问题吧。",
                "另外如果我想使用 php 调用结巴分词的话是不是只能生成文件然后相互传递了……",
                "词性标注功能已加上。结巴有 PHP 版本 https://github.com/fukuball/jieba-php ；至少你可以用管道吧",
                "感谢大神！！",
                "通过 jieba.set_dictionary(\".\"+os.sep+\"dict.txt\") 指定dict.txt 文件位于一个带中文的父目录时，使用jieba.initialize() 进行初始化报错，报错为：\nTraceback (most recent call last):\nFile \"\", line 26, in \nFile \"F:\\Qiyi\\build\\getcalldatafrom_txt\\out00-PYZ.pyz\\jieba\", line 111, in initialize\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xcc in position 3: ordinal not in range(128)\n定位于语句： jieba_init.py_ 中的 111行： default_logger.debug(\"Building prefix dict from %s ...\" % (abs_path or 'the default dictionary'))\n将程序至于全英文目录时，运行正常。",
                "用golang 实现了一下jieba, 在 https://github.com/pastebt/gieba , 欢迎指教"
            ]
        },
        {
            "time": "Mar 16, 2018",
            "title": "带中横杆\"-\"的词如何把它当做一个词",
            "contents": [
                "做个应用，抓取网页里中文，然后统计出现数量较多的关键词。content是网页的html代码。运行会出现Cannot allocate memory，free-m 看内存被使用290多m（vps是512内存，基本上就剩290多m），请问如何优化下代码使其可以正常运行？match结果大概1200多个词，每个词不超过5个字。以下是代码\ncount = {}\nmatch=re.findall(r'[\\x80-\\xff]+',content)\nss=','.join(match)\nseg_list = jieba.cut_for_search(ss)\nfor c in seg_list:\nif c.encode('utf-8') in count:\ncount[c.encode('utf-8')] = count[c.encode('utf-8')] + 1\nelse:\ncount[c.encode('utf-8')]=1\nreturn sorted(count.iteritems(),key=lambda x:(x[1],x[0]),reverse=True)[0:10]",
                "@pydotnet , 用一个小一点的字典试试：https://github.com/fxsjy/jieba/blob/master/extra_dict/dict.txt.small\n替换掉dict.txt",
                "@pydotnet ， 可参考： #17",
                "No description provided.",
                "https://www.cnblogs.com/adienhsuan/p/5674033.html，可以参考一下",
                "我也遇到这个问题了，求告知一个比较全的词性介绍。",
                "找到一个词性介绍，见jieba",
                "官方应该添加一份词性列表到文档中",
                "同问",
                "应该和这个一样\nhttp://ltp.readthedocs.io/zh_CN/latest/appendix.html#id3",
                "如何将一些带中横杆\"-\"的词如何把它当做一个词，比如一些药品名：PD-1, PDL-1 这种词当做一个词，而不是分成：\"PD\", \"-\", \"1\"",
                "我也没找到什么好的方法，只能自己定义了一个字典，添加这些带-的词，并将频率设置高一些。",
                "添加了并且词频设置的比较高，还是会拆分成开",
                "那你可以试着改一下源码，参考如下链接：http://blog.csdn.net/wangpei1949/article/details/57077007",
                "好的，谢谢！"
            ]
        },
        {
            "time": "Apr 9, 2016",
            "title": "关键词抽取处理停用词时的一个UE设计问题",
            "contents": [
                "提取的内容，方便排除部分标点，英文和数字么，需要调整一下吗？",
                "你可以看看test.py的样本和结果\nlizhengfu notifications@github.com编写：\n\n提取的内容，方便排除部分标点，英文和数字么，需要调整一下吗？\n—\nReply to this email directly or view it on GitHub.￼",
                "@youyudehexie , 你是说stopwords？ 现在还没有提供接口，你可以先试一试效果：https://github.com/fxsjy/jieba/blob/master/test/extract_tags.py",
                "THX??ֹͣ?? ?? ???ˣ???Ϊ?ҷ????????δ??뷢?ֻ???ȡ?????ֺͳ??ȴ???1?ı??㣬?Լ???????д??һ?£??ҵ??????ǣ???ȡ??ʱ?????????ֺͲ??ֱ??㣬?ǳ??????Ƶ?˼·????????ʱ?????޸ġ?\n------------------ ԭʼ?ʼ? ------------------\n??????: \"Sun Junyi\"notifications@github.com;\n????ʱ??: 2013??5??25??(??????) ????9:09\n?ռ???: \"fxsjy/jieba\"jieba@noreply.github.com;\n????: \"lizhengfu\"405574395@qq.com;\n????: Re: [jieba] ???ͷִʵĹؼ?????ȡ (#54)\n@youyudehexie , ????˵stopwords?? ???ڻ?û???ṩ?ӿڣ???????????һ??Ч????https://github.com/fxsjy/jieba/blob/master/test/extract_tags.py\n??\nReply to this email directly or view it on GitHub.",
                "@youyudehexie ，乱码？",
                "THX，停止词 和 过滤，因为我发现用这段代码发现会提取到数字和长度大于1的标点，自己简单重写了一下，我的疑问是，提取的时候，有数字和部分标点，是出于设计的思路，还是暂时不想修改。",
                "@youyudehexie , 的确是没有过滤数字和长度大于1的标点。 有些时候数字也会成为关键词吧，比如911。标点的确是应该过滤，你是怎么修改的？可以给我发一个pull request。\n这是我目前的实现，其实很简单：\nhttps://github.com/fxsjy/jieba/blob/master/jieba/analyse/__init__.py",
                "@fxsjy 不知道你现在做了标点没有，\n刚刚感觉标点不是个问题。。。直接替换成空格就好，长度大于2的空格再替换成长度为1的空格。\n但是试了一下发现用正则表达式不对，\\D连汉字都匹配上了。。。\n最近没时间做一个分支T^T\n查了一下\n正则表达式的[[:punct:]]可以匹配标点符号，我只在编辑器里试了一下。。。re模块好像要调整一下什么的，，",
                "@zjjott ,[[:punct:]] 不能搞定汉语标点符号吧？",
                "那我继续测试一下。。。。\n希望早日可以写个pull。。。\n2013/6/14 Sun Junyi notifications@github.com\n\n@zjjott https://github.com/zjjott ,[[:punct:]] 不能搞定汉语标点符号吧？\n—\nReply to this email directly or view it on GitHubhttps://github.com//issues/54#issuecomment-19439691\n.\n\n\n朱涧江\n微笑的猪头，帅气非凡，23333\n果壳网，科技有意思哟，23333\nwww.guokr.com",
                "我在加载了自定义词典之前enable_parallel，发现分词结果并没有使用词典。如果在加载自定义词典之后再enable_parallel则能按照预期结果分词。\n#encoding=utf-8\nimport sys\nimport jieba\njieba.enable_parallel(8)\njieba.load_userdict('user.dict')\n\nwords = jieba.cut(\"热水器加热时间太长\")\nprint('/'.join(words))\n\n结果是 “热水器/加热/时间/太/长”\n#encoding=utf-8\nimport sys\nimport jieba\njieba.load_userdict('user.dict') \njieba.enable_parallel(8)      # 放在load_userdict后面了\n\nwords = jieba.cut(\"热水器加热时间太长\")\nprint('/'.join(words))\n\n结果是*“热水器/加热时间/太长”*\n词典里是有“加热时间”，所以可以猜测第一个代码分词前没有载入自定义词典，可能是因为enable_parallel放load_userdict前面的。这不一致的情况是不是多线程读入词典的bug？",
                "我几个月前下了jieba使用，发现抽取关键词时，停用词未做处理，所以在网上找了几个停用词表merge以后，手工做了处理。\n现在看已经增加了自定义停用词表功能，不过我个人建议是：\n1.需要有一个默认的停用词表，在不附带任何选项的情况下，analyse.extract_tags()应该返回经过默认停用词表处理后的结果。\n2.假如有人不愿意要停用词，可以在该函数增加一个选项禁用停用词表。\n3.假如有人想用自己的停用词表，和现在一样以增设自定义词典的处理即可。\n附件为我目前使用的停用词表\nstop_words.txt",
                "这个改动建议和各选项的使用频率有关。90%以上的使用者在关键词抽取时是希望删去停用词的，因此应该作为默认选项使用。而不是让大家自己各显神通去找停用词表。"
            ]
        },
        {
            "time": "Jun 17, 2018",
            "title": "可以处理文言文吗？",
            "contents": [
                "我数据里的“有点”全部被标注成“/n”了。\n服务/vn 有点/n 情调/n\n这是怎么了？要怎么处理下才行？\n谢谢。",
                "@followYourHeart , 为了兼顾性能， dict.txt中的词只有一种词性，只有对于dict.txt中没有的词才会调用HMM去识别。你可以删掉dict.txt中的这行：有点 3706 n ，效果：\n服务 / vn ,  有 / v ,  点 / q ,  情调 / n ,\n或者，你改一下dict.txt中它的词性。",
                "No description provided.",
                "我在维护一个加速版的jieba，有issue和pr欢迎\nhttps://github.com/deepcs233/jieba_fast",
                "试了一下，赶脚效果不太好",
                "字典要换，用这个 https://github.com/The-Orizon/nlputils/tree/master/jiebazhc"
            ]
        },
        {
            "time": "Jan 25, 2017",
            "title": "运行报错：AttributeError: module 'jieba.finalseg' has no attribute 'cut'",
            "contents": [
                "必须传unicode太逆天了，至少传字符串时给个excepiton会比较友好",
                "I use spark cluster to seg word and submit the program with jieba to the cluster as following:\n/home/hadoop/spark-1.2.0-bin-hadoop2.4/bin/spark-submit --master spark://hz0124:7077 --driver-memory 1g --executor-memory 5g --total-executor-cores 60 --py-files ../sparkcode/dependency/dependency.zip seg\nword_test.py\nthe jieba is zipped as the file ../sparkcode/dependency/dependency.zip\nthen an error occured:\n\nAnyone meets this problem? how to deal with it?",
                "Using zip will cause problems to read the dictionary because it uses regular file IO.\nYou can unzip the dependencies, or put the dict.txt elsewhere (not in a zip) and then manually initialize jieba with jieba.initialize('/path/to/dictionary').",
                "运行报错，在_init_.py 和 finalseg文件里都找不到\nFile \"C:\\XXXXXXX\\Python\\Python35\\lib\\site-packages\\jieba_init_.py\", line 265, in __cut_DAG\nrecognized = finalseg.cut(buf)\nAttributeError: module 'jieba.finalseg' has no attribute 'cut'\n看了下源码，如下，请大神帮忙了!\nelse:\nif not self.FREQ.get(buf):\nrecognized = finalseg.cut(buf)\nfor t in recognized:\nyield t\nelse:\nfor elem in buf:\nyield elem\nbuf = ''",
                "遇到了同样的问题"
            ]
        },
        {
            "time": "Dec 24, 2013",
            "title": "使用其他字典出现的问题ValueError: too many values to unpack",
            "contents": [
                "兄弟，我想问问结巴分词添加了关键词功能后，一些词如：“一些”、“大概”、“已经”...能不能过滤掉？",
                "你可以自行增加 stop word 字典，就可以濾掉這些詞，詳見範例 https://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py",
                "兄弟，终于回我了，非常感谢！已添加 notifications@github.com 为联系人，发邮件比较容易联系你吗？",
                "@agetoo notifications@github.com 為系統信，還是直接在 github 上討論就會看到了～",
                "比如😂这个特殊符号，分词会将这一个符号切成了两个，出现乱码？",
                "如何将类似“abc123456”的文本切分为“abc”和“123456”？默认设置是切分不开的。",
                "给代码里的几处re_eng戳个洞，分出来一类数字看看？",
                "谢谢啦！",
                "你好，我使用其他字典，字典是根据你的格式来改的！",
                "你是用空格分隔的还是用tab分隔的？我用tab分隔也出现了这个问题",
                "还有，总共三个字段，虽然文档上面说最后一个词性可以省略，但是我看源码，貌似不能省略，要不然也会报错（我用Python3）",
                "我是修改源码来解决的...\n修改jieba: /usr/lib/python2.7/site-packages/jieba/analyse/init.py 的get_idf(abs_path), 在第29行加上:\nif lines and not lines[-1]:                                     \n    lines.pop(-1)\n\n最后是这个样子:\n  lines = content.split('\\n')\n  if lines and not lines[-1]:\n      lines.pop(-1)\n  for line in lines:\n      word,freq = line.split(' ')\n      idf_freq[word] = float(freq)"
            ]
        },
        {
            "time": "Apr 11, 2018",
            "title": "jieba无法正确识别人名啊！",
            "contents": [
                "No description provided.",
                "@54xiaosu 您好。\n我之前仿照楼主大神的算法写了一个，https://github.com/aszxqw/cppjieba\n经线上测试，挺稳定。效果蛮好。不知道是否对您有所帮助 :)",
                "@aszxqw ，线上使用了啊，方便透漏是哪家网站吗？",
                "@fxsjy 已私聊。",
                "成功安装，但是报错信息Failed building wheel for jieba，可能是这个错误的原因，jieba模块不能正常运行",
                "给详细日志啊",
                "D:\\learn-python\u003epip install jieba\nCollecting jieba\nUsing cached jieba-0.37.zip\nBuilding wheels for collected packages: jieba\nRunning setup.py bdist_wheel for jieba\nComplete output from command c:\\users\\administrator\\appdata\\local\\programs\\python\\python35-32\\python.exe -c \"import setuptools;file='C:\\Users\\ADMINI1\\AppData\\Local\\Temp\\pip-build-ibpo4da7\\j\nieba\\setup.py';exec(compile(open(file).read().replace('\\r\\n', '\\n'), file, 'exec'))\" bdist_wheel -d C:\\Users\\ADMINI1\\AppData\\Local\\Temp\\tmpf6wrfjs2pip-wheel-:\nTraceback (most recent call last):\nFile \"\", line 1, in \nUnicodeDecodeError: 'gbk' codec can't decode byte 0x80 in position 100: illegal multibyte sequence\n\nFailed building wheel for jieba\nFailed to build jieba\nInstalling collected packages: jieba\nRunning setup.py install for jieba\nSuccessfully installed jieba-0.37\n给这个你看行不行",
                "感觉是 pip 的锅：终端是 GBK 编码，setup.py 是 UTF-8，然后读入直接运行\n你的运行环境是什么？（我打算去报 bug）",
                "windows7 64位系统\nD:\\learn-python\u003epython --version\nPython 3.5.0",
                "这个现在有解决方案吗",
                "`text = \"给我办一个三十的新流量业务\"\nresult = []\nfor (word, start, end) in jieba.tokenize(text):\npseg_data = [(w, f) for (w, f) in pseg.cut(word)]\nprint((pseg_data, start, end))\nresult.append((pseg_data, start, end))\n`\n([('给', 'p')], 0, 1)\n([('我', 'r')], 1, 2)\n([('办', 'v')], 2, 3)\n([('一个三十', 'm')], 3, 7)\n([('的', 'uj')], 7, 8)\n([('新', 'a')], 8, 9)\n([('流量', 'n')], 9, 11)\n([('业务', 'n')], 11, 13)\n如果是\"两个三十\"就可以分开",
                "nr===========\n['耿直', '易怒', '易怒', '鲁莽', '安静', '耿直', '公正', '公允', '明朗', '妙语连珠', '言相告', '赤诚', '赤诚', '金石为开', '子虚', '阳奉阴违', '恩尽义', '向壁虚造', '向壁虚构', '空中楼阁', '阳奉阴违', '殷勤', '和蔼', '和善', '和悦', '过谦', '周全', '和蔼可亲', '关怀备至', '殷勤', '大智若愚', '一谦四益', '纳谏如流', '博采', '刚毅', '麻利', '英勇', '英勇', '耿直', '雷厉风飞', '和蔼', '和悦', '仁慈', '慈善', '慈祥', '和善', '温顺', '温驯', '温雅', '慈爱', '蔼然可亲', '和蔼可亲', '温文', '慈悲为怀', '仁慈', '仁慈', '温柔敦厚', '束', '安静', '安静', '安闲', '安逸', '安祥', '安然', '安神', '静谧', '静默', '若素', '和蔼可亲']\n这些词的flag全部都是nr，什么情况？难道只要是中文姓氏开头的词都是人名吗？那准确率太低了吧。。。",
                "查了几个，在dict.txt都是人名。可能是用其他词性工具生成不够准确吧。"
            ]
        },
        {
            "time": "Mar 19, 2018",
            "title": "bug? add_word失败了",
            "contents": [
                "I would like to treat !! as one token instead of 2.\njieba.add_word(u'!!')\nfor x in jieba.cut(u'巨大的恶魔!!'):\n    print x\n巨大\n的\n恶魔\n!\n!",
                "你好，请问下日期；数字这些问题应该怎么解决呢？",
                "不知道你说的是什么数字问题,如果是需要将2017-09-01这样的日期切分成一个单词的话,可以考虑将其这样格式的日期添加到自定义词典中.我目前就是这么处理的效果不错",
                "@aliray ，将这些格式的日期添加到自定义词典中，就可以处理吗？比如加入词库是2017-09-01，想要将2017-10-01分词到一起可能吗？貌似我这边不行啊",
                "词性标注是否可以指定分词模式；\n想指定搜索模式\n谢谢。",
                "O_O\nOut[4]: '\u003csong\u003e丑女七回头\u003c/song\u003e'\njieba.lcut(\"\u003csong\u003e丑女七回头\u003c/song\u003e\")\nOut[5]: ['\u003c', 'song', '\u003e', '丑女', '七', '回头', '\u003c', '/', 'song', '\u003e']\njieba.add_word(O_O)\njieba.lcut(\"\u003csong\u003e丑女七回头\u003c/song\u003e\")\nOut[7]: ['\u003c', 'song', '\u003e', '丑女', '七', '回头', '\u003c', '/', 'song', '\u003e']\njieba.lcut(O_O)\nOut[8]: ['\u003c', 'song', '\u003e', '丑女', '七', '回头', '\u003c', '/', 'song', '\u003e']"
            ]
        },
        {
            "time": "Oct 28, 2016",
            "title": "spark中import jieba.analyse失败",
            "contents": [
                "我有一批关键词，处理步骤如下：\n1、先将关键词根据自定义词典分词\n2、将分词后的结果topk排序\n3、把排序后的结果排除一些无意义的词\n现在的问题是发现topk排序，调用的分词好像没有调用自定义词典，也不知道是不是我搞错了，求作者出来解释下topk是怎么处理的，谢谢～～～\n疑问：\n1、怎么样可以topk排序调用分词后的结果呢\n2、topk排序计算的tf-idf值可以调用出来么？计算的文档数量是多少？",
                "@lbw1215 ，没太看明白。贴点代码吧",
                "@fxsjy\n雅思必备词汇有多少     雅思|必|备|词汇|有|多少   雅思|必备|词汇|有|多少  雅思|必备|词汇\n一共4列，第一列是原词，第二列是分词后的结果，第三列是对分词后的结果进行topk排序，第四列是对排序后的结果排除某些无意义的词\n现在的问题是发现第三列处理的数据，好像不能调用第二列处理后的数据，不知道topk的调用机制是怎么样呢？\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf-8')\nsys.path.append('../')\nimport jieba\njieba.initialize()\njieba.load_userdict(\"userdict.txt\")   #自定义词典\nimport jieba.posseg as pseg\nimport jieba.analyse\nimport re\nf = open('output.txt','wb')\nf_ex = open('words.txt','rb')   #排除词，一行一个单词\nwords = [line.strip() for line in f_ex.readlines()]\nfor eachline in open('input.txt','rb'):\ni = 0\nline = eachline.strip().decode('gbk')      #str\nf.write(line.encode('utf-8')+'\\t')\n\n\ncut_result = list(pseg.cut(line))                 #分词\nfor w in cut_result:\n    f.write(w.word.encode('utf-8')+'|')\nf.seek(-1,1)\nf.write('\\t')\n\ntags = jieba.analyse.extract_tags(line, topK=20)    #TOPK排序\nfor w in tags:\n    f.write(w.encode('utf-8')+'|')\nf.seek(-1,1)\nf.write('\\t')\n\nfor cut_w in tags:\n    for w in words:                           #去除指定关键词\n        #cut_w.word = re.sub(w.decode('gbk'),'',cut_w.word)\n        if w.decode('gbk') == cut_w :\n            cut_w = ''\n    if cut_w != '':\n        f.write(cut_w.encode('utf-8')+'|')\nf.seek(-1,1)\nf.write('\\t')\nf.write('\\r\\n')\n\nf.close()\nf_ex.close()",
                "不知道有什么通用的方法来生成语料库？\n我用几个语料连接到了一起，通过extract_tags生成了权重，作为IDF来使用，但似乎不好使。\nIDF中没有的词是否可以识别？\n我看如果是新词，指定了IDF后似乎都不能extract_tags出来。",
                "好吧，看了下算法，我偷懒了。\n不过还是比较希望有独立的IDF生成实现",
                "可以参考一下 nltk 的实现，先计算所有词的 IDF",
                "@gumblex 参考另外一个gist实现了一个TFIDF，等下看看效果",
                "@geekan 求idf实现方法",
                "No description provided.",
                "在线演示还有人在维护吗？",
                "在我的程序中import jieba成功，并且能够调用分词cut，但是import jieba.analyse失败，无法提取关键词，提示错误为ImportError: No module named jieba.analyse，但是我打印jieba文件夹目录如下，目录中含有analyse。\n['finalseg', 'dict.txt', 'init.pyc', 'analyse', '_compat.py', 'main.py', 'posseg', 'init.py', '_compat.pyc']\n采用from jieba import analyse的方式能够import成功，但是无法调用extract_tags.@fxsjy",
                "@ShaoyongHua 请问是如何解决的？",
                "我可以啊",
                "我也有这个问题，不过我是from jieba import posseg错误",
                "@ocsponge 请问您的问题解决了吗？",
                "在pyspark程序中，引入jieba进行分词，报no module named jieba, 但是在jupyter notebook中，import  jieba的时候是没有任何问题的，求解？",
                "这是spark的问题，不是jieba的问题，第三方包要用特殊方式载入spark的namespace，具体请查spark的文档。\n…\nOn Sun, Oct 7, 2018, 20:31 guangdi ***@***.***\u003e wrote:\n 在pyspark程序中，引入jieba进行分词，报no module named jieba, 但是在jupyter\n notebook中，import jieba的时候是没有任何问题的，求解？\n\n —\n You are receiving this because you are subscribed to this thread.\n Reply to this email directly, view it on GitHub\n \u003c#406 (comment)\u003e, or mute\n the thread\n \u003chttps://github.com/notifications/unsubscribe-auth/AA0SqphEvkPSy1dtZjNnLs6MGxgrQ9eqks5uifQFgaJpZM4Ki_yv\u003e\n ."
            ]
        },
        {
            "time": "Sep 20, 2016",
            "title": "默认词典有词存疑",
            "contents": [
                "比如将XXXX年、XX月视作一个词\n或者我需要将书名号内所有内容视作整个词\n能不能进行自定义配置？",
                "用字典不就行了。匹配正则，想下这效率得有多慢。",
                "您好，每一次我用jieba进行分词的时候，都会有\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /tmp/jieba.cache\nLoading model cost 0.128 seconds.\nPrefix dict has been built succesfully.\n\n这样的提示。怎么去除这些提示呢？",
                "找到jieba库的__init__.py， 大约28~30行左右，\nlog_console = logging.StreamHandler(sys.stderr)\ndefault_logger = logging.getLogger(__name__)\ndefault_logger.setLevel(logging.DEBUG)\ndefault_logger.addHandler(log_console)\n\n将default_logger.setLevel(logging.DEBUG) 改为 default_logger.setLevel(logging.INFO) 试试。",
                "@oisc it shows error:\nTraceback (most recent call last):\n  File \"xxx.py\", line 29, in \u003cmodule\u003e\n    jieba_logger.handlers.clear()\nAttributeError: 'list' object has no attribute 'clear'",
                "@IvyGongoogle\ntry del jieba_logger.handlers[:] if you are using Python2",
                "@aqua7regia it works. thank you~~",
                "jieba.setLogLevel(20)\n或\nimport logging\njieba.setLogLevel(logging.INFO)",
                "@oisc  sorry, What do you mean for del jieba_logger.handlers[:]?",
                "@gaoyangthu it works, but It seems to be jieba.setLogLevel(logging.INFO)",
                "@IvyGongoogle You're right!\nsetLogLevel(0) equals setLogLevel(logging.NOTSET)\nsetLogLevel(10) equals setLogLevel(logging.DEBUG)\nsetLogLevel(20) equals setLogLevel(logging.INFO)\nsetLogLevel(30) equals setLogLevel(logging.WARNING)\nsetLogLevel(40) equals setLogLevel(logging.ERROR)\nsetLogLevel(50) equals setLogLevel(logging.CRITICAL)",
                "@gaoyangthu  yes, thank you very much~~",
                "原字典自带的dict.txt里面 '阳台' 的词频是 242 , 我在我的自定义字典里, 定义:  阳台栏杆 500  阳台天花 500\n但是在分词的时候,依然会把阳台有关的词分词为:  阳台 / 栏杆  阳台/天花",
                "默认的dict.txt中有一个词'一一分', 这个词从来没见过, 是个词吗?"
            ]
        },
        {
            "time": "May 19, 2015",
            "title": "词性标注的问题",
            "contents": [
                "嗯嗯...",
                "您解决了么？",
                "在cut句子以後，如果變更字典集，不會更新到cache，有什麼方法可以在initialize還可以重新建立一次cache嗎？\nex.\njieba.cut(\"我的靴子裡有蛇\") #我/ 的/ 靴子/ 裡有/ 蛇\njieba.add_word(\"我的靴子\")\njieba.cut(\"我的靴子裡有蛇\") #還是... 我/ 的/ 靴子/ 裡有/ 蛇",
                "请教一个词性标注的问题\n现在的jieba.posseg.cut标注的词性w.flag，是不是都是从字典里面取出来的？\n但是字典里面只定义了一种词性，而很多词是多词性的。\n例如'连'这个词，在金山词霸网站可以查到，'连'有多重词性\nhttp://www.iciba.com/%E8%BF%9E\n\n本义：（动）相连；连接。（副）连续；接续；继续（表示前后没有断开）：～演三十场戏。\n本义：（动）相连；连接。（介）包括在内：～我三个人｜～根拔。\n本义：（动）相连；连接。（名）军队的编制单位；由若干排组成。\n本义：（动）相连；连接。（Lián）姓。\n\nltp是哈工大在线工具分析结果：  http://www.ltp-cloud.com/demo/\n例句一：连小学生都会。\nltp:   [连/u  小学生/n  都/d  会/v  。/wp]\njieba: [连/nr 小学生/nr 都/d 会/v 。/x]       -\u003e 字典： 连 23315 nr\n例句二：这是一个连的兵力。\nltp:   [这/r 是/v  一个/m  连/n  的/u  兵力/n  。/wp]\njieba: [这/r 是/v  一个/m  连的/d 兵力/n 。/x]\n【问题】怎样定义多性词？比如'连'在第一个句子里应该是u，第二个句子里应该是n\n谢谢。",
                "目前jieba的POS Tagging，基于词库分词后，对词库中存在的词，直接取用词典中的词性（第三列为词性）；对于未登录词，再用HMM序列标注来同时完成新词发现和词性标注。\n对于多性词，jieba的方案比较差，而中文中多性词约占了23.6%，比例还是很大的。\n哈工大的LTP并不死绑词典中的词性，而考虑了上下文，并针对未登录词做了不少优化，相较之下好很多。"
            ]
        },
        {
            "time": "Jan 15, 2014",
            "title": "关于字典",
            "contents": [
                "现在只能提取关键词 仍然不能提取频数.....",
                "@fxsjy 楼主可以开放统计频数不？这个接口很重要啊!",
                "文本（'构建了以技术为主的体系'）的分词选择jieba.posseg.cut(data,HMM=True)和jieba.cut(data,HMM=True)结果不一样，是bug吗？求修复\n另外posseg/init.py 237行elif re_eng.match(x):对吗？",
                "No description provided.",
                "首先非常感谢你开源了一个很好用的工具。\n最近我在我的网站上开始使用结巴分词来提取我们网站上的每篇内容的 3 个标签，大部分时候效果很好，但是部分主题里出现了一些奇怪的结果（主题正文右下），比如：\nhttp://www.v2ex.com/t/97153 （上换）\nhttp://www.v2ex.com/t/97151 （想省）\nhttp://www.v2ex.com/t/97140 （就够）\n所以我在想，如果我把我们网站上的所有内容导出，是否有可能经由你的工具生成一个效果更好的 dict.txt？\n谢谢。",
                "jieba的extract_tags使用的分词模式是默认模式，即带有HMM新词发现的模式\n感觉其实这样似乎有点不妥，在生产实践中，还是更偏向于信赖词典，因为新词发现容易产生垃圾词（线上如果出现垃圾词往往效果令人无法容忍），相对比起来，新词没有被发现反而可以让人接受（从而再通过改善词典慢慢优化）。\n所以建议楼主还是使用非默认模式的jieba.cut 应该会理想一些。",
                "@livid ，你好。\njieba的关键词提取功能比较简陋，是基于最简单的tf/idf排序方式。 对于idf.txt中没有的词汇，它的默认值是取得idf.txt中所有词的idf值的median，这一点搞的比较随意。  也许可以通过降低这个默认值来fix你说的这几个case。\nhttps://github.com/fxsjy/jieba/blob/master/jieba/analyse/__init__.py#L33"
            ]
        },
        {
            "time": "Apr 23, 2014",
            "title": "增量导入用户自定义词典",
            "contents": [
                "请问add_word(word, freq=None, tag=None) 这个函数增加的词语只能在当前的python jieba 编译环境下生效，python退出重新登陆时，修改的无效。\n现在我需要永久性新增或者删除词语，如何实现永久性热更新操作？\n谢谢",
                "add_word()只添加到内存中，jieba好像没有提供固化更新字典的实现。但字典格式好像是通用的，可以用一些其他字典生成工具配套使用。",
                "希望支持数据库词库..",
                "用的pip3 install 安装的\n后来又把github源码保存在文件夹下执行\n但总是报错\nmac系统",
                "有出错具体位置的，不然没法定位",
                "嗯 后来把源码放到文件夹里就好了 不知道为什么pip3 install不行",
                "pip3 install jieba3k\nagain 请贴出 bug 出错详情而不是简述",
                "根据 README 的说明，应当是 pip3 install jieba 而不是 jieba3k 吧……",
                "好奇怪",
                "file name should not be jieba.py, you can't import yourself(jieba.py)",
                "i know，thank you for your reply\n            On 02/07/2017 18:26, luotao wrote: file name should not be jieba.py, you can't import yourself(jieba.py)\n\n—You are receiving this because you commented.Reply to this email directly, view it on GitHub, or mute the thread.\n\n\n\n\n\n\n\n\n{\"api_version\":\"1.0\",\"publisher\":{\"api_key\":\"05dde50f1d1a384dd78767c55493e4bb\",\"name\":\"GitHub\"},\"entity\":{\"external_key\":\"github/fxsjy/jieba\",\"title\":\"fxsjy/jieba\",\"subtitle\":\"GitHub repository\",\"main_image_url\":\"https://cloud.githubusercontent.com/assets/143418/17495839/a5054eac-5d88-11e6-95fc-7290892c7bb5.png\",\"avatar_image_url\":\"https://cloud.githubusercontent.com/assets/143418/15842166/7c72db34-2c0b-11e6-9aed-b52498112777.png\",\"action\":{\"name\":\"Open in GitHub\",\"url\":\"https://github.com/fxsjy/jieba\"}},\"updates\":{\"snippets\":[{\"icon\":\"PERSON\",\"message\":\"@wall2flower in #299: file name should not be `jieba.py`, you can't import yourself(`jieba.py`)\"}],\"action\":{\"name\":\"View Issue\",\"url\":\"#299 (comment)\"}}}",
                "怪怪\nbyte-compiling /opt/venv27/lib/python2.7/site-packages/jieba/posseg/prob_emit.py to prob_emit.pyc\nKilled",
                "这是一个很大的数据文件，估计内存很小不够编译成 pyc 文件被杀了。你有多少可用内存？",
                "多谢多谢，怪不得，我的机器上只有300多m的可用内存，估计是不够用了",
                "不过你可以直接复制文件进去，现在 300m 应该够载入标准词典运行 jieba 的。这点内存应该也够编译啊，除非是 pypy。",
                "在aws上，小于1G内存都编译不了。说的就是那个老版最小的实例",
                "好像还是这样，我 pip install jieba --log log.txt ，log 最后一句还是：Killed ，我 ram 应该是够得都没怎么用。",
                "jieba是否支持增量导入用户自定义词典？\n即在调用load_userdict()之后，再增量地调用某个函数增加用户自定义词？"
            ]
        },
        {
            "time": "Jan 3, 2017",
            "title": "请问一个词有多个词性，自定义字典该如何定义",
            "contents": [
                "目前结巴把 www.baidu.com    等url切分成了['www', '.' ,'baidu', 'com']但是我不想让它切开  该咋么处理。\n还有my_name等样带下划线，也不想让它切开。该咋么做。",
                "中文中有很多词既是名词又是形容词，这种做词性标注时该怎么处理",
                "最近也遇到这样的问题，能一起讨论下解决方案吗？",
                "同问",
                "南京市长江大桥"
            ]
        },
        {
            "time": "Jan 8, 2015",
            "title": "extract_tags结果会遗漏分词",
            "contents": [
                "楼主，一直以来用你的jieba分词，感谢你无私的分享！\n近来发现一个问题：\nfor i in jieba.analyse.extract_tags('上海一日游攻略'):\nprint i\n输出结果：\n攻略\n上海\n我看到idf.txt中‘一日游’与‘攻略’这个词的IDF相差无几，为什么没‘一日游’这个词呢？\n不太明白其中原理。",
                "@bobo1732 , 我这里没有复现此bug，请问你的版本是多少？",
                "@fxsjy 我是用的0.35版本"
            ]
        },
        {
            "time": "Jan 24, 2017",
            "title": "你好 结巴分词模型是如何训练的 提供接口了吗",
            "contents": [
                "合并数词和量词\n数量词 分开之后基本没有意义.",
                "@luw2007 ， 可以考虑。不过只能在posseg子模块中做了，jieba.cut被没有识别词性。",
                "@fxsjy 如果不借助结巴分词，怎么解决它比较好？\n我的想法是直接用正则扫，效率很低。还请提点 :)",
                "现在量词直接被放到数词里面了 有时候会导致歧义\n比如杯子\n水晶杯 水泥杯 冷藏杯 等 直接会被解析成 n + q ..",
                "自定义词库：\n*ST舜船\n代码：\ntitle_list = jieba.cut(\"*ST舜船\")\nprint \"/ \".join(title_list)\n分词结果：\n*/ ST/ 舜船\n版本0.38    查看Changelog：4. 修复load_userdict加载用户词典不能识别含有空格等特殊字符的问题， by @gumblex;",
                "我也遇到这个问题",
                "同遇到这个问题。\n# 结巴自定义词典不支持特殊符号\nimport jieba\njieba.add_word(\"\u003cNUM\u003e\")\njieba.add_word('\u003cENG\u003e')\njieba.add_word('\u003cUNK\u003e')\ns = \"我今天买了\u003cNUM\u003e块钱的\u003cUNK\u003e，\u003cENG\u003e还不错\"\nprint(jieba.lcut(s))\n实际输出\n['我', '今天', '买', '了', '\u003c', 'NUM', '\u003e', '块钱', '的', '\u003c', 'UNK', '\u003e', '，', '\u003c', 'ENG', '\u003e', '还', '不错']\n\n期望输出\n['我', '今天', '买', '了', '\u003cNUM\u003e', '块钱', '的', '\u003cUNK\u003e', '，', '\u003cENG'\u003e', '还', '不错']",
                "代码如下：\nstring=\"这是一个test的行。和一些无意义的dm、jksajdfl\"\njieba.set_dictionary('dict.txt.big')#自定义词库\nres=jieba.lcut(string,HMM=False)\nprint(res)\n['test', 'dm', 'jksajdfl']\n自定义的dict里面只有一个测试的汉字词。",
                "自己解决了。\ninit.py 破坏掉re_eng = re.compile('[a-zA-Z0-9]', re.U) 就可以了。。。。\n比如re_eng = re.compile('aaaaaaaa', re.U)",
                "我用THULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包 训练模型能用在结巴上吗\n多谢"
            ]
        },
        {
            "time": "Jan 26, 2014",
            "title": "词性标注后，少字",
            "contents": [
                "结巴分词好像有停用字典，我在调用的时候会自动去除停用词么？如果不能，应该怎么调用？",
                "我觉得更应该关心怎么使用结巴的停用词表",
                "@DonMillion @chenweican 似乎已有类似的Issue了，请移步#77",
                "No description provided.",
                "在提取关键词的时候：\n1、如何去除掉自定义的停止词；\n2、如何增加自定义的词库，自定义的词库的格式是什么？",
                "词库格式：词语 词频 词性，空格分割，可以省略词频、词性，可以同时省略词频和词性\n文件必须为UTF-8无BOM编码\n格式可参考userdict.txt\n具体请参考readme中的相关说明",
                "停止词列表可以自定义\nreadme中有写\n\n关键词提取所使用停止词（Stop Words）文本语料库可以切换成自定义语料库的路径\n用法： jieba.analyse.set_stop_words(file_name) # file_name为自定义语料库的路径\n自定义语料库示例：https://github.com/fxsjy/jieba/blob/master/extra_dict/stop_words.txt\n用法示例：https://github.com/fxsjy/jieba/blob/master/test/extract_tags_stop_words.py",
                "但是0.36版以后analyse这个模块已经无法调用，没有写在方法里",
                "@ynyxxy 我这一切正常，版本：0.38，测试代码：\nimport jieba.analyse\njieba.analyse.set_stop_words(\"D:/a.txt\")",
                "@1354092549 我再看看，似乎是安装除了问题",
                "import jieba\nimport jieba.posseg as pseg\nwords=pseg.cut(\"又跛又啞\")\nfor w in words:\nprint w.word,w.flag\n\n\n\n输出：\nBuilding Trie..., from C:\\Python27\\lib\\site-packages\\jieba\\dict.txt\nloading model from cache c:\\users\\test\\appdata\\local\\temp\\jieba.cache\nloading model cost 1.33399987221 seconds.\nTrie has been built succesfully.\n又 d\n又 d\n啞 v\n“跛” 字不见了..................................",
                "@elfcool , 多谢提醒，已经修复。麻烦你pull一下，看是否还有问题。",
                "Hi Sun,\n已经好了，万分感谢。\n祝新年快乐~\n\nYanlei Deng\nEast China Normal University\nDepartment of Computer Science\n在 2014-01-28 14:07:09，\"Sun Junyi\" notifications@github.com 写道：\n@elfcool , 多谢提醒，已经修复。麻烦你pull一下，看是否还有问题。\n—\nReply to this email directly or view it on GitHub."
            ]
        },
        {
            "time": "Mar 8, 2018",
            "title": "如何只使用某个自定义的词典?",
            "contents": [
                "如题",
                "@gausszh ，jieba.cut没有对停用词的处理。 jieba.analyse.ChineseAnalyzer有停用词的处理：https://github.com/fxsjy/jieba/blob/master/jieba/analyse/analyzer.py",
                "只能通过这样的方法加停用词么？如果停用词表过大怎么办呢？可不可以通过自定义词典给超高词频，来削弱停用词以达到过滤的效果？",
                "@Jaybeka 我做了一些尝试，请你参考！\n你需要到对应的文件夹里找到analyzer.py并做相应的修改即可，其中stopword.txt是用户定义的停用词表\nimport codecs\n\nif os.path.exists(r\"/Library/Python/2.7/site-packages/jieba/analyse/stopword.txt\"):\n    print \"Using your own stopwords.\\n\"\n    STOP_WORDS = frozenset(( line.rstrip() for line in codecs.open('/Library/Python/2.7/site-packages/jieba/analyse/stopword.txt','r','utf-8') ))\nelse:\n    print \"Using jieba's stopwords.\\n\"\n    STOP_WORDS = frozenset(('a', 'an', 'and', 'are', 'as', 'at', 'be', 'by', 'can',\n                        'for', 'from', 'have', 'if', 'in', 'is', 'it', 'may',\n                        'not', 'of', 'on', 'or', 'tbd', 'that', 'the', 'this',\n                        'to', 'us', 'we', 'when', 'will', 'with', 'yet',\n                        'you', 'your',u'的',u'了',u'和'))",
                "@fxsjy 我在实验效果的时候看到这样的情况：\n\n我想应该是analyzer此时使用了搜索引擎模式，我最近在做文本分析，想要把结果中“人民大会堂”去掉（即不含“人民”）；请问需要如何做呢？",
                "@zihaolucky\n谢谢你的建议，我现在在研究关键词提取的问题，而原作者@fxsjy 好像没有把analyzer和analyse的停词表统一，所以我对analyse/init.py进行了修改，加了支持导入自定义停词表和自定义语料库（用于得到tf-idf），修改后的文件如下\nhttps://github.com/Jaybeka/jieba/blob/master/jieba/analyse/__init__.py\n因为python新手，可能写得比较复杂，还请多多指教。‘\n但我现在遇到的问题是用微博爬取的数据，关注了什么值得买后，关键词前N个一定是与之相关的。\n我不知道如果调低其tf-idf，是算法问题？还是语料库不够大？\n测试程序如下，备选文本还有log.txt，进行相应替换就好了\nhttps://github.com/Jaybeka/jieba/blob/master/test/test_stopword_tfidf.py",
                "@Jaybeka 关键词提取的问题要请 @fxsjy 同学来解答了。如果你可以把例子截图放上来应该就更清楚了^^;",
                "@fxsjy ，如何升级到最新版的jieba",
                "@just4thu , 最简单的方法是 pip install -U jieba",
                "@fxsjy ，thx very much",
                "@fxsjy 請問如何在斷詞之後使用停用詞處理??",
                "修改https://github.com/fxsjy/jieba/blob/master/jieba/analyse/analyzer.py这个链接给出的对应文件中ChineseTokenizer类就可以了，把mode改成default",
                "@gausszh ，jieba.cut没有对停用词的处理。 jieba.analyse.ChineseAnalyzer有停用词的处理：https://github.com/fxsjy/jieba/blob/master/jieba/analyse/analyzer.py\n\n请问为什么cut/cut_for_search 没有停用词的支持呢？我发现用jieba.analyse.extract_tags 抽取关键词虽然支持停用词字典，但原本就会把一些较不重要的词过滤掉，而被过滤掉的词可能对搜索会有用，比如谁的动物的脚谁是最长的返回['最长', '动物']，而'脚' 对于搜索引擎来说其实也是一个关键词。\n或者我直接将停用词加载到一个集合中，自己在cut/cut_for_search后过滤掉？",
                "单个字的词结巴分不出的，所以脚字不会出",
                "@ShenDezhou 其实我想问为什么cut/cut_for_search 中不能加入停用词？",
                "@morefreeze jieba做的是中文分词，先用(\\r\\n|\\s)正则分出来句子后，再进行前缀匹配或者用HMM从概率分布图上计算最短路径，在最新版本引入的ChineseAnalyzer模块作者用了whoosh模块的StopAnalyzer来剔除分词后出现的停用词。如果你想在分词前剔除停用词，可以在修改re_skip_default和re_skip_cut_all正则，但可能会影响jieba分词的F1Measure结果。",
                "@ShenDezhou 感谢回复！",
                "Please add the LICENSE file to the MANIFEST.in in both jieba and jieba3k so that it's included in the source distribution.",
                "No description provided.",
                "如题，对于不同的用户，我想使用不同的词典，请问要如何才可以做到?并且其它的词典对这一次分词没有作用."
            ]
        },
        {
            "time": "May 29, 2018",
            "title": "’查询卡’如何才能切分成‘查询’和‘卡’呢？",
            "contents": [
                "请教各位：\n1，自定义词库的词频该如何设置，分词效果会比较好？\n我的自定义词库有3w8的词量，只有词没有词频，我就统一设了个词频。起初设置词频均为1，效果不好，很多词都分不出来。后来统一增加词频到100,200,300,500,1000,2000,3000，当词频在1000下时效果随着词频增加变好，而1000以上效果差距不明显。但是我看前面的话题您有回答说词频一般不用太高，3-5就差不多了。而dict.txt这个里词频高的词也不是特别多的。我该怎么设置好呢？\n2，另外，能否解释下jieba.cut，finalseg.cut，posseg.cut这几个接口对分词效果的区别？\n非常感谢！",
                "建议实际统计一下未分词的语料中此自定义词库的词频，若不合适可乘上一个系数试试\n区别在：\n\njieba.cut 用了词典词频 + finalseg 中的 HMM 模型（可选择不用），准确率高\nfinalseg.cut 只用 HMM 模型，准确率不高，内存占用少\nposseg.cut 与 jieba.cut 相似，用于分析词性（Part of Speech）",
                "不好意思，關於自定義辭庫我也有相關的問題，但是類型不太一樣。\n我設了一個userdict.txt並用jieba.load_userdict(file_name)，將其導入。其後是使用jieba.cut分詞。\n一開始自定義的詞我只設一個詞，凱特琳，詞頻為3，此時可以將該詞分出來。\n接著我將userdict.txt擴充至442個詞，詞頻也皆為3，此時卻無法將\"凱特琳\"分出來，結果變為\"凱特\"+\"琳\"。其他詞也無法正確分詞。\n而後將\"凱特琳\"的詞頻逐漸從5,10,100,調高至1000都無法將\"凱特琳\"分出來。\n最後我是將整個userdict.txt裡的內容複製到dict.txt裡面，才有辦法將所有詞分出來。\n之後我又測試，將dict.txt還原，將userdict.txt把所有詞去掉只留下\"凱特琳\"，此時又可以分出來\"凱特琳\"了。\n可能的問題會出在哪呢？",
                "add_word 中修改词频FREQ[word] = log(freq / total) 这样只给单字打补丁又不增加 total 的算法肯定有问题。应该调整所有词汇的频率，或者干脆只记录频数，即时计算频率？（需要测试）",
                "@2153030   我也碰到这种自定义词典添加后反而不能正确切词的问题。难道这是HMM的局限？",
                "这应该是词频算法问题吧，如之前所说。同 #222   @fxsjy",
                "@gumblex , 对，因为add_word这个实现没有去更改已经加载了的词的概率，而total已经发生变化了。",
                "请教下一下，2台电脑：Mac和Ubuntu\njieba均为0.35，使用同样的用户词典\n测试代码完全一样\n但是对于「石墨烯」这个词，mac上可分出「石墨烯」而在ubuntu中分出「石墨」「烯」\n这是为什么呢？\n补充：在Mac中删掉jieba.cache，终于。。也分不出了。。",
                "No description provided.",
                "import jieba\na=jieba.cut(\"阿里妈妈1\");print(\" \".join(a))\nBuilding prefix dict from /usr/lib/python2.6/site-packages/jieba-0.37-py2.6.egg/jieba/dict.txt ...\nLoading model from cache /tmp/jieba.cache\nLoading model cost 0.256 seconds.\nPrefix dict has been built succesfully.\n阿里 妈妈 1\njieba.add_word('阿里妈妈1',100,'nz')\na=jieba.cut(\"阿里妈妈1\");print(\" \".join(a))\n阿里妈妈1\njieba.del_word('阿里妈妈1')\na=jieba.cut(\"阿里妈妈1\");print(\" \".join(a))\n阿里妈妈1",
                "这个问题有解决吗?",
                "已经手动把结巴自带的词典dict.txt中‘查询卡’这个词删掉了，并且结巴自带词典含有‘查询’和‘卡’两个词，为什么使用lcut命令还是切分成’查询卡’呢？只有del_word('查询卡’)后才能被正确切分成‘查询’和‘卡’？请问导致出现这样问题的原理是什么，不用del_word的话怎么解决这个问题"
            ]
        },
        {
            "time": "Nov 29, 2016",
            "title": "关于finalseg模块中re_han的问题",
            "contents": [
                "哪位大牛能教教我，谢谢",
                "把jieba.cache的位置由临时目录改为当前目录就可以了，我周末刚刚修改并且成功部署了：fay@e094dac#L0L65",
                "Command \"/usr/bin/python2 -u -c \"import setuptools, tokenize;file='/tmp/pip-build-6jExFt/jieba/setup.py';f=getattr(tokenize, 'open', open)(file);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, file, 'exec'))\" install --record /tmp/pip-xAfrwY-record/install-record.txt --single-version-externally-managed --compile\" failed with error code -9 in /tmp/pip-build-6jExFt/jieba/",
                "Can you provide full error message?",
                "我在py2.7下\nre_han = re.compile(\"([\\u4E00-\\u9FD5]+)\")对中英文混合分词时出现问题\n比如输入 中国tfboy说唱,篱笆女人等。\nre_han.split结果为[u‘中国’，u'tfbo' ，u'y说唱', ....]\n将re_han 改为 re.compile(u\"([\\u4E00-\\u9FD5]+)\")能正常分组"
            ]
        },
        {
            "time": "Mar 25, 2018",
            "title": "为什么会建立一个缓存文件,单独一个0.889秒,会不会每次都建立缓存.",
            "contents": [
                "终止时刻的可能状态列表如果为E或者S表示它是一个更合适的候选者，除非没有E和S状态，才考虑B和M，看了下char_state_tab，没有e,s状态的字很少。\n另外jieba.posseg.viterbi.py19行应该注释掉吗?",
                "@lurga , 对于第一个问题，需要多测试才能给出准确答复。\n对于第二个问题，非常感谢你，19行本来是我对viterbi算法做的一个剪枝策略，应该是忘了注释。\n剪枝策略应用后，分词速度会更快，准确度会下降多少我也没来得及测试。\n你有空的话，可以帮忙看看19行的策略和20行的策略，综合起来，哪个更好。谢谢。",
                "@fxsjy 我不太懂python，用scala改了个版本，由于概率文件是一样的，结果应该一样，以下测试都直接使用viterbi来分词，没有用词典。\n1、第一个问题我做了下面的测试，优先选择E,S状态的效果要更好一点，当然这只是一个个例，没办法证明它的效果始终更优。\n测试语句是\"张三说的确实在理\"\n终止状态优先选择E,S：('B', 'nrfg'), ('M', 'nrfg'), ('E', 'nrfg'), ('S', 'uj'), ('B', 'ad'), ('E', 'ad'), ('B', 'v'), ('E', 'v')\n不判断终止状态：('B', 'nrfg'), ('M', 'nrfg'), ('E', 'nrfg'), ('S', 'uj'), ('B', 'ad'), ('E', 'ad'), ('S', 'p'), ('B', 'n')\n2、第二个问题我做了下面的测试，剪枝策略对结果影响很大。\n测试语句是 “中国商务部”\ntop4：('M', 'nr'), ('M', 'nr'), ('M', 'nr'), ('M', 'nr'), ('M', 'nr')\ntop10：('E', 'nrfg'), ('S', 'n'), ('B', 'j'), ('M', 'j'), ('M', 'j')\ntop20：('E', 'nrfg'), ('S', 'n'), ('B', 'j'), ('M', 'j'), ('M', 'j')\ntop30：('M', 'nrfg'), ('E', 'nrfg'), ('B', 'j'), ('M', 'j'), ('M', 'j')\ntop40：('B', 's'), ('E', 's'), ('B', 'nt'), ('M', 'nt'), ('E', 'nt')\ntop50：('B', 'n'), ('E', 'n'), ('B', 'nt'), ('M', 'nt'), ('E', 'nt')\ntop60：('B', 'nt'), ('M', 'nt'), ('M', 'nt'), ('M', 'nt'), ('E', 'nt')\n大致取到前60个状态才跟不剪枝的结果保持一致。对t-1时刻的状态做出限定，极大可能导致最后结果出现严重误差，数学如何解释我也不清楚，盼答复。",
                "例如：\n[笑CRY]  500\n将上述整体加入词库中，并进行切分。\n目前看，做不到。\n是需要做哪些调整吗？词典或代码？",
                "补充：上例中的500是词频。",
                "我改了一下，目前支持词库中的符号和空格匹配了 https://github.com/WalkerWang731/jieba",
                "No description provided.",
                "比如：\n高级Java工程师 100 position\n高级Java工程师 110 certificate\n词频是乱填的，两者来自不同的user_dict,会产生覆盖的情况",
                "No description provided."
            ]
        },
        {
            "time": "May 28, 2018",
            "title": "請問是否能更詳細解釋DAG的建立機制",
            "contents": [
                "结巴的字典是几年前的，不知道能否加入一些新的词重新训练一份？\n使用了自定义词典感觉效果不太好。",
                "By theory, jieba is not a trainable model i'm afraid",
                "可以训练啊，你拿别的人工或机器分词/标注的语料，统计一下词频就行。",
                "@gumblex 你好 能详细说说这个过程吗, 我想做一个自己的关键字提取的idf.txt, 但是不知道如何下手, idf.txt中的第二列, 那一串数字, 不知道怎么得出  比如\n劳动防护 13.900677652\n勞動防護 13.900677652\n生化学 13.900677652",
                "First of all thank you for your work, you did so far!\nI have a question for this specific line https://github.com/fxsjy/jieba/blob/cb0de2973b2fafaa67a0245a14206d8be70db515/jieba/posseg/init.py#L17. Why do you use this specific range of unicode literals? For my specific case(russian text) your app not splitting words which is not good at all.\nFor example:\n\u003e\u003e\u003e re_han_internal = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#\u0026\\._]+)\")\n\u003e\u003e\u003e re_han_internal.split(\"\"\"Простой и безопасный способ делиться терминалом: обзор инструмента warp\"\"\")\n['Простой и безопасный способ делиться терминалом: обзор инструмента ', 'warp', '']\n\nIncreasing unicode range helps:\n\u003e\u003e\u003e re_han_internal = re.compile(\"([\\u0041-\\u9FD5a-zA-Z0-9+#\u0026\\._]+)\")\n\u003e\u003e\u003e re_han_internal.split(\"\"\"Простой и безопасный способ делиться терминалом: обзор инструмента warp\"\"\")\n['', 'Простой', ' ', 'и', ' ', 'безопасный', ' ', 'способ', ' ', 'делиться', ' ', 'терминалом', ': ', 'обзор', ' ', 'инструмента', ' ', 'warp', '']",
                "it's based on the dict which is ONLY for chinese by default, u wanna use the program, u should add a special russian dict.",
                "进程是长时间运行进程，这个过程中自定义词典会更新，需要在词典更新后使用新的自定义词典。\n文档中没有关于这个功能的说明。是否可以增加这个功能呢？",
                "你这个解决了吗？\n我现在做法是重新调用load_userdict方法来重新加载自定义词典",
                "「基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)」\n從辭庫到Trie乃至於DAG的形成過程有所不解\n若我的目標為「男朋友」，那麼會是 {0: [0, 1, 2]}嗎?\n那這樣應該會有「男」、「男朋」、「男朋友」3種可能不是嗎?\n但為何只會出現「男」、「男朋友」2種?\n還是其實是{0: [0, 2]} ?\n謝謝"
            ]
        },
        {
            "time": "Jun 18, 2015",
            "title": "新词发现",
            "contents": [
                "您好, 我想在我的论文中引用您的分词器, 请问我可以直接引用github的链接吗? 或者您有发表的作品我需要一并引用? 十分感谢!",
                "No description provided.",
                "请问如何使用新词发现功能？以您举得例子来说，\"他来到了网易杭研大厦\"，分词后确实出现了“杭研”，但我怎么判断“杭研”是个新词？",
                "同问，怎么样爬取，学习新词？\n\nRegards,\nLeon Lee\n(86)1382-510-1940\nSkype:lt.leon0519\n2015-06-18 15:39 GMT+08:00 maybeluo notifications@github.com:\n\n请问如何使用新词发现功能？以您举得例子来说，\"他来到了网易杭研大厦\"，分词后确实出现了“杭研”，但我怎么判断“杭研”是个新词？\n—\nReply to this email directly or view it on GitHub\n#273.",
                "【新词】这个概念吧，基本上就是说词典里面没有的就是新词，如果词典里面有，就不是新词。。。。",
                "jieba是否有接口获取发现的新词列表？或者有接口获取已有的词典用来判断哪些是新词？",
                "确实是词典里面没有就是新词，但是在词典很大的情况下一个个搜索分出来的词是不是“新词”也是不现实的。"
            ]
        },
        {
            "time": "Jun 13, 2014",
            "title": "生成关键词，里面有一些符号如何处理，如()空格等一些符号",
            "contents": [
                "使用精确分词和搜索引擎分词得到的结果均为：\n\n我们/ 中出/ 了/ 一个/ 叛徒\n\n而使用词性分词得到结果：\n\n我们/r 中/f 出/v 了/ul 一个/m 叛徒/n",
                "23333\n词库里竟然有 中出 这个词。。。我觉得“中出”的出现频率应该不会高于“出了”，为何没有分成功呢。",
                "@shanzi 哈哈哈哈",
                "哈哈哈",
                "哈哈哈",
                "('枪杆子', '中', '出', '政权')\n(枪杆子/n, 中/f, 出/v, 政权/n)\n\n毕竟“中出 叛徒”比“中出 政权”的可能性大",
                "@tonghuix , 分词效果不一致的原因是因为：精确分词中用的HMM模型只有四种中状态（B,E,M,S)，而词性分词中HMM模型的状态是(B,E,M,S)与各种词性的笛卡尔积。",
                "感觉这句话念着好别扭。呵呵…",
                "@fxsjy 感谢详解！那么也就是说，这种分词不一致的情况是不是应该还是很普遍呢？但我看在线演示的例子里 “工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作” 、”我们买了一个美的空调“ 等等句子的分词结果是正确的",
                "@tonghuix , sorry，当前的算法还不够智能不能cover所有的case。 最简单的方法就是你把这种bad case按照你的意图分好，然后让它“强化”学习，得到新的词频文件和HMM模型文件。不过这种调优往往是按下葫芦起了瓢。",
                "@fxsjy 确实如你所说。有没有一种类似敏感词库这种机制来匹配类似情况呢？也就是当发生bad case的时候启用强制分词，当然这种方法比较dirty",
                "jieba.cut('我们中出了一个叛徒', HMM=False) 没有问题，说明“中出”是 HMM 新词发现造出来的。",
                "那也是因为作者训练语料选得好（邪恶……）\nB '\\u4e2d': -4.596743315282086,\nE '\\u4e2d': -5.476938898829366,\nM '\\u4e2d': -5.253194167156688,\nS '\\u4e2d': -4.81355762044073,\nB '\\u51fa': -5.448380394833719,\nE '\\u51fa': -5.556879470894473,\nM '\\u51fa': -7.232833997741765,\nS '\\u51fa': -5.854838121207067,",
                "部分代码：\nsentence = getText('三国演义.txt')\nkeywords = jieba.analyse.extract_tags(sentence, topK=20, withWeight=True, allowPOS=('ns'))\n按地名统计前20，出来的结果是：\n将军 0.11860080694708297\n丞相 0.0921638281251496\n主公 0.0715118726461463\n军士 0.061001657055284146\n商议 0.05922492016471381\n云长 0.054936383725351035\n军马 0.05441077997191448\n大喜 0.05001738396254889\n后主 0.04707253654544958\n先主 0.04402888433316952\n都督 0.04296257804954991\n众将 0.04167988860499132\n天下 0.03893417374252234\n陛下 0.03882608055724891\n太守 0.035023324694504726\n人马 0.03359843706149093\n城上 0.03265662810340311\n天子 0.03212681223959663\n后人 0.03164288695617565\n众官 0.03082541531255808\n这是怎么统计出来的地名？完全不对啊！",
                "生成关键词，里面有一些符号如何处理，如()空格等一些符号",
                "例如：全角符号，在全模式下不会出现。但是在精确模式下就会出现。",
                "你是想要清掉標點符號嗎？ 像這篇？ #169"
            ]
        },
        {
            "time": "Feb 29, 2016",
            "title": "中文标点符号如何处理？",
            "contents": [
                "在自己有raw语料的情况下, 有一组单词(没有词频, 主要是想通过获得这些单词的词频去做jieba分词), 如何统计词频会比较合理有效呢?",
                "人工标注或者用其他准确度高的分词软件分好。 然后，统计一下。",
                "链接文章里的新词发现算法也许对你有帮助。http://www.matrix67.com/blog/archives/5044\n2014-12-18 9:53 GMT+08:00 Sun Junyi notifications@github.com:\n\n人工标注或者用其他准确度高的分词软件分好。 然后，统计一下。\n—\nReply to this email directly or view it on GitHub\n#209 (comment).",
                "@fxsjy 能不能公开建立词库、finalseg 和 posseg 概率模型的源码？这对于建立自定义模型很有帮助。",
                "在使用用户词典时，运行样例，若是有词包含空格，例如样例中的\nEdu Trust认证 2000\n则运行时会出现如下的错误：\nPrefix dict has been built succesfully.\nTraceback (most recent call last):\nFile \"test.py\", line 6, in \njieba.load_userdict(\"userdict.txt\")\nFile \"/usr/local/lib/python2.7/dist-packages/jieba/init.py\", line 381, in load_userdict\nf.name, lineno, line))\nValueError",
                "pypi 上的版本没更新",
                "如果我想自己產生詞典, 我的資料只有\n结巴中文分词\n這句話\n那我的詞典應該是這樣嗎\n结 1\n巴 1\n中 1\n文 1\n分 1\n词 1\n结巴 1\n巴中 1\n中文 1\n文分 1\n分词 1\n结巴中 1\n.\n.\n.",
                "切词后会有中文标点，我想要去掉， 有没有相应的词表？",
                "切词之前正则过滤吧，方便快捷。\n表达式随手google一下就有"
            ]
        },
        {
            "time": "Jun 14, 2018",
            "title": "在Windows和linux下，jieba.posseg用于词性提取英语，词性不一样",
            "contents": [
                "import jieba\nimport jieba.posseg as pseg\ns = \"挺好挺好挺好挺好挺好挺好挺好挺好挺好挺好挺好挺好\"\nseg = jieba.cut(s, cut_all=False)\n挺 好 挺 好 挺 好 挺 好 挺 好 挺 好 挺 好 挺 好 挺 好 挺 好 挺 好 挺 好\n\n\n\n\n\n\npos=pseg.cut(s)\n挺好挺好挺好挺好挺好挺好挺好挺好挺好挺好挺好挺好/z\n\n\n\n这个怎么处理下，比较好呢？",
                "用户自定义词典定义了中文和英文结合的词 ，分词没用效果\n比如词典里定义了一个a字群\n分词好像没有效果",
                "小明nr 比p 小红nr 可爱v ，x 但c 都d 没有v 珍珍n 可爱v",
                "无自定义词典\nstr=\"QFII和RQFII迎政策利好 外资更敢来玩了？\"\n在Windows下，seglist=jieba.posseg.cut(str)得到 QFII eng，RQFII eng\n在linux下，seglist=jieba.posseg.cut(str)得到 FII x，FII x",
                "还是用户词典导致了 分词出错了"
            ]
        },
        {
            "time": "Mar 3, 2016",
            "title": "中英文数字混合切分",
            "contents": [
                "在用户自定义字典的时候，内容支持正则吗？",
                "@isafe , 目前还不支持。有开源分词组件支持正则词典的？",
                "好像不支持，有个问题想请教下，我用 jieba提取关键字，提取的内容中是中英文混合的\n比如 sentence = “beijing,北京欢迎你”\njieba.analyse.extract_tags(sentence,1),比如这样出来的可能是“北京” 但是我想要第一个出来的是欢迎 怎么调整？\n2014-07-29 19:02 GMT+08:00 Sun Junyi notifications@github.com:\n\n@isafe https://github.com/isafe , 目前还不支持。有开源分词组件支持正则词典的？\n—\nReply to this email directly or view it on GitHub\n#172 (comment).",
                "@isafe 這樣你要調整 idf.txt 裡面歡迎的權重值，目前最新版的 jieba 可以切換 idf 語料庫，這樣你就可以調整成你想要的權重值～ 不過建議 idf 的權重值應該還是要自己蒐集足夠量的文本之後計算出每個詞的 idf 權重值會比較客觀",
                "@fxsjy  你说的开源分词组建是哪个？",
                "我倒是也有类似的需求，比如凌晨（5:00-6:00）这样的想分成一个词。",
                "jieba.enable_parallel()\njieba.load_userdict(\"./aux/dict\")\n如上所示，如果先开启并行，再载入自定义词典，会导致自定义的词典没有产生效果\n如果先载入自定义词典再进行并行计算开启，自定义词典就是有效的，不知道这算不算bug",
                "确实如此。\n先开启并行，再载入自定义词典。此时，从jieba.dt.get_FREQ和jieba.dt.user_word_tag_tab中都能查到用户词典中的词，说明已导入用户词典中的词，但分词结果中并未产生效果。",
                "这是bug吧？同遇到了，定位了半天问题，求修复啊",
                "如果词典中有个关键词是ck，英文单词lock会被切分为lo、ck。",
                "中英文数字混合切分时会出现英文字母和长串数字切在一起的情况。\neg:\nh123123123\n应该切分为h 123123123"
            ]
        },
        {
            "time": "Nov 10, 2016",
            "title": "词性 eng 是啥？   为什么官方没有词性对照表？",
            "contents": [
                "查看源码,代码中会对文本进行decode,最终生成的分词结果并没有encode回来?\n如果后续用其他工具对分词结果进行处理,会出UnicodeEncodeError,如果对分词结果手动encode就没问题.\n猜想decode是为了把一个字当做一个字符处理,防止一个字的长度大于1.但是切词完之后不encode回来算是隐性的bug?",
                "I wonder the efficiency of Jieba python edition.\nI have a set of corpus around 1.5G size, with 3 of 4 cores parallel processing, using Jieba python and have waited for more than 2hr to finish the segment task.\nIs this normal or anything wrong?\nMy platform is MBP '13 spring with 10G RAM, MacOS 10.9., python 3.4.2.\nThanks a lot.",
                "能自定义词性吗？\n将一部分特殊词放在自定义词典里，标注自定义词性(例如:unc)，jieba.cut后得到特殊词的词性？\n例如：先生，女士，经理，标注为称谓词",
                "明显可以\n你可以把下面的加到自定义字典里\n先生 w 1000\n女士 w 1000\n经理 w 1000\n\n在这里\n单行字典的结构是\nword pos freq\n词 词性 词频",
                "这个格式正确吗？\nword freq pos?\n还有就是会出现自定义词典后，词的词性变为x",
                "word freq pos 是这个格式啊",
                "词性 eng 是啥？   为什么官方没有词性对照表？  好纠结， 上网查了资料也找不到eng是啥",
                "http://blog.csdn.net/syani/article/details/52276282",
                "eng是英语的意思。\n我也没找到官方的词性对照表。楼上那个词性对照表是很久之前的东西了，现在使用的词性集又加入了一些新的词性。",
                "词性参考ICTCLAS呀：\nPOS = {\n    \"n\": {  # 1. 名词  (1个一类，7个二类，5个三类)\n        \"n\": \"名词\",\n        \"nr\": \"人名\",\n        \"nr1\": \"汉语姓氏\",\n        \"nr2\": \"汉语名字\",\n        \"nrj\": \"日语人名\",\n        \"nrf\": \"音译人名\",\n        \"ns\": \"地名\",\n        \"nsf\": \"音译地名\",\n        \"nt\": \"机构团体名\",\n        \"nz\": \"其它专名\",\n        \"nl\": \"名词性惯用语\",\n        \"ng\": \"名词性语素\"\n    },\n    \"t\": {  # 2. 时间词(1个一类，1个二类)\n        \"t\": \"时间词\",\n        \"tg\": \"时间词性语素\"\n    },\n    \"s\": {  # 3. 处所词(1个一类)\n        \"s\": \"处所词\"\n    },\n    \"f\": {  # 4. 方位词(1个一类)\n        \"f\": \"方位词\"\n    },\n    \"v\": {  # 5. 动词(1个一类，9个二类)\n        \"v\": \"动词\",\n        \"vd\": \"副动词\",\n        \"vn\": \"名动词\",\n        \"vshi\": \"动词“是”\",\n        \"vyou\": \"动词“有”\",\n        \"vf\": \"趋向动词\",\n        \"vx\": \"形式动词\",\n        \"vi\": \"不及物动词（内动词）\",\n        \"vl\": \"动词性惯用语\",\n        \"vg\": \"动词性语素\"\n    },\n    \"a\": {  # 6. 形容词(1个一类，4个二类)\n        \"a\": \"形容词\",\n        \"ad\": \"副形词\",\n        \"an\": \"名形词\",\n        \"ag\": \"形容词性语素\",\n        \"al\": \"形容词性惯用语\"\n    },\n    \"b\": {  # 7. 区别词(1个一类，2个二类)\n        \"b\": \"区别词\",\n        \"bl\": \"区别词性惯用语\"\n    },\n    \"z\": {  # 8. 状态词(1个一类)\n        \"z\": \"状态词\"\n    },\n    \"r\": {  # 9. 代词(1个一类，4个二类，6个三类)\n        \"r\": \"代词\",\n        \"rr\": \"人称代词\",\n        \"rz\": \"指示代词\",\n        \"rzt\": \"时间指示代词\",\n        \"rzs\": \"处所指示代词\",\n        \"rzv\": \"谓词性指示代词\",\n        \"ry\": \"疑问代词\",\n        \"ryt\": \"时间疑问代词\",\n        \"rys\": \"处所疑问代词\",\n        \"ryv\": \"谓词性疑问代词\",\n        \"rg\": \"代词性语素\"\n    },\n    \"m\": {  # 10. 数词(1个一类，1个二类)\n        \"m\": \"数词\",\n        \"mq\": \"数量词\"\n    },\n    \"q\": {  # 11. 量词(1个一类，2个二类)\n        \"q\": \"量词\",\n        \"qv\": \"动量词\",\n        \"qt\": \"时量词\"\n    },\n    \"d\": {  # 12. 副词(1个一类)\n        \"d\": \"副词\"\n    },\n    \"p\": {  # 13. 介词(1个一类，2个二类)\n        \"p\": \"介词\",\n        \"pba\": \"介词“把”\",\n        \"pbei\": \"介词“被”\"\n    },\n    \"c\": {  # 14. 连词(1个一类，1个二类)\n        \"c\": \"连词\",\n        \"cc\": \"并列连词\"\n    },\n    \"u\": {  # 15. 助词(1个一类，15个二类)\n        \"u\": \"助词\",\n        \"uzhe\": \"着\",\n        \"ule\": \"了 喽\",\n        \"uguo\": \"过\",\n        \"ude1\": \"的 底\",\n        \"ude2\": \"地\",\n        \"ude3\": \"得\",\n        \"usuo\": \"所\",\n        \"udeng\": \"等 等等 云云\",\n        \"uyy\": \"一样 一般 似的 般\",\n        \"udh\": \"的话\",\n        \"uls\": \"来讲 来说 而言 说来\",\n        \"uzhi\": \"之\",\n        \"ulian\": \"连 \"  # （“连小学生都会”）\n    },\n    \"e\": {  # 16. 叹词(1个一类)\n        \"e\": \"叹词\"\n    },\n    \"y\": {  # 17. 语气词(1个一类)\n        \"y\": \"语气词(delete yg)\"\n    },\n    \"o\": {  # 18. 拟声词(1个一类)\n        \"o\": \"拟声词\"\n    },\n    \"h\": {  # 19. 前缀(1个一类)\n        \"h\": \"前缀\"\n    },\n    \"k\": {  # 20. 后缀(1个一类)\n        \"k\": \"后缀\"\n    },\n    \"x\": {  # 21. 字符串(1个一类，2个二类)\n        \"x\": \"字符串\",\n        \"xx\": \"非语素字\",\n        \"xu\": \"网址URL\"\n    },\n    \"w\": {   # 22. 标点符号(1个一类，16个二类)\n        \"w\": \"标点符号\",\n        \"wkz\": \"左括号\",  # （ 〔  ［  ｛  《 【  〖 〈   半角：( [ { \u003c\n        \"wky\": \"右括号\",  # ） 〕  ］ ｝ 》  】 〗 〉 半角： ) ] { \u003e\n        \"wyz\": \"全角左引号\",  # “ ‘ 『\n        \"wyy\": \"全角右引号\",  # ” ’ 』\n        \"wj\": \"全角句号\",  # 。\n        \"ww\": \"问号\",  # 全角：？ 半角：?\n        \"wt\": \"叹号\",  # 全角：！ 半角：!\n        \"wd\": \"逗号\",  # 全角：， 半角：,\n        \"wf\": \"分号\",  # 全角：； 半角： ;\n        \"wn\": \"顿号\",  # 全角：、\n        \"wm\": \"冒号\",  # 全角：： 半角： :\n        \"ws\": \"省略号\",  # 全角：……  …\n        \"wp\": \"破折号\",  # 全角：——   －－   ——－   半角：---  ----\n        \"wb\": \"百分号千分号\",  # 全角：％ ‰   半角：%\n        \"wh\": \"单位符号\"  # 全角：￥ ＄ ￡  °  ℃  半角：$\n    }\n}",
                "see: https://gist.github.com/luw2007/6016931\nsame as: #453"
            ]
        },
        {
            "time": "Oct 27, 2018",
            "title": "特殊分词控制",
            "contents": [
                "建议提供 debug 选项控制类似下面这样的输出\nprint \u003e\u003e sys.stderr, f_name, ' at line', lineno, line",
                "@shuge ， 什么样的控制？是控制让这些日志不显示，还是说输出到其他地方？",
                "def foo(..., debug=False, log_file=sys.stderr):\n    if debug:\n        print \u003e\u003e log_file, f_name, ' at line', lineno, line\n    ...\n\n您觉得这样会不会更好些？",
                "@shuge , 好的，谢谢你的建议。你要是能给我发pull request就更好了:-)",
                "稍后吧，我上周临时用 pymmseg + AdvancdLangConv 解决了\njieba 是一个很有意思的项目",
                "比如我有一句话，\"2015年度本公司营业利润较2014年度减少-3480.83万元，降幅为-26.95%，主要原因系国内宏观经济增速放缓，下游市场需求不足，市场景气程度有所降低，公司销售收入整体下降。\"\n我先对结巴做了add_word设置\njieba.add_word(\"-26.95%\")\njieba.add_word(u\"-3480.83万元\")\n然后分词结果是：\n2015年度|本|公司|营业利润|较|2014年度|减少|-|3480.83万元|，|降幅|为|-|26.95%|，|主要|原因|系|国内|宏观经济|增速|放缓|，|下游|市场需求|不足|，|市场|景气|程度|有所|降低|，|公司|销售收入|整体|下降|。|\n带符号的就不行",
                "遇到同样问题。\n加入用户字典    test-test 词 依然会被切分成 test/-/test。\n提高词频依然没用，不知道是不是 -  会被固定切出来，不收其他影响。",
                "同样的问题，比如：\njieba.add_word(\"弗里德里克·泰勒\",freq=888888); jieba.add_word(\"冯·诺伊曼\",freq=888888); jieba.suggest_freq(('弗里德里克·泰勒', '冯·诺伊曼'), True); txt = \"这个外国人叫弗里德里克·泰勒，是美国前国防部部长。冯·诺伊曼是公认的计算机发明人\"; print(\" | \".join(jieba.cut(txt, HMM=True, cut_all=False)))\n结果还是：\n'这个 | 外国人 | 叫 | 弗里德里克 | · | 泰勒 | ， | 是 | 美国 | 前 | 国防部 | 部长 | 。 | 冯 | · | 诺伊曼 | 是 | 公认 | 的 | 计算机 | 发明人'\n大家有什么好办法？",
                "有同样的需求，需要保证指定的词绝对不分。不知该怎么做。",
                "代码里面有如下的正则表达式，用来提取中文\n\nre_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#\u0026._%]+)\", re.U)\n也就是说对于”弗里德里克·泰勒”中的“·”不认为是中文，而是作为类似逗号和句号来处理。\n\n一个不太好但是可以用的办法就是，修改上面的正则表达式，将“·”加入。其中\\xb7就是“·”。\n\nre_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#\u0026.\\xb7%]+)\", re.U)\n\n测试结果：\n》 这个 | 外国人 | 叫 | 弗里德里克·泰勒 | ， | 是 | 美国 | 前 | 国防部 | 部长 | 。 | 冯·诺伊曼 | 是 | 公认 | 的 | 计算机 | 发明人",
                "请问下同义词该怎么处理呢？\n如：\n北京大学,北大,pku\n清华大学,清华,Tsinghua University",
                "sent = \"基础上的\"\n我想把它分词成 基础上/的 。\n但是不切分“上的”。\n应该怎么实现这样的功能呢？\n类似的词还有 “既要有” “越要有”\n想分成 既要/有 越要/有",
                "drop word list\n\n\n\n\n\n来自钉钉专属商务邮箱------------------------------------------------------------------\n发件人：OrangeIUH\u003cnotifications@github.com\u003e\n日　期：2018年10月27日 15:45:09\n收件人：fxsjy/jieba\u003cjieba@noreply.github.com\u003e\n抄　送：Subscribed\u003csubscribed@noreply.github.com\u003e\n主　题：[fxsjy/jieba] 特殊分词控制 (#680)\n\nsent = \"基础上的\"\n 我想把它分词成 基础上/的 。\n 但是不切分“上的”。\n 应该怎么实现这样的功能呢？\n 类似的词还有 “既要有” “越要有”\n 想分成 既要/有 越要/有\n—\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread."
            ]
        },
        {
            "time": "Apr 15, 2015",
            "title": "带有词性标注的分词效率是多少？",
            "contents": [
                "为什么使用 Pycharm 调用 jieba 时候，cut 函数找不到？？？？\n输入\nimport jieba\njieba.cut(\"我来到北京清华大学\")\n输出\nAttributeError: 'module' object has no attribute 'cut'\n\n更奇怪的是，输入\nimport jieba\nprint(\"我来到北京清华大学\")\n输出两行这个字\n我来到北京清华大学\n我来到北京清华大学\n\n这些情况单独用 python 运行没问题。\n请问是怎么回事",
                "Pycharm 4.5.3 pro能出cut的呀！",
                "不要将运行的文件名命名为jieba.py，自己撸自己当然出错了",
                "哈哈，我也犯了同样的错误，不要将运行的文件名命名为jieba.py！！！",
                "比如“黄焖鸡米饭”，在词典中我添加了“黄焖鸡”，本机分词正确。但在其他机器上，加载我的词典后，分词仍然为“黄焖”，请问是调用方式错了吗？\n代码如下：\nimport jieba\njieba.load_userdict('new.dic')\nsplit_names=jieba.cut(value)",
                "检查一下那台机器的字典是否正确加载了吧。。暂时没遇到过这种情况",
                "带有词性标注的分词效率是多少？"
            ]
        },
        {
            "time": "Jan 23, 2016",
            "title": "AttributeError: module 'jieba' has no attribute 'cut'",
            "contents": [
                "我想用wiki做自定义词库，已经用gensim训练出一个模型。\n不知道用wiki是否比人民日报的效果好？\n不知道自定义词库格式要求是怎么样的？\n有没有现成的工具可以制作词库？",
                "文档中可以jieba支持自定义加载用户词库的功能，而且自定义的词库的格式也比较简单，我自己是写了一个python脚本来建立自己的词库的。",
                "谢谢 @super1-chen\n请问用了user_dict是否会覆盖原来的默认词库？",
                "基本上是各种\\U000xxxxx的节奏，范围大概是 https://en.wikipedia.org/wiki/Template:CJK_ideographs_in_Unicode 里面的 Unified 和 Compatibility 那样。",
                "python3.5，新建一个py，把给的第一个用法示例复制进去运行，就报这个错误。",
                "我发现了，是起名字的问题，不要命名为jieba。。。",
                "果然 我也遇到这个问题",
                "我的文件名不是jieba也报这个错了，这是为什么",
                "文件名不要用jieba",
                "文件名不要用jieba,一改就好了，神奇",
                "文件名也不要用tokenize.py, 否则也是同样的错误。",
                "@mrkingsun 感谢回答，问题已经解决的话，请close该issue"
            ]
        },
        {
            "time": "Jan 3, 2014",
            "title": "结巴分词有命名实体识别功能吗？",
            "contents": [
                "切词为 吴国忠/ 臣/ 伍子胥\n吴国 174 ns\n吴国忠14 nr\n臣 5666 n\n忠臣 316 n\n是因为P(吴国)_P(忠臣)\u003cP(吴国忠)_P(臣)么",
                "是的",
                "我需要加载千万级的词典，词典才300M，但是加载后内存高达5G。\nself.FREQ[word] = freq\nself.total += freq\nif tag:\nself.user_word_tag_tab[word] = tag\nfor ch in xrange(len(word)):\nwfrag = word[:ch + 1]\nif wfrag not in self.FREQ:\nself.FREQ[wfrag] =\nFREQ\nuser_word_tag_tab\n用了多个字典去存储数据，是不是浪费内存啊，这里可以合并成一个词典吧",
                "@zhangzhenhu ， 可以试一试，发个PR吧 :-)",
                "@fxsjy 128m内存直接跑不起来，256的才勉强能跑。\n我用的bae。",
                "是的，我也遇到一样的问题。加载词典太耗内存了！",
                "jieba.load_userdict('entity_dic.txt')\nTraceback (most recent call last):\nFile \"\", line 1, in \nFile \"/usr/local/lib/python2.7/dist-packages/jieba/init.py\", line 119, in wrapped\nreturn fn(_args, *_kwargs)\nFile \"/usr/local/lib/python2.7/dist-packages/jieba/init.py\", line 334, in load_userdict\nlogger.debug('%s at line %s %s' % (f_name, lineno, line))\nNameError: global name 'f_name' is not defined\n我是用pip安装的。。。",
                "应该是 #257 的问题，还是升级版本吧。",
                "现在版本是0.36",
                "这个问题出现在 0.36 整个版本，你可以自己改正：\njieba/__init__.py第 334 行：\nlogger.debug('%s at line %s %s' % (f.name, line_no, line))",
                "请问，官方什么时候能出一个PHP版本呢？因为分词可能是对网页传来的信息直接就分词了，而不是一直读数据库的",
                "rt。。。"
            ]
        },
        {
            "time": "Jun 26, 2018",
            "title": "复合词，这种怎么分词处理",
            "contents": [
                "请问jieba分词有提供关键词词典吗？（用来获取该关键词的同义词）谢谢！",
                "同求",
                "http://jiebademo.ap01.aws.af.cm/\n在线演示挂啦，直接跳PasS平台咯。",
                "每次都这样...\nBuilding prefix dict from /usr/local/lib/python2.7/site-packages/jieba/dict.txt ...\nLoading model from cache /tmp/jieba.cache\nLoading model cost 0.704 seconds.\nPrefix dict has been built succesfully.",
                "同样遇到加载慢的问题。通常是首次加载，或者长时间空闲后首次加载。\n有时会慢至 6、7 秒甚至十几秒。\n为什么？\n那个 cache 文件很小啊。",
                "待分词的内容：\n\n敏感词|XX功\n大笨蛋\nXX功\n敏感词\n法轮功\n\n问题：无法分出“敏感词”、“大笨蛋”、“XX功”",
                "自定义词典不能解决吗"
            ]
        },
        {
            "time": "Jan 25, 2017",
            "title": "繁體與簡體的轉換結果不同",
            "contents": [
                "\"书品相”，无论用add word，还是大幅提高\"品相\"，均无法切出”书“和”品相“这一结果。",
                "“车、门、窗” 这类单字词无法抽取，这个可以加一个可选参数来来允许单字关键字呢？",
                "print(jieba.lcut('伊莎貝拉海灘'))\n['伊莎貝拉海灘']\nprint(jieba.lcut('伊莎贝拉海滩'))\n['伊莎贝拉', '海滩']\n請問同樣的字串在簡體跟繁體下的拆解結果不同的原因，如果想要拆解繁體得到簡體的結果，該如何調整？",
                "自問自答：掛上 dict.txt.big 之後繁體與簡體的結果就一樣了。"
            ]
        },
        {
            "time": "Feb 16, 2016",
            "title": "jieba.load_userdict 可以支持tuple吗？",
            "contents": [
                "Hello,\n請問應該如何避免句子「我賺了123,244.2元！」被錯誤的分隔成\n['我', '賺', '了', '123', ',', '244.2', '元', '！']\n\n謝謝！",
                "你好，我在使用结巴分词时，出现同同一词汇词性不同的情况，我使用了动态加词汇，修改dict，导入自己的词汇表，都会出现这种情况，比如我定义：街 1000000000000 ns 我的目标是分割出的“街”都是“ns”，但是在实际中会出现：(m)欢乐(a)街(n)怎么(r)走(v)？  去(v)美食(n)街(ns) 这两种不同的情况，请问是怎么回事呢？\nps:我只是在使用jieba,并不懂里面的理论，如果有基本错误还请原谅。",
                "你好，在做基于情感词典的情绪分析，结果发现，结巴分词，会把很多词合在一起。比如：你真笨；\n我想要的结果是：你，真，笨，\n但是返回的却是：你，真笨\n还有：你太笨，返回的也是：你，太笨\n感觉很不科学呐。真和太明明是一种程度词，怎么和笨合成一个词了。\n同时调研了其它多个分词开源工具，发现只有你们会合在一起。",
                "是不是只有自定义的词比你们已有的词长才会有效，而比你们的词短的就不生效？",
                "你可以试试lcut(seq,HMM=false)\n\nBrent\n\nOn Aug 7, 2017, at 18:55, jiangchao123 \u003cnotifications@github.com\u003cmailto:notifications@github.com\u003e\u003e wrote:\n\n\n是不是只有自定义的词比你们已有的词长才会有效，而比你们的词短的就不生效？\n\n—\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\u003c#503 (comment)\u003e, or mute the thread\u003chttps://github.com/notifications/unsubscribe-auth/AM-fgZNs2YqKUicyDBIwL5pGAZrIcKnbks5sVu07gaJpZM4OvHe9\u003e.",
                "试了一下，这样在有些词是可以的，有些依然不行，并且如果HMM设为FALSE的话，好多人名都会被切开了，比如：小明被切成了小，明；李小福被切成了李，小，福；\n找代驾我想分成找，代驾；但是HMM设为True为找代驾；HMM设为False为找，代，驾\n看来是不能两全？？？",
                "jieba.load_userdict 可以支持tuple吗？我想从数据库导出后一把调这个函数，省得到file再绕一圈，或者反复suggest",
                "循环用 add_word(word, freq=None, tag=None)，freq 不填就是 suggest",
                "谢谢",
                "请教以下add_word是永久添加到词典里还是临时添加啊？",
                "我试了一下，是临时 @natsuapo"
            ]
        },
        {
            "time": "Nov 2, 2016",
            "title": "如何用代码修改默认词库？",
            "contents": [
                "以下句子分词可改进：\n句子：今天天气不错\njieba result: 今天天气    不错\n期望结果：今天    天气     不错\n句子：如果放到post中将出错。\njieba result: 如果 放到 post 中将   出错\n期望结果：如果 放到 post 中    将    出错",
                "fxsjy closed this in #351 on 16 Mar",
                "现在我cut\n我想看电视\n结果是\n我 想 看电视\n看了下默认的词库里有 看电视 这个词。。\n我想搞成\n我 想 看 电视",
                "jieba.add_word(\"看电视\",0)#或者jieba.del_word(\"看电视\")#发现主干这个del_word的bug已经修了\njieba.add_word(\"看\",100,\"v\")\njieba.add_word(\"电视\",100,\"n\")"
            ]
        },
        {
            "time": "Mar 9, 2017",
            "title": "jieba 能用于elasticsearch吗？",
            "contents": [
                "打扰，请问这个工具用的算法和Stanford Word Segmenter用的算法有什么主要区别吗？\n以及，如果使用了这个工具，需要引用你们的论文吗？（或者说这个工具有基于某篇论文吗？）——我也可以直接引用github链接。。。",
                "No description provided.",
                "你可能用yield過濾。比如說：\ndef 過濾(input_text):\n    for token in jieba.cut(input_text):\n        if token not in \",.?;'[]()`~!@#$%^\u0026*/+_-=\u003c\u003e{}:，。？！·；：‘“、\\\"\":\n            yield token",
                "jieba 能用于elasticsearch吗？"
            ]
        },
        {
            "time": "Jul 30, 2018",
            "title": "更新词典，缓存jieba.cache是否会更新?",
            "contents": [
                "如图，结巴加载后不会往下执行，而是停在最后一行。\n(py3env1) ➜  nlpTest python -m jieba -d cutTest.txt \u003e cuted2.txt\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /var/folders/fv/9t0ldhvx03j4ch64rv3j9qh40000gn/T/jieba.cache\nLoading model cost 0.855 seconds.\nPrefix dict has been built succesfully.",
                "我在自定义词典里加入了带有罗马数字的词，如“纽约Ⅰ线”，并且为其赋予很高的词频，但是总是被分隔开。请问我要如何才能强制把“纽约Ⅰ线”这样的自定义词完整切分出来？？？\n词典相关内容如下：\n纽约Ⅰ线 100000 ns\npython版本3.6.2\n代码大致如下：\nline = \"并明确控制纽约Ⅰ线功率\"\njieba.load_userdict(\"dict.txt\")\nprint(jieba.get_FREQ(\"纽约Ⅰ线\"))\nprint(jieba.get_FREQ(\"纽约\"))\nprint(jieba.get_FREQ(\"Ⅰ\"))\nprint(jieba.get_FREQ(\"线\"))\nprint(jieba.get_FREQ(\"Ⅰ线\"))\nwords = posseg.cut(line)\nfor word in words:\nprint(word)\n输出结果如下：\n100000\n1758\nNone\n7688\nNone\n并/c\n明确/ad\n控制/updown\n纽约/ns\nⅠ/x\n线/n\n功率/n",
                "Hi  抱歉咨询一个简单的问题， 自定义词典更新后，缓存文件jieba.cache是否会更新?",
                "我发现没有更新，好郁闷！不知道如何更新。"
            ]
        },
        {
            "time": "Mar 3, 2017",
            "title": "试试GANS 自行生成分界 如何",
            "contents": [
                "我想要判斷一篇文章，斷詞後，哪些詞是目前字典中並沒有收錄的   (生詞)\n官方說明有提到，Jieba核心程式，預設遇到生詞，會自動用HMM來辨識\n而我想擷取出這些生詞\n請問有何辦法呢?\n感激!",
                "【词频省略时使用自动计算的能保证分出该词的词频】这句话不太明白。\n词频省略的时候，要如何自动计算词频呢（详细），谢谢~",
                "ganerating advanced nets",
                "generative adversarial networks?"
            ]
        },
        {
            "time": "Apr 1, 2014",
            "title": "pip安装jieba，出现MemoryError",
            "contents": [
                "import jieba.posseg as pseg\nimport jieba\n加载用户词典\njieba.load_userdict(\"C:\\Python27\\Lib\\site-packages\\jieba\\new_dict.txt\")\nwords = pseg.cut(\"女性有月经暴力作用后腹痛\")\nw_list=[]\nprint type(words)\nfor w in words:\n#if w.flag.startswith('n') or w.flag.startswith('v') :\nprint w.word,w.flag\n加载后自定义词典后，居然把“月经”分成“月”和“经”两个词了。\n在自定义的词典中月经词如下：\n月经 5 n",
                "\u003e\u003e\u003e jieba.FREQ[u'月经']\n742\n\u003e\u003e\u003e jieba.FREQ[u'月']\n110207\n\u003e\u003e\u003e jieba.FREQ[u'经']\n21042\n\u003e\u003e\u003e jieba.total\n60101967\n\nP(月经) \u003e P(月)P(经) \u003e P(月经)new\n742/60101967  \u003e 110207×21042/601019672 \u003e 5/60101967\n添加词汇/载入用户词库会覆盖原有词频，而不是相加。已验证这和 #210 的 bug 无关。",
                "@gumblex , 多谢！！明白了",
                "@gumblex ，0.38版本，jieba.total 编译报错？",
                "@cavonchen 改成 jieba.dt.total",
                "好的，谢谢！~",
                "No description provided.",
                "keywords = jieba.analyse.textrank(content, topK=20, withWeight=True)\nfor keyword in keywords:\nprint keyword[0], keyword[1]\n和TF-idf一样的方法，这样就可以了。",
                "python3上面使用这种方法将会得到分开的字，不是score.",
                "你好，\n请问一下，关于结巴的分词功能，有没有进行过什么准确度的测试吗？在哪里可以找到结果？\n谢谢",
                "我在windows server 2008的服务器上pip install jieba（jieba-0.32），出现了如下错误：\nInstalling collected packages: jieba\nRunning setup.py install for jiaba\nSorry: MemoryError: ( )\nSorry: MemoryError: ( )\nSuccessfully installed jieba\nCleaning up......\n这样带来的问题就是，当我用django-haystack + whoosh + jieba的时，用rebuild_index，会出现MemoryError ，而无法生成索引。\n我的是python 2.7，希望得到解决，谢谢",
                "实测原来是服务器内存不够，升级服务器内存之后，上述问题均得到了解决"
            ]
        },
        {
            "time": "Dec 7, 2016",
            "title": "关于词语中有空格的情况",
            "contents": [
                "刚开始导入库时设定了自定义词典:\nimport jieba\njieba.load_userdict(\n    \"/home/weiwu/share/deep_learning/data/dict/finance_dict.txt\")\n\n\n如果程序中途想更换另一个词典，请问该怎么做？",
                "reload(jieba)?",
                "File \"D:/PythonCodes/fenciExp/jiebaExp/main.py\", line 31, in start\nfor x in jieba.cut(i):\nFile \"D:\\��װ\\Anaconda2\\lib\\site-packages\\jieba__init__.py\", line 301, in cut\nfor word in cut_block(blk):\nFile \"D:\\��װ\\Anaconda2\\lib\\site-packages\\jieba__init__.py\", line 233, in cut_DAG\nDAG = self.get_DAG(sentence)\nFile \"D:\\��װ\\Anaconda2\\lib\\site-packages\\jieba__init.py\", line 179, in get_DAG\nself.check_initialized()\nFile \"D:\\��װ\\Anaconda2\\lib\\site-packages\\jieba__init__.py\", line 168, in check_initialized\nself.initialize()\nFile \"D:\\��װ\\Anaconda2\\lib\\site-packages\\jieba__init__.py\", line 143, in initialize\nself.FREQ, self.total = self.gen_pfdict(self.get_dict_file())\nFile \"D:\\��װ\\Anaconda2\\lib\\site-packages\\jieba__init__.py\", line 352, in get_dict_file\nreturn get_module_res(DEFAULT_DICT_NAME)\nFile \"D:\\��װ\\Anaconda2\\lib\\site-packages\\jieba_compat.py\", line 8, in \nos.path.join(*res))\nFile \"D:\\��װ\\Anaconda2\\lib\\site-packages\\setuptools-23.0.0-py2.7.egg\\pkg_resources__init__.py\", line 1178, in resource_stream\nFile \"D:\\��װ\\Anaconda2\\lib\\site-packages\\setuptools-23.0.0-py2.7.egg\\pkg_resources__init__.py\", line 1577, in get_resource_stream\nFile \"D:\\��װ\\Anaconda2\\lib\\site-packages\\setuptools-23.0.0-py2.7.egg\\pkg_resources__init__.py\", line 1530, in _fn\nFile \"D:\\��װ\\Anaconda2\\lib\\ntpath.py\", line 85, in join\nresult_path = result_path + p_path\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xb0 in position 1: ordinal not in range(\n就是一个普通的测试：\n···\nimport jieba\nif name==\"main\":\nstr = '我爱北京天安门'\nfor i in jieba.cut(str):\nprint i\n···",
                "改为\nstr = u'我爱北京天安门'\nPython 处理 UTF-8 的字符串应该在前面加u。",
                "不是这里的问题\n读取字典时传入的字符串是 unicode；库提供的绝对路径是 str，该路径包含中文，所以最终 str + unicode 就出现了解码问题。方便的解决方案就是把你的 Anaconda 放在非中文目录。（这种 Python 2 问题代码里不好解决）",
                "如果词语中有空格，应该怎么写词典条目？",
                "修改正则re_han_default，里面加个空格就能把有空格的词条分出来了",
                "看了一下源码，对于用户词典用的解析方法是re_han_default，对于默认词典的解析方法是直接split空格，这样是不是有问题。",
                "我也有这样的需求，不知道该怎么解决？\n我的自定义词典：\nCriminal Minds\nMurdoch Mysteries\nIron Fist\n中间带空格的词条都被分成单个的单词了。"
            ]
        },
        {
            "time": "Nov 7, 2018",
            "title": "请问支持先分词，再基于分词结果进行词性标注吗？",
            "contents": [
                "用英文试了一下结巴的分词，但是出来的单词很奇怪，很多好像都被切掉了。\n如图\n\n想问下可以支持用自己的分词结果去做比如extract嘛？就比如传list而不是text",
                "问题来源：对 'mmol/L9.69' 字符串进行分词\nstr = 'mmol/L9.69'\njieba.add_word('mmol/L')\njieba.suggest_freq('mmol/L', tune=True)\nres = jieba.cut(des, cut_all=True)\n结果为：[\"mmol\", \"L9\", \"69\"]\n若是cut_all=False\nres = jieba.cut(des, cut_all=False)\n结果为：[\"mmol\", \"/\", \"L9\", \".\", \"69\"]\n我的目的是：将mmol/l分成一个词。\n其他测试：我将'/'改为其他符号，除了'.'号外，其他均不能得到我想要的结果。\n有两个问题：1）jieba分词中是不是给符号分了比较高的权重？ 2）如何达成我的目的？\n谢谢 :)",
                "我也有这个问题，但是开发人员好像无意优化这个，估计要修改挺多内容的",
                "import jieba\n\njieba.suggest_freq(\"{UM}\", tune=True)\njieba.lcut(\"{UM}卡上写的地址就是那个\", HMM=False)\n# ['{', 'UM', '}', '卡', '上', '写', '的', '地址', '就是', '那个']\njieba.suggest_freq(\"卡上\", tune=True)\njieba.lcut(\"{UM}卡上写的地址就是那个\", HMM=False)\n# ['{', 'UM', '}', '卡上', '写', '的', '地址', '就是', '那个']\nsuggest_freq只能处理全部为中文的情况, 如果希望把特殊符号识别成一个词就会出错.",
                "我想先分完词，再基于分词结果做词性标注，请问结巴分词支持吗？我看哈工大的pyltp是支持的（[https://blog.csdn.net/baidu_15113429/article/details/78909666]），但是我不想使用pyltp",
                "這裡好像快倒了 沒啥人氣 慘....",
                "https://blog.csdn.net/enter89/article/details/80619805",
                "參考我修正的項目 #670"
            ]
        },
        {
            "time": "Mar 12, 2018",
            "title": "如何删除已添加的用户词典",
            "contents": [
                "在自己的代码中用suggest_freq(word)操作确保特殊词不被拆分, 但同时代码其他部分引用的外部模块也同时引用了jieba, suggest_freq(word)的操作会影响外部模块分词操作(并不是希望看到的). 对此是否提供清除suggest_freq的操作?",
                "import importlib\nimportlib.reload(jieba) is okey...",
                "by using importlib.reload method, you may encounter this scenario in iPython/jupyter:\nBuilding prefix dict from the default dictionary ...\nBuilding prefix dict from the default dictionary ...\nBuilding prefix dict from the default dictionary ...\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /tmp/jieba.cache\nLoading model from cache /tmp/jieba.cache\nLoading model from cache /tmp/jieba.cache\nLoading model from cache /tmp/jieba.cache\nDumping model to file cache /tmp/jieba.cache\nDumping model to file cache /tmp/jieba.cache\nDumping model to file cache /tmp/jieba.cache\nDumping model to file cache /tmp/jieba.cache\n...\nLoading model cost 2.636 seconds.\nLoading model cost 2.636 seconds.\nLoading model cost 2.636 seconds.\nLoading model cost 2.636 seconds.\nPrefix dict has been built succesfully.\nPrefix dict has been built succesfully.\nPrefix dict has been built succesfully.\nPrefix dict has been built succesfully.\n\nOnce more you reload jieba module, there is one more line output in each section when jieba.dt.initialize() is implemented.\nI'm not sure if there's any influence to other codes.",
                "Every time I reboot the Jieba the setting I added using jieba.add_word(word, freq=None, tag=None) will be washed away. Can I keep it somewhere making the setting permanent?",
                "You can write a custom dictionary, and then load the dictionary at start.",
                "比如动词 名词之类的",
                "https://github.com/fxsjy/jieba#4-词性标注",
                "词性标注的缩写有没有一个对照表，虽然大部分能猜出来",
                "请问一下，添加后自定义词典后如何删掉已添加的词典？比如针对项目特性填加如下所示的词典，设置了中特异的词性jp，项目结束后不想使用了，如何清除已添加的词典。关闭软件再打开后发现还是能分词出jp词性的词并给予jp词性\n用户词典：\n子宫颈阴道上部 jp\n子宫口 jp\n子宫阔韧带 jp\n子宫内膜 jp"
            ]
        },
        {
            "time": "Mar 10, 2017",
            "title": "哈喽LZ\u0026all，想用结巴做语义分析，有什么好的建议么？",
            "contents": [
                "我有一段文本，其中有很多英文以及标点符号，希望只对其中的中文进行分词处理，除了用 stop_words ,还有什么好的办法吗？",
                "先用正则把非中文的字符去除",
                "可以试试和这个类似的正则表达式 http://git.io/piM6rQ",
                "文本中有IP地址，如127.0.0.1，已經用suggest_freq添加到詞典，仍然無法分出。測試代碼：\nprint list(jieba.cut('127.0.0.1 is ip address'))\nprint jieba.suggest_freq('127.0.0.1', True)\nprint list(jieba.cut('127.0.0.1 is ip address'))\n\n輸出：\n[u'127.0', u'.', u'0.1', u' ', u'is', u' ', u'ip', u' ', u'address']\n4\n[u'127.0', u'.', u'0.1', u' ', u'is', u' ', u'ip', u' ', u'address']",
                "首先感谢 @fxsjy 的开源共享精神，让我们能处理中文分词，真的很棒！\n那么问题来了，很多人都想用机器分析用户输入的中文语义，然后做后续的应用开发，比如机器人、人工智能、客服bot……\n有什么好办法去做语义分析吗\n比如，我想知道用户的一句话是一般陈述句还是一般疑问句，还是特殊疑问句，主谓宾是谁\n这样才能让机器去检索数据库，才能回答给具体主语还是宾语这个主体\n目前google开源了他的SyntaxNet，但是只对英文有效，有谁研究过这个么"
            ]
        },
        {
            "time": "Jan 18, 2017",
            "title": "年、月、日以及分、秒这种不应该是量词q吗，为什么jieba的dict里面都是标称数词m呢",
            "contents": [
                "首先感谢 @fxsjy 做了这个项目。\n请问 @fxsjy ，jieba分词的效果如何呢，跟ICTCLASS相比呢？我是说是否做过相关的评测。\n我测试了一些句子，感觉效果还可以。",
                "@hitalex , 目前还没有进行学术上的规范化测试，因为不知道去哪里找这些benchmark数据",
                "（1）我看有人说“将用户词典覆盖jieba/dict.txt 即可”，有人说“用load.userdict方法添加用户词典”。所以请问是不是上述两种方法都可行？\n（2）如果使用后一种方法的话呢，用户词典和jieba默认自带的dict.txt 是同时起作用？还是只有用户词典起作用呢？  “起作用”的意思==“jieba用谁”\n（3）如果一个单词，例如“天安门”，同时在dict.txt 和用户词典中出现并且带有不同的词频，那么用词频计算时，是选择哪一个词频呢？不用引起冲突么？",
                "作为一个大四学生，刚刚学过python，想看看一些成熟的项目具体代码，透彻地进行分析，不知道jieba适不适合呢？",
                "個人感覺挺好的, http://www.cnblogs.com/zhbzz2007/tag/Natural%20language%20processing/",
                "刚刚接触本项目，注意到对于含未登录词的句子引用cut(..., HMM=True)方法时，会将整句作为observable放入HMM模型，用viterbi算法推测分词情况。请问这一方法对于未分词的句子的已识别的部分，是否在模型内体现了用cut(..., HMM=False)的方法？\n如果能确定句子中仅包含已登录词的部分，及其分词情况，进而直接定义好HMM模型中对应的隐藏状态，需要计算概率的HMM的隐藏状态段减少，是否能提高分段预测情况？",
                "No description provided.",
                "支持日期不好"
            ]
        },
        {
            "time": "Apr 2, 2018",
            "title": "如何实现单个字的分词及统计，希望能画出单个字的云图",
            "contents": [
                "按照给定的标点符号规则，将文本断句切割\n例如：\n我是大哥,他是二哥,三哥没有.\n按照逗号断句\n返回结果是\n我是大哥,\n他是二哥,\n三哥没有.\n这样的，jeiba支持吗？",
                "你其实可以自己训练HMM模型来断句。我本来想以jieba为基础做一个，可是这方面需求好像不大，就懒得做了。",
                "jieba能做文本相似度的处理吗？",
                "可以提取关键词，文本相似度可用别的库。话说你想干什么？",
                "文本相似度有哪些好的库？\n做毕设",
                "这要看你的具体应用了，去看一下各种文本或数据相似度算法的介绍和比较，再做决定。每种成熟的算法基本上都有对应的Python库。",
                "我基本是先分句再喂给jieba,用的re.split().",
                "@gumblex 如果用HMM 該怎麼做? 就像分詞一樣的做法嗎?",
                "似乎没有添加自己的停用词可能，是不是可以考虑添加",
                "很多问题都是针对2个字以上词语进行分词的，如何利用JIEBA进行单个字的分词，研究需要研究单个字的出现词频，不需要词组词频，请指点",
                "单字切分不需要结巴了吧？先去停用词，去标点符号\ncorpus = '我永远喜欢结城明日奈'\nlist = []\nfor each in corpus:\nlist.append(each)"
            ]
        },
        {
            "time": "May 15, 2015",
            "title": "怎样将某个词必然与其他的词分隔开？",
            "contents": [
                "我用淘宝一个搜索词的所有商品名字组成一个字符串，交给jieba 切出来的都是一个个的字。用正常的句子试了试也是一样。用的是pip install jieba下载的",
                "比如我想要光彩这个词被单独分开，但是当出现光彩照人时，它们就被连在了一起，我希望将光彩固定与其他词分开",
                "@CrazymageLee 有两种方式：\n1、jieba.suggest_freq(('光彩','照人'),tune=True)\n2、删除‘光彩照人'这个词, jieba.del_word('光彩照人')",
                "@bobo1732 这种方法只适用于光彩照人这一种情况，光彩有时候会与其他的词分在一起，我是希望某个词永远与其他所有词都分开。",
                "当然，简单粗暴的方法是手动指定「光彩」一个特别高的词频，100000之类。"
            ]
        },
        {
            "time": "Feb 4, 2017",
            "title": "jieba.lcut()的并行分词问题",
            "contents": [
                "jieba.suggest_freq()中没有这个方法和add_word()，composer 拉取的结巴分词，为什么？",
                "met a problem when using jython+jieba\njython version :jython-standalone 2.7.1\njieba :0.38\nException in thread \"main\" java.lang.ExceptionInInitializerError Caused by: Traceback (most recent call last): File \"jiebademo.py\", line 7, in \u003cmodule\u003e import jieba File \"/usr/local/lib/python2.7/site-packages/jieba/__init__.py\", line 16, in \u003cmodule\u003e from . import finalseg File \"/usr/local/lib/python2.7/site-packages/jieba/finalseg/__init__.py\", line 30, in \u003cmodule\u003e start_P, trans_P, emit_P = load_model() File \"/usr/local/lib/python2.7/site-packages/jieba/finalseg/__init__.py\", line 24, in load_model start_p = pickle.load(get_module_res(\"finalseg\", PROB_START_P)) File \"/opt/software/apache-maven-3.3.9/m3/repos/org/python/jython-standalone/2.7.1/jython-standalone-2.7.1.jar/Lib/pickle.py\", line 1378, in load File \"/opt/software/apache-maven-3.3.9/m3/repos/org/python/jython-standalone/2.7.1/jython-standalone-2.7.1.jar/Lib/pickle.py\", line 858, in load File \"/opt/software/apache-maven-3.3.9/m3/repos/org/python/jython-standalone/2.7.1/jython-standalone-2.7.1.jar/Lib/pickle.py\", line 858, in load File \"/opt/software/apache-maven-3.3.9/m3/repos/org/python/jython-standalone/2.7.1/jython-standalone-2.7.1.jar/Lib/pickle.py\", line 966, in load_string ValueError: insecure string pickle\nany help??",
                "这个需要你把\n/opt/software/apache-maven-3.3.9/m3/repos/org/python/jython-standalone/2.7.1/jython-standalone-2.7.1.jar/Lib/pickle.py\nline 966\n上下文贴出来看看\nif you can show the context of\n/opt/software/apache-maven-3.3.9/m3/repos/org/python/jython-standalone/2.7.1/jython-standalone-2.7.1.jar/Lib/pickle.py\nline 966\nmaybe i can help you",
                "简单查看了一下jieba.enable_parallel()部分的源代码\ndef enable_parallel(processnum=None):\n    \"\"\"\n    Change the module's `cut` and `cut_for_search` functions to the\n    parallel version.\n    Note that this only works using dt, custom Tokenizer\n    instances are not supported.\n    \"\"\"\n    global pool, dt, cut, cut_for_search\n    from multiprocessing import cpu_count\n    if os.name == 'nt':\n        raise NotImplementedError(\n            \"jieba: parallel mode only supports posix system\")\n    else:\n        from multiprocessing import Pool\n    dt.check_initialized()\n    if processnum is None:\n        processnum = cpu_count()\n    pool = Pool(processnum)\n    cut = _pcut\n    cut_for_search = _pcut_for_search\n其中只是改写了cut和cut_for_search，请问可不可以理解为即使调用了enable_parallel()，使用lcut类的函数还是单进程运行？\n以及，Windows系统下Python同样支持from multiprocessing import Pool，请问为什么一定要限制在os.name != 'nt'下才能使用？",
                "自己回答吧\n虽然不知道为什么原作者不在Windows下开启多进程模式，但是自己改写了一个粗略版。\n思想就是暴力多进程，而且有可能严重消耗内存，适合数据量大同时主机内存空间较大使用\n#-*-coding:utf-8-*- \nimport jieba\nfrom multiprocessing import Pool,cpu_count\n\ndef cut(sentence):\n\tif sentence!=None:\n\t\tsentence = jieba.lcut(sentence,cut_all=False)\n\t\treturn [i for i in sentence]\n\telse :\n\t\treturn None\n\nif __name__ == '__main__':\n        path = raw_input(\"Enter the path: \")\n        f = open(path,'r')\n        # 读取数据\n\tdata = f.readlines()\n        f.close()\n        # 创建进程池\n\tpool = Pool(cpu_count())\n\tdata = pool.map(cut, data)\n\tpool.close()\n\tpool.join()\n这里要求data为可迭代对象，返回值为多维列表",
                "自己回答吧\n虽然不知道为什么原作者不在Windows下开启多进程模式，但是自己改写了一个粗略版。\n思想就是暴力多进程，而且有可能严重消耗内存，适合数据量大同时主机内存空间较大使用\n#-*-coding:utf-8-*- \nimport jieba\nfrom multiprocessing import Pool,cpu_count\n\ndef cut(sentence):\n\tif sentence!=None:\n\t\tsentence = jieba.lcut(sentence,cut_all=False)\n\t\treturn [i for i in sentence]\n\telse :\n\t\treturn None\n\nif __name__ == '__main__':\n        path = raw_input(\"Enter the path: \")\n        f = open(path,'r')\n        # 读取数据\n\tdata = f.readlines()\n        f.close()\n        # 创建进程池\n\tpool = Pool(cpu_count())\n\tdata = pool.map(cut, data)\n\tpool.close()\n\tpool.join()\n这里要求data为可迭代对象，返回值为多维列表\n\n我测试了下，修改了jieba并行分词的源码，如果在win下运行并行分词，会出现错误，而错误貌似是python-x86/x64下才会出现的问题"
            ]
        },
        {
            "time": "May 5, 2014",
            "title": "结巴词库不支持拼音很遗憾",
            "contents": [
                "对--\"我来到北京清华大学\" 分词，\n用 get_DAG(sentence)方法，返回的DAG为：\ndict: {0: [0], 1: [1, 2], 2: [2], 3: [3, 4], 4: [4], 5: [5, 6, 8], 6: [6, 7], 7: [7, 8], 8: [8]}\n而5为清：在dict.txt中做trie树后，查找结果应该为 清华 ，清大 为---5：[5,6,7]\n个人认为方法那个迭代有问题，谢谢！",
                "@gucasbrg, 有什么问题？是闭区间。\n5: [5, 6, 8]\n55：清\n56：清华\n5~8：清华大学",
                "In add_word, it should be:\nfreq = int(freq) if freq is not None else self.suggest_freq(word, False)\nin order to be compatible to:\ndef del_word(self, word):\nself.add_word(word, 0)",
                "You can make a PR.",
                "在Hadoop上怎么加载自定义字典啊",
                "假如我们使用自己的语料来训练HMM，如果HMM的分词不准确，该怎么修改呢？\n方法1. 我们在HMM的语料中人为的增加这个词的出现几率吗？\n但是这样我们就会陷入一个循环：1.手工修改语料---\u003e  2.训练HMM-----\u003e3.测试分词结果   ----如果不行继续回到第一步？",
                "有趣的问题。\n你可以想想看其他的学习方法\n\nBrent\n\nOn Jul 4, 2018, at 17:42, 王晓珂 \u003cnotifications@github.com\u003cmailto:notifications@github.com\u003e\u003e wrote:\n\n\n假如我们使用自己的语料来训练HMM，如果HMM的分词不准确，该怎么修改呢？\n方法1. 我们在HMM的语料中人为的增加这个词的出现几率吗？\n但是这样我们就会陷入一个循环：1.手工修改语料---\u003e 2.训练HMM-----\u003e3.测试分词结果 ----如果不行继续回到第一步？\n\n—\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\u003c#644\u003e, or mute the thread\u003chttps://github.com/notifications/unsubscribe-auth/AM-fgWuc7VQrNuSN_aU5hoxSOpLVjjktks5uDI38gaJpZM4VCRlN\u003e.",
                "用感知机，CRF之类的高级算法",
                "rt，其实有时候拼音纠错还是很大作用的，不支持拼音就意味着不支持同音字，非常希望能加入这个功能"
            ]
        },
        {
            "time": "Jan 12, 2016",
            "title": "jieba.set_dictionary 會忽略詞性",
            "contents": [
                "In [39]: from jieba.analyse import ChineseAnalyzer\nImportError                               Traceback (most recent call last)\n in ()\n----\u003e 1 from jieba.analyse import ChineseAnalyzer\nImportError: cannot import name ChineseAnalyzer\n请问是什么原因？",
                "看看ChineseTokenizer这个类依赖的库是不是没install，比如whoosh这个库，pip install whoosh一下",
                "No description provided.",
                "英文的你要中文分词怎么做，加入词典看看",
                "同样的问题 可以用同义词 但是试了不好用",
                "这里可以将Python ， C++分出来。",
                "“我很帅”这句话词性标注时，“帅”这个词标注为了形容词a，但是在“我非常帅”中“帅”这个词标注为了nr。词典中的“帅”词性为nr。这是什么原因啊，我刚开始使用词性标注工具，原理还不太懂。麻烦懂的解答一下。",
                "set_dictionary :\njieba.set_dictionary('data/dict.txt.big')\n會有 #103 的問題發生"
            ]
        },
        {
            "time": "Nov 6, 2016",
            "title": "词性冲突",
            "contents": [
                "import jieba as jb\nimport jieba.posseg as jp\nif name == 'main':\nr = \"我来自中华人民共和国。\"\nd = jb.cut_for_search(r);\nd1 = jp.cut(\" \".join(d));\n像这样写两次吗？",
                "搜索引擎模式貌似把词拆碎了，再标注词性的用处不大了吧？",
                "我也想问，主要是过滤标点……",
                "No description provided.",
                "当我自定义词典里面的词语的词性与原词典里面相同词语词性冲突时，如何设置让他返回我定义的词性？ 谢谢",
                "请问这个问题有解决吗"
            ]
        },
        {
            "time": "Dec 22, 2016",
            "title": "可以直接对切分语料用baum算法求模型参数吗？",
            "contents": [
                "例如：\n中共中央/总书记/，/国家/主席/，/中央军委/主席\n合并成三个词：中共中央总书记/国家主席/中央军委主席\n或者\n一带/一路\n合并成：一带一路",
                "同问,在关键词抽取这个场景下,太细粒度的分词后抽取到的关键词,不一定是最准确的,这种组合起来的分词更符合关键词提取需求,NLPIR分词后会发现行业新词,因此抽取效果上感觉更好",
                "为什么简体字文章用jieba.analyse.extract_tags提取关键字是繁体字",
                "需求有些特别：要解决的问题的起点并不是文章。而是经过正确分词后的list。有没有什么办法在不经过jieba分词功能的情况下，直接使用关键词提取的功能（调用关键词提取方法之前，传入自定义的分词结果－list格式）",
                "你是不是问题没描述明白？\n你自定义的分词结果是什么东西分出来的？如果是jieba分出来的，那就是可以用自定义词典。\n如果是其他方式分出来的，可以A+B。合并列表。",
                "No description provided."
            ]
        },
        {
            "time": "Feb 15, 2017",
            "title": "demo地址失效",
            "contents": [
                "现在结巴的用户自定义词典是保存成文本文件的，我想问问是否可以增加功能变为使用数据库保存自定义词典呢？这样也可以直接在数据库里面添加新词，维护词典也更方便一些。",
                "much slower when init",
                "在我的自定义词典中有这样一个专有名词\"ens\"，在分词时却将“license”强行分成了“lic/ens/e”。这个要怎么处理这种情况？",
                "在词典里加上“license”，再给个比“ens”较高的数",
                "我觉得jieba应该尊重英文词的边界,当自定义词典里起始或结束是英文字母,例如:\n\n\n\njieba.add_word('w底')\njieba.lcut(\"太好了w底出现了\")  # 这里\"w底\"的w不是英文词的连续,可以切分\n['太好了', 'w底', '出现', '了']\njieba.lcut(\"wow底出现了\") # 这里\"w底\"的w是英文词的连续,不应该切分\n['wo', 'w底', '出现', '了']",
                "@chunsheng-chen 那是不是得有个字典存英文单词，感觉会很大哦",
                "不知道具体细节，但我猜测jieba对英文词的分解是基于类似\"[a-zA-Z0-9]*\"的模式，所以不需要英文字典，例如:\n\n\n\njieba.lcut(\"this is a 1test1-abc2! call 911\")\n['this', ' ', 'is', ' ', 'a', ' ', '1test1', '-', 'abc2', '!', ' ', 'call', ' ', '911']\n\n\n\n如果能尊重英文词的自然分割方式，就不会出现上面的情况了: license是一个完整的词，wow是一个完整的词。",
                "自定义词典中部分词含有日文的假名，但是分词出来好像全部无视了。",
                "http://jiebademo.ap01.aws.af.cm/ 打不开，console里提示net::ERR_NAME_NOT_RESOLVED"
            ]
        },
        {
            "time": "Mar 11, 2016",
            "title": "在线演示出错了，需要升级",
            "contents": [
                "我知道dict.txt是jieba的詞庫，請問當程式用jieba lib分析一篇文章時，\n如何列出該篇文章不在dict.txt的所有關鍵詞?\n底下是我想到的簡單作法:\n是先用cut把文章做分詞之後，然後再一一與dict.txt的詞比較，不知道有沒有其他方式?",
                "我知道dict.txt是jieba的词库，请问当程式用jieba的lib分析一篇文章时，\n如何列出该篇文章不在dict.txt的所有关键词？\n底下是我想到的简单作法：\n是先用cut把文章做分詞之後，然後再一一與dict.txt的詞比較，不知道有沒有其他方式?",
                "繁体文本分词的时候用的是官方提供的dict.txt.big。可以看到里面“清洁机”和“清潔機”的词频、词性都是相同的。但是切分同一个、仅是简繁体不同的句子时：\n“这台清洁机引出一条胶管。”\n“這台清潔機引出一條膠管。”\n“清洁机”和“清潔機”得出的词性却不同，而且“清潔機”的词性是“x”，即“非语素字”。不止一个词有这样的情况。",
                "我也遇到這個問題，而且是默認字典裡面的詞才會有這樣的問題\n如 “攝影機” 在dict.txt.big是n，但是切出來的詞性卻是x。",
                "之前看过你写的goseg，不过好像很久没更新了，期待Golang版的jieba",
                "早就有了\nhttps://github.com/yanyiwu/gojieba",
                "Thank you for visiting AppFog\nThe final sunset date for AppFog v1 is March 15, 2016, (located here at appfog.com). After March 15, 2016 AppFog v1 will no longer be available.\nAppFog v2, located at ctl.io/appfog, has replaced AppFog v1.\nFor more information on this migration and needed action for AppFog v1 users, please see our original communication and migration guides."
            ]
        },
        {
            "time": "Jan 8, 2017",
            "title": "词语是Unicode对词语做文本分析存在的问题",
            "contents": [
                "当去获取词性时，可能有些词没有词性，故会损失准确度\n比如“韵达” 如果用分词，就会分出 韵达\n如果用获取词性，会得到 韵 m 达  v 没有出现 韵达的词性\n我觉得这样并不好，如果没有词性，可否返回一个空或者其它呢，也即尽量不要破坏分词的准度",
                "使用词性标注，分词结果没有精确分词结果准确，比如在精确模式下可以识别出干锅，而在词性标注模式下变成了干和锅两个词，请问如何修正这个问题？",
                "我想把只有在指定的字典有的关键词提出来。\nimport jieba\njieba.initialize()\njieba.set_dictionary('mydic.txt')\n但是出来的词还是包含一堆不在字典中的。请问我该怎么做？谢谢",
                "如果 jieba.analyse.extract_tags 能只返回指定字典里的就更好了",
                "No description provided.",
                "用正则直接匹配可不可以？",
                "正则是可以，如果词库中包含电影电视名 估计能直接拆分出来",
                "用 jieba.add_word",
                "用结巴对文本分完词之后，由于词语是以Unicode形式存在，每当我要将其导出为txt或CSV文件，就会存在编码的问题。我想了一个本办法，把打印出来的结果直接复制粘贴，可是对于有几万行的输出，有什么别的办法吗？",
                "编一下码不就好了么？比如：data.encode(\"utf-8\")"
            ]
        },
        {
            "time": "Oct 8, 2018",
            "title": "如何将识别出的新词加入用户字典？",
            "contents": [
                "像7月5日或四四五二或两万三千零二十这样的要怎么分？",
                "@sunsol ， 现在还不行啊，有什么思路吗？",
                "日期貌似可以蛮力法来解决。",
                "词库支持表达式觉得能 就爽了",
                "比如我使用关键词提取的时候不想要动词，有选项可以直接排除吗？\n另外，我发现关键词提取会提取到数字，我不想要怎么办？",
                "同问，楼楼有答案吗？",
                "@dandan0503 搞不定呢。",
                "用demo里的程序就没有问题，自己新建一个python文件，复制里面的import，执行时总是提示ImportError: No module named analyse\n什么原因\nC:\\Python27\\Lib\\site-packages\\jieba-0.38-py2.7.egg-info\njieba-0.38-py2.7.egg-info\\jieba\n都存在",
                "应该是包结构的问题，看看你init.py是否正确，试试不用没有包结构的情况",
                "`\n\n\n\nseg_list = jieba.cut(\"他来到了网易杭研大厦\")  # 默认是精确模式\nprint(\", \".join(seg_list))\n他, 来到, 了, 网易, 杭研, 大厦`\n\n\n\n但是在分词过程中判断词性\n`\n\n\n\nwords = pseg.cut(\"来到了网易杭研大厦\")\nfor word, flag in words:\n...  print('%s %s' % (word, flag))`\n\n\n\n依旧会把“杭研”分为两个词  杭 j 研 vn  ；\n如何判断“杭研”是否为新词？",
                "可以參考我修正的錯誤 #670"
            ]
        },
        {
            "time": "Mar 8, 2016",
            "title": "jieba.dt找不到",
            "contents": [
                "目前jieba的大部分功能可以在PyPy环境运行。\n希望能作为官方的标准环境，以及性能数据用PyPy环境为准。\n:)\n可以大大增强性能数据的说服力。",
                "@linkerlin , 目前我还没有发现结巴分词在PyPy下的兼容性问题。至于性能方面，pypy的性能并不是总是比CPython好。",
                "这个要看数据量大小，我这里的现象是，可以减少一半的耗时（小时级别的任务）。",
                "['a', ' ', 'b', ' ', 'c']\n出来空格。。",
                "import jieba\n\nFile \"C:\\Python27\\lib\\site-packages\\jieba__init__.py\", line 9, in \nimport logging\nFile \"C:\\Python27\\lib\\logging__init__.py\", line 26, in \nimport sys, os, time, cStringIO, traceback, warnings, weakref, collections\nFile \"C:\\Python27\\lib\\collections.py\", line 10, in \nfrom keyword import iskeyword as iskeyword\nFile \"C:\\Users\\2200621\\Dropbox\\Learning\\Python\\py\\spider\\keyword.py\", line 4,\nin \nimport jieba.analyse\nFile \"C:\\Python27\\lib\\site-packages\\jieba\\analyse__init_.py\", line 2, in \nfrom .tfidf import TFIDF\nFile \"C:\\Python27\\lib\\site-packages\\jieba\\analyse\\tfidf.py\", line 5, in \nimport jieba.posseg\nFile \"C:\\Python27\\lib\\site-packages\\jieba\\posseg__init__.py\", line 257, in \ndt = POSTokenizer(jieba.dt)\nAttributeError: 'module' object has no attribute 'dt'",
                "如果我没记错，应该是 0.37 之后才加上 dt ，不妨检查下 jieba 的版本",
                "同样遇到这个问题。\nTraceback (most recent call last):\nFile \"demo.py\", line 6, in \nimport jieba\nFile \"C:\\Python27\\lib\\site-packages\\jieba__init__.py\", line 9, in \nimport logging\nFile \"C:\\Python27\\lib\\logging__init__.py\", line 26, in \nimport sys, os, time, cStringIO, traceback, warnings, weakref, collections\nFile \"C:\\Python27\\lib\\collections.py\", line 10, in \nfrom keyword import iskeyword as iskeyword\nFile \"D:\\python\\jieba\\keyword.py\", line 7, in \nimport jieba.posseg\nFile \"C:\\Python27\\lib\\site-packages\\jieba\\posseg__init_.py\", line 257, in \ndt = POSTokenizer(jieba.dt)\nAttributeError: 'module' object has no attribute 'dt'",
                "我也碰到楼上的问题，之前碰到的是找不到jieba.analyse模块。\n重新安装之后就遇到楼上的问题，也许依赖顺序的问题？\n我是用的是python2.7．",
                "@seaguest 和 Python 版本没关系，和 jieba 版本有关系， import jieba 后执行 print jieba.__version__ 和 print jieba.__file__ 看看就知道了",
                "@Linusp\n经过试验证明，是我的一个文件命名的问题。\n我有一个文件叫keyword.py，这个文件导致import jieba出错，当我吧名字改成keywords.py,问题就解决了。不知道这背后是什么原因。\n上面的那个错误是发生在import jieba的时候，所以无法执行 print jieba.version 和 print jieba.file。",
                "@seaguest , thanks. your solution works. It happens to me as well."
            ]
        },
        {
            "time": "Apr 13, 2014",
            "title": "是否有和Haystack集成的官方说明？",
            "contents": [
                "想請問:\n我使用ChineseAnalyzer for Whoosh 搜索引擎，當writer.add_document裡面的content字不多的時候，設定的keyword會跟content完全符合才會被列出來; 但當content字多到一定程度的時候(我用bog文章)，content的字只要有部分和keyword一樣就會被列出來。不知道該怎麼處理? 是預設的模式有變化嗎? 看了Whoosh說明文件也不太懂，謝謝。",
                "简单改下：\nclass ChineseTokenizer(Tokenizer):\n应该就可以实现。\nif len(context) \u003c MIN_CONTEXT:\ntoken = Token()\ntoken.text = context\ntoken.pos = 0\n...\nyield token\nelse:\nwords = jieba.tokenize(text, mode=\"search\")\ntoken = Token()\nfor (w,start_pos,stop_pos) in words:\nif not accepted_chars.match(w) and len(w)\u003c=1:\ncontinue\ntoken.original = token.text = w\ntoken.pos = start_pos\ntoken.startchar = start_pos\ntoken.endchar = stop_pos\nyield token",
                "想到可以沿用原來的程式，最後面把print hit.highlights(\"content\")的結果中\"\u003c.....\u003eXXX\"的XXX抓出來和keyword比對，如果一樣就是真的一樣。只是這樣就跟原來ChineseAnalyzer的效果不大一樣??",
                "把痘痘加入自定词典后，也无法把长痘痘中的长和痘痘分开。各位大佬有什么办法嘛？",
                "当前需要hack Haystack，有点丑：\nhttp://ashin.sinaapp.com/article/119/\nhttp://blog.csdn.net/wenxuansoft/article/details/8170714"
            ]
        },
        {
            "time": "May 18, 2015",
            "title": "lcut不可用",
            "contents": [
                "Hi，你有算法详细一点的介绍文档么？github里的algorithm 写得太简单了。",
                "久远的帖子了，今天闲着没事，翻了过去的记录，给你发一些链接：\nhttp://book.51cto.com/art/201106/269050.htm ——概率语言模型的分词方法原理介绍（HMM模型的好像也有介绍）\nhttp://www.matrix67.com/blog/archives/4212 ——漫话中文自动分词和语义识别（上）：中文分词算法\nhttp://www.isnowfy.com/python-chinese-segmentation/ ——最简单的概率分词实现\nhttp://www.oschina.net/code/snippet_1180874_24398 ——最简单的概率分词实现（c++版）\n求介绍文档没太多意思，重要的还是看代码，上面的链接我也并没有完全看完:)",
                "Currently the testcases is in a mess. I wondering if it's ok to make a PR to reorganize all the testcases based on pytest and setup TravisCI to enable continuous integration.",
                "免费分享，造损免责。\n打开默认词典（根目录）或自定义词典，把所有用来间隔词频和词性的空格间隔符改成@@\n（选用@@是因为一般关键词里遇到这个分隔符的几率比较小吧）\n继续，打开jieba根目录下init.py\n\n搜索\nre_han_default = re.compile(\"([\\u4E00-\\u9FD5a-zA-Z0-9+#\u0026\\._]+)\", re.U)\n改成\nre_han_default = re.compile(\"(.+)\", re.U)\n\n\n\n搜索\nre_userdict = re.compile('^(.+?)( [0-9]+)?( [a-z]+)?$', re.U)\n改成\nre_userdict = re.compile('^(.+?)(\\u0040\\u0040[0-9]+)?(\\u0040\\u0040[a-z]+)?$', re.U)\n\n\n\n搜索\nword, freq = line.split(' ')[:2]\n改成\nword, freq = line.split('\\u0040\\u0040')[:2]\n\n\n\n补充：若用的全模式继续改。\n搜索\nre_han_cut_all = re.compile(\"([\\u4E00-\\u9FD5]+)\", re.U)\n改成\nre_han_cut_all = re.compile(\"(.+)\", re.U)",
                "您好，我正好碰见这个问题，我想把 迈克尔·乔丹 不要拆开，试了您的方法，\n将默认的dict改成了如下：\nAT\u0026T@@3@@nz\nB超@@3@@n\nc#@@3@@nz\nC#@@3@@nz\nc++@@3@@nz\n将user_dict变成了如下：\n小皇帝\n科比布莱恩特\n迈克尔·乔丹\ninit.py也按照您的方法改了，没用全模式所以没改最后一个\n加载报错：\nValueError: invalid POS dictionary entry in C:\\Tools\\anaconda\\lib\\site-packages\\jieba\\dict_gai.txt at Line 1: AT\u0026T@@3@@nz\n请您指教！感激不尽！",
                "@xusong123 我试了一下，没有问题啊，是不是用用了记事本打开txt导致加上了bom头。建议用notepad++创建编辑文本。",
                "您好，我试了试，还是不行，方便的话我把这个贴上来给您看看\ndict_gai3.txt\n我加载的是dict_gai3.txt",
                "@xusong123  你试一下，不用我的方法，然后把你字典里的点·删掉，然后看看报错不。如果也报错，就证明你后来新加的词，编码上有问题。",
                "谢谢，已经解决，最好用python3",
                "@xusong123 请问怎么解决的？我也出现同样的错误了，谢谢",
                "我也遇到了这个问题。通常是/tmp/jieba.cache未即时更新引起的。\n解决方法：删除jieba.cache，把默认字典（dict.txt）中的空格替换为@@即可。\n@wonderfreer",
                "结巴分词挺好用的，但是我想直接用lcut生成一个list时出现里下面的问题：\nTraceback (most recent call last):\nFile \"test.py\", line 21, in \nseg_list = jieba.lcut(L[3])\nAttributeError: 'module' object has no attribute 'lcut'\n编译的时候报错没有lcut是为什么？\n部分代码如下：\nimport jieba\nwith open('/home/arnold-hu/dataset/news_data_200line.txt','r') as f:\nline = f.readline()\nL = line.split('\\t')\nseg_list = jieba.lcut(L[3])",
                "你是通过pip install安装的？这个模块似乎目前得用github源代码安装才能用上",
                "用的easy_install，因为用源码安装比较麻烦，这个可以写入readme文档里说明一下吧，虽然lcut貌似用的不多",
                "这是不久之前更新的功能，还没在PyPI上更新。"
            ]
        },
        {
            "time": "Jun 22, 2014",
            "title": "对长标点符号的处理",
            "contents": [
                "刚试了下自定义词典，里边加入这两行：\n打底裤 10000\n打底衫 10000\n发现这两个词仍然分开了。debug之后发现这些词并没有加载。问题在这里：\njieba/init.py:\ndef load_userdict(f):\n......\nif not line.rstrip():\ncontinue\ntup = line.split(\" \")\nword, freq = tup[0], tup[1]\nif freq.isdigit() is False:\ncontinue\nwindows下userdict结尾是\"\\r\\n\", 因为我的词典，词频“1000”后面立刻是\"\\r\\n\", \"\\n\"被split吃掉了，\"\\r\"还在line结尾。所以tup里，freq=\"1000\\r\"，判断 isdigit()为False，这几个词都没被加载。\n怀疑前面的\nif not line.rstrip():\ncontinue\n是不是应该是\nline = line.rstrip()\nif not line:\ncontinue\n这么改之后，词是加载了（在load_userdict()里用print看了），而且“打底衫”分好了，但“打底裤”仍然分成了“打/底裤”。我在词频后面加两个零，结果还是一样。那是什么问题呢？",
                "试试 Git 里的新版本",
                "@gumblex 找到原因了。。原来是我画蛇添足给文件加了个BOM头。老版本并没有删掉这个头。现在好了 :)",
                "@gumblex  java版的结巴， 支持添加自定义词典吗？",
                "@myccc456 Jython 应该是支持的。如果你说的是 这个 Java 版本，根据 README 也应该是支持的。",
                "@gumblex   谢谢， 试了下，通过如下代码即可自定义添加词典 java版\n    WordDictionary dictAdd = WordDictionary.getInstance();\n\n    File file = new File(\"D:/jieba-analysis/conf/user.dict\");\n    dictAdd.loadUserDict(file);",
                "孙中山 孙中山 nr\n毛泽东 毛泽东 nr\n周恩来 周恩来 t\n邓小平 邓小平 nr\n\"周恩来\"被标注成时间词",
                "长标点符号应该被分为一个词。\n例如：\nlongpunct = set(u'——', #EM dash U+2014, 常用\nu'--', #半角-\nu'－－', #U+FF0D, 全角－\nu'――', #Horizontal bar, U+2015\nu'──', #Box Drawing Light Horizontal, U+2500\nu'……',\nu'......', # '.'*6\nu'...', # '.'*3\n) #etc."
            ]
        },
        {
            "time": "Nov 4, 2018",
            "title": "如何验证分词的准确性问题",
            "contents": [
                "最近研究 中文分词，准备自己做一个，采用双向匹配分词和HMM处理未登录词、削歧义。\n不过看了jieba，感觉细节已经做了很多。\n准备实现 whoosh 的分词接口，就用在下一个项目中。\n不知道能不能提供些 jieba设计方面的资料",
                "可以提供源码 ;)",
                "@terminus318 , 最近工作较忙，还未研究whoosh。 在网上搜索时发现有人对whoosh和jieba做了集成。先mark一下： http://blog.csdn.net/wenxuansoft/article/details/8170714",
                "http://www.verydemo.com/demo_c230_i1903.html",
                "@terminus318 , @oldcai , 结巴0.30版已经添加了用于Whoosh的分词接口：ChineseAnalyzer。\n用法：https://github.com/fxsjy/jieba/blob/master/test/test_whoosh.py",
                "哈哈，感谢。\n另：工信处女干事好忙，每次测试都要请她过来。",
                "class Tokenizer(object):\n    ..\n    def suggest_freq(self, segment, tune=False):\n        ..\n        if tune:\n            add_word(word, freq)\n此处,add_word(word, freq)应为self.add_word(word, freq),否则更新的是默认分词器jieba.dt,而不是用户的new_dt = Tokenizer().",
                "我想把新的词汇保存到txt文档里，请问大神怎么做？？？",
                "如何验证分词的准确性\n请问用jieba分完词后，如何去验证分词的准确性？",
                "Same question. What is the training data? 人民日报语料吗？",
                "Same question. What is the training data? 人民日报语料吗？\n不是人民日报的，是网上爬取的一些评论数据，最近看了数学之美的分词，有一些启发"
            ]
        },
        {
            "time": "Mar 25, 2018",
            "title": "3.6的协程语法完全无用,多协程无提升",
            "contents": [
                "Hi I'm using Jieba with Java and I have a question.\nMy purpose to use Jieba is getting words from sentences and add mean per word. But sometimes the result is not what I expected.\nsentence : 我要九个\nresult : 我要 / 九个\nexptected : 我 / 要 / 九 / 个\nsentence : 吃三丸\nresult : \"吃三丸\"\nexpected : 吃 / 三 / 丸\nBecause I have a Chinese English dictionary like this.\n我 : I\n要 : want\n九 : nine\n个 :  individual; measure word for people or objects\nBut for current result I have to have all possible cases in the dictionary.\nex）一个， 两个，三个，四个，六个，七个，八个，九个，是个。。。。\nThanks in advance.",
                "某些新词或不常用词拆开和合并的分词质量比较？\n未来会不会有这种功能的API，方便定量研究？",
                "No description provided.",
                "如果是始终占用着cpu当然无用，如果是http请求或者其他i/o操作，你就能看到协程的真正强大之处了"
            ]
        }
    ]
}